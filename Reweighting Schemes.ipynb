{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically reload modules after executing each cell.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "\n",
    "# Plotting\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib import rc\n",
    "import matplotlib.font_manager\n",
    "rc('font', family='serif')\n",
    "rc('text', usetex=True)\n",
    "rc('font', size=22)\n",
    "rc('xtick', labelsize=15)\n",
    "rc('ytick', labelsize=15)\n",
    "rc('legend', fontsize=15)\n",
    "\n",
    "# ML imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import Adagrad\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reweighting Schemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef train(model):\\n    model_params = {'bce': ['relu', 'relu', 'relu', 'sigmoid', bce, 'adam', 1],\\n                    'mse': ['relu', 'relu', 'relu', 'sigmoid', mse, 'adam', ]}\\n    if model == 'bce':\\n        bce_inputs = Input((1, ))\\n        bce_layer_1 = Dense(64, activation='relu')(bce_inputs)\\n        bce_layer_2 = Dense(128, activation='relu')(bce_layer_1)\\n        bce_layer_3 = Dense(64, activation='relu')(bce_layer_2)\\n        bce_outputs = Dense(1, activation='sigmoid')(bce_layer_3)\\n\\n        bce_model = Model(inputs=bce_inputs, outputs=bce_outputs)\\n        bce_model.compile(loss=bce,\\n                          optimizer='adam',\\n                          metrics='accuracy')\\n\\n        bce_history = bce_model.fit(X_train,\\n                                    y_train,\\n                                    epochs=200,\\n                                    batch_size=int(0.1*N), \\n                                    validation_data=(X_test, y_test),\\n                                    callbacks=[earlystopping])\\n        return bce_model\\n    elif model == 'mse'\\n\\ndef train_mse()\\n    \""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loss functions\n",
    "def bce(y_true, y_pred):\n",
    "    return -((y_true) * K.log(y_pred + K.epsilon()) + (1. - y_true) * K.log(1. - y_pred))\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    return -((y_true) * -K.square(1. - y_pred) + (1. - y_true) * -K.square(y_pred + K.epsilon()))\n",
    "\n",
    "def sqr(y_true, y_pred):    \n",
    "    return -((y_true) * -1. / K.sqrt(y_pred + K.epsilon()) + (1. - y_true) * -K.sqrt(y_pred + K.epsilon()))\n",
    "\n",
    "def mlc(y_true, y_pred):\n",
    "    return -((y_true) * K.log(y_pred + K.epsilon()) + (1. - y_true) * (1. - y_pred))\n",
    "\n",
    "# Models\n",
    "def train(model):\n",
    "    model_params = {'bce': ['relu', 'relu', 'relu', 'sigmoid', bce, 'adam', 1],\n",
    "                    'mse': ['relu', 'relu', 'relu', 'sigmoid', mse, 'adam', 1],\n",
    "                    'sqr': ['relu', 'relu', 'relu', 'relu', sqr, ]}\n",
    "    if model == 'bce':\n",
    "        bce_inputs = Input((1, ))\n",
    "        bce_layer_1 = Dense(64, activation='relu')(bce_inputs)\n",
    "        bce_layer_2 = Dense(128, activation='relu')(bce_layer_1)\n",
    "        bce_layer_3 = Dense(64, activation='relu')(bce_layer_2)\n",
    "        bce_outputs = Dense(1, activation='sigmoid')(bce_layer_3)\n",
    "\n",
    "        bce_model = Model(inputs=bce_inputs, outputs=bce_outputs)\n",
    "        bce_model.compile(loss=bce,\n",
    "                          optimizer='adam',\n",
    "                          metrics='accuracy')\n",
    "\n",
    "        bce_history = bce_model.fit(X_train,\n",
    "                                    y_train,\n",
    "                                    epochs=200,\n",
    "                                    batch_size=int(0.1*N), \n",
    "                                    validation_data=(X_test, y_test),\n",
    "                                    callbacks=[earlystopping])\n",
    "        return bce_model\n",
    "    elif model == 'mse'\n",
    "\n",
    "def train_mse()\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10**6 # Sample size\n",
    "mu = 0.1\n",
    "\n",
    "earlystopping = EarlyStopping(patience=10,\n",
    "                              verbose=1,\n",
    "                              restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate the data.\n",
    "\n",
    "# Background is Normal(-μ, 1)\n",
    "# Signal is Normal(μ, 1))\n",
    "bgd = np.random.normal(-mu, 1, N)\n",
    "sig = np.random.normal(mu, 1, N)\n",
    "X = np.concatenate([bgd, sig])\n",
    "y = np.concatenate([np.zeros(N), np.ones(N)])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a model for each loss\n",
    "\n",
    "# BCE\n",
    "bce_inputs = Input((1, ))\n",
    "bce_layer_1 = Dense(50, activation='relu')(bce_inputs)\n",
    "bce_layer_2 = Dense(50, activation='relu')(bce_layer_1)\n",
    "bce_layer_3 = Dense(50, activation='relu')(bce_layer_2)\n",
    "bce_outputs = Dense(1, activation='sigmoid')(bce_layer_3)\n",
    "\n",
    "bce_model = Model(inputs=bce_inputs, outputs=bce_outputs)\n",
    "bce_model.compile(loss=bce,\n",
    "                  optimizer='adam',\n",
    "                  metrics='accuracy')\n",
    "\n",
    "# MSE\n",
    "mse_inputs = Input((1, ))\n",
    "mse_layer_1 = Dense(50, activation='relu')(mse_inputs)\n",
    "mse_layer_2 = Dense(50, activation='relu')(mse_layer_1)\n",
    "mse_layer_3 = Dense(50, activation='relu')(mse_layer_2)\n",
    "mse_outputs = Dense(1, activation='sigmoid')(mse_layer_3)\n",
    "\n",
    "mse_model = Model(inputs=mse_inputs, outputs=mse_outputs)\n",
    "mse_model.compile(loss=mse,\n",
    "                  optimizer='adam',\n",
    "                  metrics='accuracy')\n",
    "\n",
    "# SQR\n",
    "sqr_inputs = Input((1, ))\n",
    "sqr_layer_1 = Dense(50, activation='relu')(sqr_inputs)\n",
    "sqr_layer_2 = Dense(50, activation='relu')(sqr_layer_1)\n",
    "sqr_layer_3 = Dense(50, activation='relu')(sqr_layer_2)\n",
    "sqr_outputs = Dense(1, activation='relu')(sqr_layer_3)\n",
    "\n",
    "sqr_model = Model(inputs=sqr_inputs, outputs=sqr_outputs)\n",
    "sqr_model.compile(loss=sqr,\n",
    "                  optimizer=RMSprop(lr=1e-6, momentum=0.5),\n",
    "                  metrics='accuracy')\n",
    "\n",
    "# MLC\n",
    "mlc_inputs = Input((1, ))\n",
    "mlc_layer_1 = Dense(50, activation='relu')(mlc_inputs)\n",
    "mlc_layer_2 = Dense(50, activation='relu')(mlc_layer_1)\n",
    "mlc_layer_3 = Dense(50, activation='relu')(mlc_layer_2)\n",
    "mlc_outputs = Dense(1, activation='relu')(mlc_layer_3)\n",
    "\n",
    "mlc_model = Model(inputs=mlc_inputs, outputs=mlc_outputs)\n",
    "mlc_model.compile(loss=mlc,\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Cross Entropy\n",
      "Epoch 1/200\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.6883 - accuracy: 0.5395 - val_loss: 0.6881 - val_accuracy: 0.5403\n",
      "Epoch 2/200\n",
      "150/150 [==============================] - 1s 8ms/step - loss: 0.6882 - accuracy: 0.5399 - val_loss: 0.6882 - val_accuracy: 0.5401\n",
      "Epoch 3/200\n",
      "150/150 [==============================] - 1s 8ms/step - loss: 0.6882 - accuracy: 0.5399 - val_loss: 0.6881 - val_accuracy: 0.5403\n",
      "Epoch 4/200\n",
      "150/150 [==============================] - 1s 8ms/step - loss: 0.6882 - accuracy: 0.5398 - val_loss: 0.6881 - val_accuracy: 0.5403\n",
      "Epoch 5/200\n",
      "150/150 [==============================] - 1s 8ms/step - loss: 0.6882 - accuracy: 0.5399 - val_loss: 0.6881 - val_accuracy: 0.5404\n",
      "Epoch 6/200\n",
      "150/150 [==============================] - 1s 8ms/step - loss: 0.6882 - accuracy: 0.5398 - val_loss: 0.6881 - val_accuracy: 0.5404\n",
      "Epoch 7/200\n",
      "150/150 [==============================] - 1s 8ms/step - loss: 0.6882 - accuracy: 0.5399 - val_loss: 0.6881 - val_accuracy: 0.5403\n",
      "Epoch 8/200\n",
      "150/150 [==============================] - 1s 8ms/step - loss: 0.6882 - accuracy: 0.5397 - val_loss: 0.6881 - val_accuracy: 0.5403\n",
      "Epoch 9/200\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.6882 - accuracy: 0.5398 - val_loss: 0.6882 - val_accuracy: 0.5402\n",
      "Epoch 10/200\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.6882 - accuracy: 0.5400 - val_loss: 0.6881 - val_accuracy: 0.5404\n",
      "Epoch 11/200\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.6882 - accuracy: 0.5399 - val_loss: 0.6881 - val_accuracy: 0.5402\n",
      "Epoch 12/200\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.6882 - accuracy: 0.5400 - val_loss: 0.6881 - val_accuracy: 0.5403\n",
      "Epoch 13/200\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.6882 - accuracy: 0.5398 - val_loss: 0.6881 - val_accuracy: 0.5404\n",
      "Epoch 14/200\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.6882 - accuracy: 0.5399 - val_loss: 0.6881 - val_accuracy: 0.5402\n",
      "Epoch 15/200\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.6882 - accuracy: 0.5398 - val_loss: 0.6881 - val_accuracy: 0.5403\n",
      "Epoch 16/200\n",
      "150/150 [==============================] - 1s 10ms/step - loss: 0.6882 - accuracy: 0.5399 - val_loss: 0.6881 - val_accuracy: 0.5403\n",
      "Epoch 17/200\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.6882 - accuracy: 0.5399 - val_loss: 0.6882 - val_accuracy: 0.5403\n",
      "Epoch 18/200\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.6882 - accuracy: 0.5398 - val_loss: 0.6881 - val_accuracy: 0.5404\n",
      "Epoch 19/200\n",
      "150/150 [==============================] - 1s 10ms/step - loss: 0.6882 - accuracy: 0.5400 - val_loss: 0.6881 - val_accuracy: 0.5404\n",
      "Epoch 20/200\n",
      "150/150 [==============================] - 1s 10ms/step - loss: 0.6882 - accuracy: 0.5400 - val_loss: 0.6881 - val_accuracy: 0.5404\n",
      "Epoch 21/200\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.6882 - accuracy: 0.5398 - val_loss: 0.6881 - val_accuracy: 0.5403\n",
      "Epoch 22/200\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.6882 - accuracy: 0.5399 - val_loss: 0.6881 - val_accuracy: 0.5403\n",
      "Epoch 23/200\n",
      "150/150 [==============================] - 1s 10ms/step - loss: 0.6882 - accuracy: 0.5398 - val_loss: 0.6881 - val_accuracy: 0.5404\n",
      "Epoch 24/200\n",
      "150/150 [==============================] - 1s 10ms/step - loss: 0.6882 - accuracy: 0.5398 - val_loss: 0.6881 - val_accuracy: 0.5404\n",
      "Epoch 25/200\n",
      "150/150 [==============================] - 1s 10ms/step - loss: 0.6882 - accuracy: 0.5399 - val_loss: 0.6882 - val_accuracy: 0.5403\n",
      "Epoch 26/200\n",
      "150/150 [==============================] - 2s 10ms/step - loss: 0.6882 - accuracy: 0.5399 - val_loss: 0.6881 - val_accuracy: 0.5404\n",
      "Epoch 27/200\n",
      "150/150 [==============================] - 1s 10ms/step - loss: 0.6882 - accuracy: 0.5399 - val_loss: 0.6881 - val_accuracy: 0.5402\n",
      "Epoch 28/200\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.6882 - accuracy: 0.5399 - val_loss: 0.6881 - val_accuracy: 0.5402\n",
      "Epoch 29/200\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.6882 - accuracy: 0.5400 - val_loss: 0.6881 - val_accuracy: 0.5404\n",
      "Epoch 30/200\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.6882 - accuracy: 0.5399 - val_loss: 0.6881 - val_accuracy: 0.5403\n",
      "Epoch 31/200\n",
      "145/150 [============================>.] - ETA: 0s - loss: 0.6882 - accuracy: 0.5398Restoring model weights from the end of the best epoch.\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.6882 - accuracy: 0.5397 - val_loss: 0.6881 - val_accuracy: 0.5403\n",
      "Epoch 00031: early stopping\n",
      "\n",
      "==============================\n",
      "Mean Squared Error\n",
      "Epoch 1/200\n",
      "150/150 [==============================] - 2s 10ms/step - loss: 0.2476 - accuracy: 0.5391 - val_loss: 0.2475 - val_accuracy: 0.5403\n",
      "Epoch 2/200\n",
      "150/150 [==============================] - 1s 10ms/step - loss: 0.2476 - accuracy: 0.5399 - val_loss: 0.2475 - val_accuracy: 0.5403\n",
      "Epoch 3/200\n",
      "150/150 [==============================] - 2s 10ms/step - loss: 0.2476 - accuracy: 0.5398 - val_loss: 0.2475 - val_accuracy: 0.5401\n",
      "Epoch 4/200\n",
      "150/150 [==============================] - 1s 10ms/step - loss: 0.2476 - accuracy: 0.5399 - val_loss: 0.2475 - val_accuracy: 0.5403\n",
      "Epoch 5/200\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.2475 - accuracy: 0.5399 - val_loss: 0.2475 - val_accuracy: 0.5401\n",
      "Epoch 6/200\n",
      "150/150 [==============================] - 2s 10ms/step - loss: 0.2476 - accuracy: 0.5399 - val_loss: 0.2475 - val_accuracy: 0.5403\n",
      "Epoch 7/200\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.2475 - accuracy: 0.5400 - val_loss: 0.2475 - val_accuracy: 0.5402\n",
      "Epoch 8/200\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.2476 - accuracy: 0.5399 - val_loss: 0.2475 - val_accuracy: 0.5403\n",
      "Epoch 9/200\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.2475 - accuracy: 0.5399 - val_loss: 0.2475 - val_accuracy: 0.5403\n",
      "Epoch 10/200\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.2476 - accuracy: 0.5400 - val_loss: 0.2475 - val_accuracy: 0.5403\n",
      "Epoch 11/200\n",
      "145/150 [============================>.] - ETA: 0s - loss: 0.2476 - accuracy: 0.5398Restoring model weights from the end of the best epoch.\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.2476 - accuracy: 0.5399 - val_loss: 0.2475 - val_accuracy: 0.5402\n",
      "Epoch 00011: early stopping\n",
      "\n",
      "==============================\n",
      "Square Root\n",
      "Epoch 1/200\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 1579.3684 - accuracy: 0.5006 - val_loss: 1586.4583 - val_accuracy: 0.4983\n",
      "Epoch 2/200\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 1579.3669 - accuracy: 0.5006 - val_loss: 1586.4583 - val_accuracy: 0.4983\n",
      "Epoch 3/200\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 1579.3678 - accuracy: 0.5006 - val_loss: 1586.4583 - val_accuracy: 0.4983\n",
      "Epoch 4/200\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 1579.3679 - accuracy: 0.5006 - val_loss: 1586.4583 - val_accuracy: 0.4983\n",
      "Epoch 5/200\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 1579.3666 - accuracy: 0.5006 - val_loss: 1586.4583 - val_accuracy: 0.4983\n",
      "Epoch 6/200\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 1579.3676 - accuracy: 0.5006 - val_loss: 1586.4583 - val_accuracy: 0.4983\n",
      "Epoch 7/200\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 1579.3665 - accuracy: 0.5006 - val_loss: 1586.4583 - val_accuracy: 0.4983\n",
      "Epoch 8/200\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 1579.3654 - accuracy: 0.5006 - val_loss: 1586.4583 - val_accuracy: 0.4983\n",
      "Epoch 9/200\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 1579.3691 - accuracy: 0.5006 - val_loss: 1586.4583 - val_accuracy: 0.4983\n",
      "Epoch 10/200\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 1579.3662 - accuracy: 0.5006 - val_loss: 1586.4583 - val_accuracy: 0.4983\n",
      "Epoch 11/200\n",
      " 642/1500 [===========>..................] - ETA: 1s - loss: 1579.6234 - accuracy: 0.5005"
     ]
    }
   ],
   "source": [
    "# Train models\n",
    "\n",
    "print('Binary Cross Entropy')\n",
    "bce_model.fit(X_train,\n",
    "              y_train,\n",
    "              epochs=200,\n",
    "              batch_size=10000, \n",
    "              validation_data=(X_test, y_test),\n",
    "              callbacks=[earlystopping])\n",
    "\n",
    "print('\\n==============================\\nMean Squared Error')\n",
    "mse_model.fit(X_train,\n",
    "              y_train,\n",
    "              epochs=200,\n",
    "              batch_size=10000, \n",
    "              validation_data=(X_test, y_test),\n",
    "              callbacks=[earlystopping])\n",
    "\n",
    "print('\\n==============================\\nSquare Root')\n",
    "sqr_model.fit(X_train,\n",
    "              y_train,\n",
    "              epochs=200,\n",
    "              batch_size=1000, \n",
    "              validation_data=(X_test, y_test),\n",
    "              callbacks=[earlystopping])          \n",
    "\n",
    "print('\\n==============================\\nMaximum Likelihood Classifier')\n",
    "mlc_model.fit(X_train,\n",
    "              y_train,\n",
    "              epochs=200,\n",
    "              batch_size=1000, \n",
    "              validation_data=(X_test, y_test),\n",
    "              callbacks=[earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr(x):\n",
    "    return np.exp(-0.5*( (x - mu)**2 - (x + mu)**2))\n",
    "\n",
    "def bce_lr(x):\n",
    "    f = bce_model.predict(x)\n",
    "    return np.squeeze(f / (1. - f))\n",
    "\n",
    "def mse_lr(x):\n",
    "    f = mse_model.predict(x)\n",
    "    return np.squeeze(f / (1. - f))\n",
    "\n",
    "def sqr_lr(x):\n",
    "    f = sqr_model.predict(x)\n",
    "    return np.squeeze(f)\n",
    "\n",
    "def mlc_lr(x):\n",
    "    f = mlc_model.predict(x)\n",
    "    return np.squeeze(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to make the below plots, but with average neural networks instead of a particular one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.linspace(-6, 6, 1000)\n",
    "\n",
    "fig, ax_1 = plt.subplots(figsize = (10, 6))\n",
    "\n",
    "plt.plot(xs, lr(xs), label = 'Exact', c = 'k', ls='-')\n",
    "plt.plot(xs, bce_lr(xs), label = 'BCE', c = 'brown', ls=':')\n",
    "plt.plot(xs, mse_lr(xs), label = 'MSE', c = 'green', ls='--')\n",
    "plt.plot(xs, sqr_lr(xs), label = 'SQR', c = 'blue', ls='-.')\n",
    "plt.plot(xs, mlc_lr(xs), label = 'MLC', c = 'red', ls='--')\n",
    "plt.legend()\n",
    "ax_1.minorticks_on()\n",
    "ax_1.tick_params(direction='in', which='both',length=5)\n",
    "plt.ylabel('Likelihood Ratio')\n",
    "\n",
    "ax_2 = ax_1.twinx()\n",
    "bins = np.linspace(-6, 6, 100)\n",
    "plt.hist(sig, alpha=0.1, bins=bins)\n",
    "plt.hist(bgd, alpha=0.1, bins=bins)\n",
    "ax_2.minorticks_on()\n",
    "ax_2.tick_params(direction='in', which='both',length=5)\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.xlim(-6, 6)\n",
    "plt.xlabel(r'$x$')\n",
    "plt.title(r\"$\\mu_{\\rm{sgnl}}=\"+str(mu)+r\", \\mu_{\\rm{bkgd}}=\"+str(-mu)+r\"$\",loc=\"right\",fontsize=20);\n",
    "plt.savefig('plots/lrs.png', dpi=1200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_1 = plt.subplots(figsize = (10, 6))\n",
    "\n",
    "plt.plot(xs, bce_lr(xs) / lr(xs), label = 'BCE', c = 'brown', ls=':')\n",
    "plt.plot(xs, mse_lr(xs) / lr(xs), label = 'MSE', c = 'green', ls='--')\n",
    "plt.plot(xs, sqr_lr(xs) / lr(xs), label = 'SQR', c = 'blue', ls='-.')\n",
    "plt.plot(xs, mlc_lr(xs) / lr(xs), label = 'MLC', c = 'red', ls='--')\n",
    "plt.axhline(1,ls=\":\",color=\"grey\", lw=0.5)\n",
    "plt.axvline(0,ls=\":\",color=\"grey\", lw=0.5)\n",
    "plt.legend()\n",
    "ax_1.minorticks_on()\n",
    "ax_1.tick_params(direction='in', which='both',length=5)\n",
    "plt.ylim(0.94, 1.06)\n",
    "plt.ylabel('Ratio')\n",
    "\n",
    "ax_2 = ax_1.twinx()\n",
    "bins = np.linspace(-6, 6, 100)\n",
    "plt.hist(sig, alpha=0.1, bins=bins)\n",
    "plt.hist(bgd, alpha=0.1, bins=bins)\n",
    "ax_2.minorticks_on()\n",
    "ax_2.tick_params(direction='in', which='both',length=5)\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.xlim(-6, 6)\n",
    "plt.xlabel(r'$x$')\n",
    "plt.title(r\"$\\mu_{\\rm{sgnl}}=\"+str(mu)+r\", \\mu_{\\rm{bkgd}}=\"+str(-mu)+r\"$\",loc=\"right\",fontsize=20);\n",
    "plt.savefig('plots/lr_ratios.png', dpi=1200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10**np.arange(1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ns = 10**np.arange(1, 10)\n",
    "mu = 0.1\n",
    "earlystopping = EarlyStopping(patience=10,\n",
    "                              verbose=1,\n",
    "                              restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bce_models = []\n",
    "mse_models = []\n",
    "sqr_models = []\n",
    "mlc_models = []\n",
    "\n",
    "for N in Ns:\n",
    "    # Generate data\n",
    "    bgd = np.random.normal(-mu, 1, N)\n",
    "    sig = np.random.normal(mu, 1, N)\n",
    "    X = np.concatenate([bgd, sig])\n",
    "    y = np.concatenate([np.zeros(N), np.ones(N)])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "    # BCE\n",
    "    bce_inputs = Input((1, ))\n",
    "    bce_layer_1 = Dense(64, activation='relu')(bce_inputs)\n",
    "    bce_layer_2 = Dense(128, activation='relu')(bce_layer_1)\n",
    "    bce_layer_3 = Dense(64, activation='relu')(bce_layer_2)\n",
    "    bce_outputs = Dense(1, activation='sigmoid')(bce_layer_3)\n",
    "\n",
    "    bce_model = Model(inputs=bce_inputs, outputs=bce_outputs)\n",
    "    bce_model.compile(loss=bce,\n",
    "                      optimizer='adam',\n",
    "                      metrics='accuracy')\n",
    "    \n",
    "    bce_history = bce_model.fit(X_train,\n",
    "                                y_train,\n",
    "                                epochs=200,\n",
    "                                batch_size=int(0.1*N), \n",
    "                                validation_data=(X_test, y_test),\n",
    "                                callbacks=[earlystopping])\n",
    "    bce_models.append(bce_model)\n",
    "    \n",
    "    # MSE\n",
    "    mse_inputs = Input((1, ))\n",
    "    mse_layer_1 = Dense(50, activation='relu')(mse_inputs)\n",
    "    mse_layer_2 = Dense(50, activation='relu')(mse_layer_1)\n",
    "    mse_layer_3 = Dense(50, activation='relu')(mse_layer_2)\n",
    "    mse_outputs = Dense(1, activation='sigmoid')(mse_layer_3)\n",
    "\n",
    "    mse_model = Model(inputs=mse_inputs, outputs=mse_outputs)\n",
    "    mse_model.compile(loss=mse,\n",
    "                      optimizer='adam',\n",
    "                      metrics='accuracy')\n",
    "    \n",
    "    mse_history = mse_model.fit(X_train,\n",
    "                                y_train,\n",
    "                                epochs=200,\n",
    "                                batch_size=int(0.1*N), \n",
    "                                validation_data=(X_test, y_test),\n",
    "                                callbacks=[earlystopping])\n",
    "    mse_models.append(mse_model)\n",
    "    \n",
    "    # SQR\n",
    "    while True:\n",
    "        sqr_inputs = Input((1, ))\n",
    "        sqr_layer_1 = Dense(50, activation='relu')(sqr_inputs)\n",
    "        sqr_layer_2 = Dense(50, activation='relu')(sqr_layer_1)\n",
    "        sqr_layer_3 = Dense(50, activation='relu')(sqr_layer_2)\n",
    "        sqr_outputs = Dense(1, activation='relu')(sqr_layer_3)\n",
    "\n",
    "        sqr_model = Model(inputs=sqr_inputs, outputs=sqr_outputs)\n",
    "        sqr_model.compile(loss=sqr,\n",
    "                          optimizer=RMSprop(lr=1e-6, momentum=0.5),\n",
    "                          metrics='accuracy')\n",
    "\n",
    "        sqr_history = mse_model.fit(X_train,\n",
    "                                    y_train,\n",
    "                                    epochs=200,\n",
    "                                    batch_size=int(0.1*N), \n",
    "                                    validation_data=(X_test, y_test),\n",
    "                                    callbacks=[earlystopping])\n",
    "        \n",
    "        if sqr_history.history['val_loss'][-1] < 1.1:\n",
    "            sqr_models.append(mse_model)\n",
    "            break\n",
    "    \n",
    "    # MLC\n",
    "    while True:\n",
    "        mlc_inputs = Input((1, ))\n",
    "        mlc_layer_1 = Dense(50, activation='relu')(mlc_inputs)\n",
    "        mlc_layer_2 = Dense(50, activation='relu')(mlc_layer_1)\n",
    "        mlc_layer_3 = Dense(50, activation='relu')(mlc_layer_2)\n",
    "        mlc_outputs = Dense(1, activation='relu')(mlc_layer_3)\n",
    "\n",
    "        mlc_model = Model(inputs=mlc_inputs, outputs=mlc_outputs)\n",
    "        mlc_model.compile(loss=mlc,\n",
    "                          optimizer='rmsprop',\n",
    "                          metrics='accuracy')\n",
    "\n",
    "        mlc_history = mlc_model.fit(X_train,\n",
    "                                    y_train,\n",
    "                                    epochs=200,\n",
    "                                    batch_size=int(0.1*N), \n",
    "                                    validation_data=(X_test, y_test),\n",
    "                                    callbacks=[earlystopping])\n",
    "        \n",
    "        if mlc_history.history['val_loss'][-1] < 0.1:\n",
    "            mlc_models.append(mlc_model)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bce_lr(model):\n",
    "    def model_bce_lr(x):\n",
    "        f = model.predict(x)\n",
    "        return np.squeeze(f / (1. - f))\n",
    "    return model_bce_lr\n",
    "\n",
    "def get_mse_lr(model):\n",
    "    def model_mse_lr(x):\n",
    "        f = model.predict(x)\n",
    "        return np.squeeze(f / (1. - f))\n",
    "    return model_mse_lr\n",
    "\n",
    "def get_sqr_lr(model):\n",
    "    def model_sqr_lr(x):\n",
    "        f = model.predict(x)\n",
    "        return np.squeeze(f)\n",
    "    return model_sqr_lr\n",
    "\n",
    "def get_mlc_lr(model):\n",
    "    def model_mlc_lr(x):\n",
    "        f = model.predict(x)\n",
    "        return np.squeeze(f)\n",
    "    return model_mlc_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean absolute errors.\n",
    "\n",
    "N = 10**6\n",
    "\n",
    "bgd = np.random.normal(-mu, 1, N)\n",
    "sig = np.random.normal(mu, 1, N)\n",
    "X = np.concatenate([bgd, sig])\n",
    "\n",
    "maes = []\n",
    "for i in range(len(Ns)):\n",
    "    model_bce_lr = get_bce_lr(bce_models[i])\n",
    "    bce_maes += [np.mean(np.abs(model_bce_lr(X) - lr(X)))]\n",
    "    \n",
    "    model_mse_lr = get_mse_lr(mse_models[i])\n",
    "    mse_maes += [np.mean(np.abs(model_mse_lr(X) - lr(X)))]\n",
    "    \n",
    "    model_sqr_lr = get_sqr_lr(sqr_models[i])\n",
    "    sqr_maes += [np.mean(np.abs(model_sqr_lr(X) - lr(X)))]\n",
    "    \n",
    "    model_mlc_lr = get_mlc_lr(mlc_models[i])\n",
    "    mlc_maes += [np.mean(np.abs(model_mlc_lr(X) - lr(X)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (10, 6))\n",
    "\n",
    "plt.plot(Ns, bce_maes, c = 'brown', label = 'BCE')\n",
    "plt.plot(Ns, mse_maes, c = 'green', label = 'MSE')\n",
    "plt.plot(Ns, sqr_maes, c = 'blue', label = 'SQR')\n",
    "plt.plot(Ns, mlc_maes, c = 'red', label = 'MLC')\n",
    "\n",
    "plt.xscale(\"log\", basex=10)\n",
    "plt.minorticks_on()\n",
    "plt.tick_params(direction='in', which='both',length=5)\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.xlabel(r'$N$')\n",
    "\n",
    "plt.title(r\"$\\mu_{\\rm{sgnl}}=\"+str(mu)+r\", \\mu_{\\rm{bkgd}}=\"+str(-mu)+r\"$\",loc=\"right\",fontsize=20);\n",
    "plt.savefig('plots/ns_lr_ratios.png', dpi=1200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, model_from_json\n",
    "from tensorflow.keras import Sequential\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras.layers import Lambda, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "    \n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import gridspec\n",
    "from matplotlib import patches\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"serif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLC loss\n",
    "\n",
    "N = 100000\n",
    "\n",
    "theta0 = 0.1\n",
    "\n",
    "X_MC = np.random.normal(-theta0,1,N)\n",
    "X_data = np.random.normal(theta0,1,N)\n",
    "X_MLE_train, X_MLE_val, Y_MLE_train, Y_MLE_val = train_test_split(np.concatenate([X_data,X_MC]), np.concatenate([np.ones(len(X_data)),np.zeros(len(X_MC))]), test_size=0.5)\n",
    "\n",
    "\n",
    "def CustomLoss2(y_true, y_pred):\n",
    "    return -y_true*K.log(y_pred**2+0.00000001) + (1.-y_true)*y_pred**2\n",
    "\n",
    "def CustomLoss(y_true, y_pred):\n",
    "    return -y_true*K.log(y_pred+0.00000001) + (1.-y_true)*y_pred\n",
    "\n",
    "model_MLE2 = Sequential()\n",
    "model_MLE2.add(Dense(64, activation='relu',input_shape =(1,))) \n",
    "model_MLE2.add(Dense(128, activation='relu'))\n",
    "model_MLE2.add(Dense(64, activation='relu'))\n",
    "model_MLE2.add(Dense(1, activation='linear')) #was sigmoid\n",
    "model_MLE2.compile(loss=lambda y_true, y_pred: CustomLoss2(y_true, y_pred), optimizer='adam', metrics=['accuracy'])\n",
    "hist_MLE2 = model_MLE2.fit(X_MLE_train, Y_MLE_train, epochs=100, batch_size=int(0.01*len(X_MLE_train)),validation_data=(X_MLE_val, Y_MLE_val),callbacks=[earlystopping])\n",
    "\n",
    "model_MLE = Sequential()\n",
    "model_MLE.add(Dense(64, activation='relu',input_shape =(1,))) \n",
    "model_MLE.add(Dense(128, activation='relu'))\n",
    "model_MLE.add(Dense(64, activation='relu'))\n",
    "model_MLE.add(Dense(1, activation='relu')) #was sigmoid\n",
    "model_MLE.compile(loss=lambda y_true, y_pred: CustomLoss(y_true, y_pred), optimizer='adam', metrics=['accuracy'])\n",
    "hist_MLE = model_MLE.fit(X_MLE_train, Y_MLE_train, epochs=100, batch_size=int(0.01*len(X_MLE_train)),validation_data=(X_MLE_val, Y_MLE_val),callbacks=[earlystopping])\n",
    "\n",
    "model_BCE = Sequential()\n",
    "model_BCE.add(Dense(64, activation='relu',input_shape =(1,))) \n",
    "model_BCE.add(Dense(128, activation='relu'))\n",
    "model_BCE.add(Dense(64, activation='relu'))\n",
    "model_BCE.add(Dense(1, activation='sigmoid'))\n",
    "model_BCE.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
    "hist_BCE = model_BCE.fit(X_MLE_train, Y_MLE_train, epochs=100, batch_size=int(0.01*len(X_MLE_train)),validation_data=(X_MLE_val, Y_MLE_val),callbacks=[earlystopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6))\n",
    "gs = gridspec.GridSpec(1, 1) \n",
    "ax1 = plt.subplot(gs[0])\n",
    "ax1.yaxis.set_ticks_position('both')\n",
    "ax1.xaxis.set_ticks_position('both')\n",
    "ax1.tick_params(direction=\"in\",which=\"both\")\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "ax1.minorticks_on()\n",
    "\n",
    "xx = np.linspace(-3,3,100)\n",
    "preds_BCE = model_BCE.predict(xx)\n",
    "plotBCE = plt.plot(xx,preds_BCE/(1.-preds_BCE),label=\"NN (BCE)\",ls=\":\",lw=5,color='green')\n",
    "preds_test2 = model_MLE2.predict(xx)**2\n",
    "plot12 = plt.plot(xx,preds_test2,label=\"NN (MLC, squared)\",ls=\":\",color='green')\n",
    "preds_test = model_MLE.predict(xx)\n",
    "plot11 = plt.plot(xx,preds_test,label=\"NN (MLC, linear)\",ls=\"-\",color='green')\n",
    "plot2 = plt.plot(xx,np.exp(-((xx-theta0)**2-(xx+theta0)**2)/(2*1**2)),label=\"exact\",color='orange')\n",
    "plt.ylim([0,2])\n",
    "plt.axvline(0,ls=\":\",color=\"grey\")\n",
    "plt.axhline(1,ls=\":\",color=\"grey\")\n",
    "plt.xlabel(r\"$x$\",fontsize=20)\n",
    "plt.ylabel(\"likelihood ratio\",fontsize=20)\n",
    "plt.title(r\"$\\mu_{MC} = \"+str(theta0)+r\", \\mu_{data}=0.5$\",loc=\"right\",fontsize=20)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "_,_,_=plt.hist(X_MLE_train[Y_MLE_train==0],bins=np.linspace(-3,3,50),alpha=0.3,label=\"Back.\",color='red')\n",
    "_,_,_=plt.hist(X_MLE_train[Y_MLE_train==1],bins=np.linspace(-3,3,50),alpha=0.3,label=\"Signal\")\n",
    "ax2.tick_params(axis='y', labelcolor=\"red\")\n",
    "ax2.set_ylabel(\"histogram\",color='red',fontsize=20)\n",
    "\n",
    "for label in ax2.yaxis.get_majorticklabels():\n",
    "        label.set_fontsize(20)\n",
    "\n",
    "leg = plt.legend([plotBCE[0], plot11[0],plot12[0],plot2[0]],['BCE', 'MLC, linear','MLC, squared','Exact'], loc=\"upper left\",frameon=False,fontsize=18)\n",
    "plt.legend(frameon=False,fontsize=20)\n",
    "plt.gca().add_artist(leg)\n",
    "\n",
    "#plt.savefig(\"ensembleLearnPlots/MLCloss.pdf\",bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omnifold",
   "language": "python",
   "name": "omifold"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
