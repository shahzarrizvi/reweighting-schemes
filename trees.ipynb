{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7ced574-924b-4fd3-9444-614365da7fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically reload modules after executing each cell.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d2777dc-b825-4800-bc82-d1b23953c263",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# General imports\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from joblib import dump, load \n",
    "\n",
    "# Utility imports\n",
    "from utils.losses import *\n",
    "from utils.plotting import *\n",
    "from utils.training import *\n",
    "\n",
    "np.random.seed(666) # Need to do more to ensure data is the same across runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18c301cf-f760-4479-aeef-27c5a063035f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # pick a number < 4 on ML4HEP; < 3 on Voltan \n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eff551-de89-49c5-a2c0-a1c15ae11183",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Vertical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d526a150-2df1-4284-b6da-d1b4959d5352",
   "metadata": {
    "tags": []
   },
   "source": [
    "## $d = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cae75b-8b8b-44bb-a06d-78c677817809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "num = 0\n",
    "reps = 100\n",
    "d = 1\n",
    "Ns = 10**np.arange(2, 8)\n",
    "\n",
    "# Model parameters\n",
    "bce_params = {'loss':bce, 'd': d}\n",
    "\n",
    "filestr = 'models/trees/{}/set_{}/'.format(d, num)\n",
    "bce_filestr = filestr + 'bce/model_{}_{}.h5'\n",
    "bdt_filestr = filestr + 'bdt/model_{}.h5'\n",
    "gbc_filestr = filestr + 'gbc/model_{}_{}.h5'\n",
    "\n",
    "if not os.path.isdir(filestr):\n",
    "    os.mkdir(filestr)\n",
    "\n",
    "if not os.path.isdir(filestr + 'bce/'):\n",
    "    os.mkdir(filestr + 'bce/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'bdt/'):\n",
    "    os.mkdir(filestr + 'bdt/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'gbc/'):\n",
    "    os.mkdir(filestr + 'gbc/')\n",
    "\n",
    "# Data parameters\n",
    "X = np.load('data/trees/{}/X_trn.npy'.format(d)).reshape(-1, 1)\n",
    "y = np.load('data/trees/{}/y_trn.npy'.format(d)).astype('float32')\n",
    "\n",
    "bkgd = stats.norm(-0.1, 1)\n",
    "sgnl = stats.norm(+0.1, 1)\n",
    "\n",
    "lr = make_lr(bkgd, sgnl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c89e58-6e56-4b84-9385-049327bed049",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10**6\n",
    "data, m, s = split_data(X[:N], y[:N])\n",
    "\n",
    "bce_lrs = [None] * reps\n",
    "gbc_lrs = [None] * reps\n",
    "for i in range(reps):\n",
    "    print(i, end = ' ')\n",
    "    bce_model = create_model(**bce_params)\n",
    "    bce_model.load_weights(bce_filestr.format(N, i))\n",
    "    bce_lrs[i] = odds_lr(bce_model, m, s)\n",
    "\n",
    "    gbc_model = load(gbc_filestr.format(N, i))\n",
    "    gbc_lrs[i] = tree_lr(gbc_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5856c227-271c-440f-95d0-3e93becf92e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.linspace(-6, 6, 1201).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ca7c19-d2bc-4bc6-b872-8a44b142272f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bce_preds = get_preds(bce_lrs, xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19a6431-0b68-4d35-a8c3-8c37c3ea4db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc_preds = get_preds(gbc_lrs, xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b130a1cc-e58a-4743-9016-e06387cf46bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_bce = bce_preds.mean(axis = 0)\n",
    "avg_gbc = gbc_preds.mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09e0a31-4089-402d-ab48-f62fb0d35123",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_plot([bce_preds, gbc_preds], ['BCE', 'GBC'], lr, xs.reshape(-1), \n",
    "           figsize = (w, h), title = '\\it Likelihood Ratio Models', \n",
    "           filename = 'plots/lr_models.png') "
   ]
  },
  {
   "cell_type": "raw",
   "id": "c2945cb9-735d-4029-a433-32efe7aa5f8b",
   "metadata": {},
   "source": [
    "for N in Ns:\n",
    "    print('===================================================\\n{}'.format(N))\n",
    "    # Take the first N samples.\n",
    "    data, m, s = split_data(X[:N], y[:N])\n",
    "    \n",
    "    # Train BDT model (only need to train 1)\n",
    "    #bdt_model = XGBClassifier(early_stopping_rounds = 10)\n",
    "    #X_trn, X_vld, y_trn, y_vld = data\n",
    "    #bdt_model.fit(X_trn, y_trn, eval_set = [(X_vld, y_vld)], verbose = 0)\n",
    "    #trace = bdt_model.evals_result()['validation_0']\n",
    "    #print(trace['logloss'][-1], '\\t', len(trace['logloss']), end = '\\n')\n",
    "    #bdt_model.save_model(bdt_filestr.format(N))\n",
    "\n",
    "    for i in range(reps):\n",
    "        print(i, end = ' ')\n",
    "        # Train BCE model\n",
    "        #bce_model, trace = train(data, **bce_params)\n",
    "        #bce_model.save_weights(bce_filestr.format(N, i))\n",
    "        \n",
    "        # Train GBC model\n",
    "        gbc_model = GradientBoostingClassifier(validation_fraction = 0.25,\n",
    "                                               n_iter_no_change = 10)\n",
    "        gbc_model.fit(X[:N], y[:N])\n",
    "        dump(gbc_model, gbc_filestr.format(N, i))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78d6730-4b33-40bd-8abb-acf1be1d2f30",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## $d = 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12422843-2cf2-4280-b1b4-aae3127e5cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "num = 0\n",
    "reps = 100\n",
    "d = 2\n",
    "Ns = 10**np.arange(2, 8)\n",
    "\n",
    "# Model parameters\n",
    "bce_params = {'loss':bce, 'd': d}\n",
    "\n",
    "filestr = 'models/trees/{}/set_{}/'.format(d, num)\n",
    "bce_filestr = filestr + 'bce/model_{}_{}.h5'\n",
    "bdt_filestr = filestr + 'bdt/model_{}.h5'\n",
    "gbc_filestr = filestr + 'gbc/model_{}_{}.h5'\n",
    "\n",
    "if not os.path.isdir(filestr):\n",
    "    os.mkdir(filestr)\n",
    "\n",
    "if not os.path.isdir(filestr + 'bce/'):\n",
    "    os.mkdir(filestr + 'bce/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'bdt/'):\n",
    "    os.mkdir(filestr + 'bdt/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'gbc/'):\n",
    "    os.mkdir(filestr + 'gbc/')\n",
    "\n",
    "# Data parameters\n",
    "X = np.load('data/trees/{}/X_trn.npy'.format(d))\n",
    "y = np.load('data/trees/{}/y_trn.npy'.format(d)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1a1f4c-7d62-4617-9eaf-1c2ab3f56bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for N in Ns:\n",
    "    print('===================================================\\n{}'.format(N))\n",
    "    # Take the first N samples.\n",
    "    data, m, s = split_data(X[:N], y[:N])\n",
    "    \n",
    "    # Train BDT model (only need to train 1)\n",
    "    #bdt_model = XGBClassifier(early_stopping_rounds = 10)\n",
    "    #X_trn, X_vld, y_trn, y_vld = data\n",
    "    #bdt_model.fit(X_trn, y_trn, eval_set = [(X_vld, y_vld)], verbose = 0)\n",
    "    #trace = bdt_model.evals_result()['validation_0']\n",
    "    #print(trace['logloss'][-1], '\\t', len(trace['logloss']), end = '\\n')\n",
    "    #bdt_model.save_model(bdt_filestr.format(N))\n",
    "\n",
    "    for i in range(reps):\n",
    "        print(i, end = ' ')\n",
    "        # Train BCE model\n",
    "        #bce_model, trace = train(data, **bce_params)\n",
    "        #bce_model.save_weights(bce_filestr.format(N, i))\n",
    "        \n",
    "        # Train GBC model\n",
    "        gbc_model = GradientBoostingClassifier(validation_fraction = 0.25,\n",
    "                                               n_iter_no_change = 10)\n",
    "        gbc_model.fit(X[:N], y[:N])\n",
    "        dump(gbc_model, gbc_filestr.format(N, i))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d942469-7ef3-4907-87eb-c9296e3409bb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## $d=4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9f85bd-b60b-473c-bcc8-373a9d120f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "num = 0\n",
    "reps = 100\n",
    "d = 4\n",
    "Ns = 10**np.arange(2, 8)\n",
    "\n",
    "# Model parameters\n",
    "bce_params = {'loss':bce, 'd': d}\n",
    "\n",
    "filestr = 'models/trees/{}/set_{}/'.format(d, num)\n",
    "bce_filestr = filestr + 'bce/model_{}_{}.h5'\n",
    "bdt_filestr = filestr + 'bdt/model_{}.h5'\n",
    "gbc_filestr = filestr + 'gbc/model_{}_{}.h5'\n",
    "\n",
    "if not os.path.isdir(filestr):\n",
    "    os.mkdir(filestr)\n",
    "\n",
    "if not os.path.isdir(filestr + 'bce/'):\n",
    "    os.mkdir(filestr + 'bce/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'bdt/'):\n",
    "    os.mkdir(filestr + 'bdt/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'gbc/'):\n",
    "    os.mkdir(filestr + 'gbc/')\n",
    "\n",
    "# Data parameters\n",
    "X = np.load('data/trees/{}/X_trn.npy'.format(d))\n",
    "y = np.load('data/trees/{}/y_trn.npy'.format(d)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb3e63d-4966-43eb-af89-f6ae1d3370eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for N in Ns:\n",
    "    print('===================================================\\n{}'.format(N))\n",
    "    # Take the first N samples.\n",
    "    data, m, s = split_data(X[:N], y[:N])\n",
    "    \n",
    "    # Train BDT model (only need to train 1)\n",
    "    #bdt_model = XGBClassifier(early_stopping_rounds = 10)\n",
    "    #X_trn, X_vld, y_trn, y_vld = data\n",
    "    #bdt_model.fit(X_trn, y_trn, eval_set = [(X_vld, y_vld)], verbose = 0)\n",
    "    #trace = bdt_model.evals_result()['validation_0']\n",
    "    #print(trace['logloss'][-1], '\\t', len(trace['logloss']), end = '\\n')\n",
    "    #bdt_model.save_model(bdt_filestr.format(N))\n",
    "\n",
    "    for i in range(91, reps):\n",
    "        print(i, end = ' ')\n",
    "        # Train BCE model\n",
    "        #bce_model, trace = train(data, **bce_params)\n",
    "        #bce_model.save_weights(bce_filestr.format(N, i))\n",
    "        \n",
    "        # Train GBC model\n",
    "        gbc_model = GradientBoostingClassifier(validation_fraction = 0.25,\n",
    "                                               n_iter_no_change = 10)\n",
    "        gbc_model.fit(X[:N], y[:N])\n",
    "        dump(gbc_model, gbc_filestr.format(N, i))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1b039a-fa91-4a35-8233-c4a017babc44",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## $d=8$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe81baf6-0352-4b70-bcc8-ff37baeac13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "num = 0\n",
    "reps = 100\n",
    "d = 8\n",
    "Ns = 10**np.arange(2, 8)\n",
    "\n",
    "# Model parameters\n",
    "bce_params = {'loss':bce, 'd': d}\n",
    "\n",
    "filestr = 'models/trees/{}/set_{}/'.format(d, num)\n",
    "bce_filestr = filestr + 'bce/model_{}_{}.h5'\n",
    "bdt_filestr = filestr + 'bdt/model_{}.h5'\n",
    "gbc_filestr = filestr + 'gbc/model_{}_{}.h5'\n",
    "\n",
    "if not os.path.isdir(filestr):\n",
    "    os.mkdir(filestr)\n",
    "\n",
    "if not os.path.isdir(filestr + 'bce/'):\n",
    "    os.mkdir(filestr + 'bce/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'bdt/'):\n",
    "    os.mkdir(filestr + 'bdt/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'gbc/'):\n",
    "    os.mkdir(filestr + 'gbc/')\n",
    "\n",
    "# Data parameters\n",
    "X = np.load('data/trees/{}/X_trn.npy'.format(d))\n",
    "y = np.load('data/trees/{}/y_trn.npy'.format(d)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e72def4-b63c-46e2-92d5-1d724b5d079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for N in Ns:\n",
    "    print('===================================================\\n{}'.format(N))\n",
    "    # Take the first N samples.\n",
    "    data, m, s = split_data(X[:N], y[:N])\n",
    "    \n",
    "    # Train BDT model (only need to train 1)\n",
    "    #bdt_model = XGBClassifier(early_stopping_rounds = 10)\n",
    "    #X_trn, X_vld, y_trn, y_vld = data\n",
    "    #bdt_model.fit(X_trn, y_trn, eval_set = [(X_vld, y_vld)], verbose = 0)\n",
    "    #trace = bdt_model.evals_result()['validation_0']\n",
    "    #print(trace['logloss'][-1], '\\t', len(trace['logloss']), end = '\\n')\n",
    "    #bdt_model.save_model(bdt_filestr.format(N))\n",
    "\n",
    "    for i in range(reps):\n",
    "        print(i, end = ' ')\n",
    "        # Train BCE model\n",
    "        #bce_model, trace = train(data, **bce_params)\n",
    "        #bce_model.save_weights(bce_filestr.format(N, i))\n",
    "        \n",
    "        # Train GBC model\n",
    "        gbc_model = GradientBoostingClassifier(validation_fraction = 0.25,\n",
    "                                               n_iter_no_change = 10)\n",
    "        gbc_model.fit(X[:N], y[:N])\n",
    "        dump(gbc_model, gbc_filestr.format(N, i))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7b33f6-be09-404b-bbca-7e567a651b1b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## $d=16$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed78be7a-1c20-4cde-bd24-d5a46012377e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "num = 0\n",
    "reps = 100\n",
    "d = 16\n",
    "Ns = 10**np.arange(2, 8)\n",
    "\n",
    "# Model parameters\n",
    "bce_params = {'loss':bce, 'd': d}\n",
    "\n",
    "filestr = 'models/trees/{}/set_{}/'.format(d, num)\n",
    "bce_filestr = filestr + 'bce/model_{}_{}.h5'\n",
    "bdt_filestr = filestr + 'bdt/model_{}.h5'\n",
    "gbc_filestr = filestr + 'gbc/model_{}_{}.h5'\n",
    "\n",
    "if not os.path.isdir(filestr):\n",
    "    os.mkdir(filestr)\n",
    "\n",
    "if not os.path.isdir(filestr + 'bce/'):\n",
    "    os.mkdir(filestr + 'bce/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'bdt/'):\n",
    "    os.mkdir(filestr + 'bdt/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'gbc/'):\n",
    "    os.mkdir(filestr + 'gbc/')\n",
    "\n",
    "# Data parameters\n",
    "X = np.load('data/trees/{}/X_trn.npy'.format(d))\n",
    "y = np.load('data/trees/{}/y_trn.npy'.format(d)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5262859-deb7-413f-b674-9c84a157ad73",
   "metadata": {},
   "outputs": [],
   "source": [
    "for N in Ns:\n",
    "    print('===================================================\\n{}'.format(N))\n",
    "    # Take the first N samples.\n",
    "    data, m, s = split_data(X[:N], y[:N])\n",
    "    \n",
    "    # Train BDT model (only need to train 1)\n",
    "    #bdt_model = XGBClassifier(early_stopping_rounds = 10)\n",
    "    #X_trn, X_vld, y_trn, y_vld = data\n",
    "    #bdt_model.fit(X_trn, y_trn, eval_set = [(X_vld, y_vld)], verbose = 0)\n",
    "    #trace = bdt_model.evals_result()['validation_0']\n",
    "    #print(trace['logloss'][-1], '\\t', len(trace['logloss']), end = '\\n')\n",
    "    #bdt_model.save_model(bdt_filestr.format(N))\n",
    "\n",
    "    for i in range(reps):\n",
    "        print(i, end = ' ')\n",
    "        # Train BCE model\n",
    "        #bce_model, trace = train(data, **bce_params)\n",
    "        #bce_model.save_weights(bce_filestr.format(N, i))\n",
    "        \n",
    "        # Train GBC model\n",
    "        gbc_model = GradientBoostingClassifier(validation_fraction = 0.25,\n",
    "                                               n_iter_no_change = 10)\n",
    "        gbc_model.fit(X[:N], y[:N])\n",
    "        dump(gbc_model, gbc_filestr.format(N, i))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ca2309-9077-432f-933c-77be9ede35e1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## $d = 32$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f08136-c5dc-4649-989b-4f4cea166cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "num = 0\n",
    "reps = 100\n",
    "d = 32\n",
    "Ns = 10**np.arange(2, 8)\n",
    "\n",
    "# Model parameters\n",
    "bce_params = {'loss':bce, 'd': d}\n",
    "\n",
    "filestr = 'models/trees/{}/set_{}/'.format(d, num)\n",
    "bce_filestr = filestr + 'bce/model_{}_{}.h5'\n",
    "bdt_filestr = filestr + 'bdt/model_{}.h5'\n",
    "gbc_filestr = filestr + 'gbc/model_{}_{}.h5'\n",
    "\n",
    "if not os.path.isdir(filestr):\n",
    "    os.mkdir(filestr)\n",
    "\n",
    "if not os.path.isdir(filestr + 'bce/'):\n",
    "    os.mkdir(filestr + 'bce/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'bdt/'):\n",
    "    os.mkdir(filestr + 'bdt/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'gbc/'):\n",
    "    os.mkdir(filestr + 'gbc/')\n",
    "\n",
    "# Data parameters\n",
    "X = np.load('data/trees/{}/X_trn.npy'.format(d))\n",
    "y = np.load('data/trees/{}/y_trn.npy'.format(d)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f9e44b-a9fb-49c9-923d-9c414e2c6a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "for N in Ns:\n",
    "    print('===================================================\\n{}'.format(N))\n",
    "    # Take the first N samples.\n",
    "    data, m, s = split_data(X[:N], y[:N])\n",
    "    \n",
    "    # Train BDT model (only need to train 1)\n",
    "    #bdt_model = XGBClassifier(early_stopping_rounds = 10)\n",
    "    #X_trn, X_vld, y_trn, y_vld = data\n",
    "    #bdt_model.fit(X_trn, y_trn, eval_set = [(X_vld, y_vld)], verbose = 0)\n",
    "    #trace = bdt_model.evals_result()['validation_0']\n",
    "    #print(trace['logloss'][-1], '\\t', len(trace['logloss']), end = '\\n')\n",
    "    #bdt_model.save_model(bdt_filestr.format(N))\n",
    "\n",
    "    for i in range(reps):\n",
    "        print(i, end = ' ')\n",
    "        # Train BCE model\n",
    "        #bce_model, trace = train(data, **bce_params)\n",
    "        #bce_model.save_weights(bce_filestr.format(N, i))\n",
    "        \n",
    "        # Train GBC model\n",
    "        gbc_model = GradientBoostingClassifier(validation_fraction = 0.25,\n",
    "                                               n_iter_no_change = 10)\n",
    "        gbc_model.fit(X[:N], y[:N])\n",
    "        dump(gbc_model, gbc_filestr.format(N, i))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ed990a-9a86-46df-a2b5-45d88cdb0e88",
   "metadata": {},
   "source": [
    "# Zenodo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c7beb4-a137-4bfc-a7a5-5cbc00b6b5ee",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## $d = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "982a7583-5b0d-48b8-b4f1-c37fda6ba098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "num = 0\n",
    "reps = 100\n",
    "d = 1\n",
    "Ns = 10**np.arange(2, 8)\n",
    "\n",
    "# Model parameters\n",
    "bce_params = {'loss':bce, 'd': d}\n",
    "\n",
    "filestr = 'models/zenodo/{}/set_{}/'.format(d, num)\n",
    "bce_filestr = filestr + 'bce/model_{}_{}.h5'\n",
    "bdt_filestr = filestr + 'bdt/model_{}.h5'\n",
    "gbc_filestr = filestr + 'gbc/model_{}_{}.h5'\n",
    "\n",
    "if not os.path.isdir(filestr):\n",
    "    os.mkdir(filestr)\n",
    "\n",
    "if not os.path.isdir(filestr + 'bce/'):\n",
    "    os.mkdir(filestr + 'bce/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'bdt/'):\n",
    "    os.mkdir(filestr + 'bdt/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'gbc/'):\n",
    "    os.mkdir(filestr + 'gbc/')\n",
    "\n",
    "# Data parameters\n",
    "X = np.load('data/zenodo/{}/X_trn.npy'.format(d)).reshape(-1, 1)\n",
    "y = np.load('data/zenodo/{}/y_trn.npy'.format(d)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "faa8fdd1-bf3c-49dd-9e78-a2051e91857e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================\n",
      "10000000\n",
      "0.6910528750873685 \t 45\n",
      "87 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-10 19:32:06.351600: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-10 19:32:07.004419: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22243 MB memory:  -> device: 0, name: Quadro RTX 6000, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6913653612136841 \t 100\t\n",
      "88 0.6912950277328491 \t 100\t\n",
      "89 0.6913024187088013 \t 100\t\n",
      "90 0.6912943720817566 \t 100\t\n",
      "91 0.6913145780563354 \t 100\t\n",
      "92 0.691287100315094 \t 100\t\n",
      "93 0.6912788152694702 \t 100\t\n",
      "94 0.6912984848022461 \t 100\t\n",
      "95 0.6912990808486938 \t 100\t\n",
      "96 0.6913532018661499 \t 100\t\n",
      "97 0.6912767887115479 \t 100\t\n",
      "98 0.6912504434585571 \t 100\t\n",
      "99 0.6913511753082275 \t 100\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Ns = [10**7]\n",
    "for N in Ns:\n",
    "    print('===================================================\\n{}'.format(N))\n",
    "    # Take the first N samples.\n",
    "    data, m, s = split_data(X[:N], y[:N])\n",
    "    \n",
    "    # Train BDT model (only need to train 1)\n",
    "    bdt_model = XGBClassifier(early_stopping_rounds = 10)\n",
    "    X_trn, X_vld, y_trn, y_vld = data\n",
    "    bdt_model.fit(X_trn, y_trn, eval_set = [(X_vld, y_vld)], verbose = 0)\n",
    "    trace = bdt_model.evals_result()['validation_0']\n",
    "    print(trace['logloss'][-1], '\\t', len(trace['logloss']), end = '\\n')\n",
    "    bdt_model.save_model(bdt_filestr.format(N))\n",
    "\n",
    "    for i in range(87, reps):\n",
    "        print(i, end = ' ')\n",
    "        # Train BCE model\n",
    "        bce_model, trace = train(data, **bce_params)\n",
    "        bce_model.save_weights(bce_filestr.format(N, i))\n",
    "        \n",
    "        # Train GBC model\n",
    "        gbc_model = GradientBoostingClassifier(validation_fraction = 0.25,\n",
    "                                               n_iter_no_change = 10)\n",
    "        gbc_model.fit(X[:N], y[:N])\n",
    "        dump(gbc_model, gbc_filestr.format(N, i))\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf622c3-cfce-4a54-b8d5-2dca4a76c167",
   "metadata": {
    "tags": []
   },
   "source": [
    "## $d = 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c07e462f-21a5-4517-a390-57d7e4e498fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "num = 0\n",
    "reps = 100\n",
    "d = 2\n",
    "Ns = 10**np.arange(2, 8)\n",
    "\n",
    "# Model parameters\n",
    "bce_params = {'loss':bce, 'd': d}\n",
    "\n",
    "filestr = 'models/zenodo/{}/set_{}/'.format(d, num)\n",
    "bce_filestr = filestr + 'bce/model_{}_{}.h5'\n",
    "bdt_filestr = filestr + 'bdt/model_{}.h5'\n",
    "gbc_filestr = filestr + 'gbc/model_{}_{}.h5'\n",
    "\n",
    "if not os.path.isdir(filestr):\n",
    "    os.mkdir(filestr)\n",
    "\n",
    "if not os.path.isdir(filestr + 'bce/'):\n",
    "    os.mkdir(filestr + 'bce/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'bdt/'):\n",
    "    os.mkdir(filestr + 'bdt/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'gbc/'):\n",
    "    os.mkdir(filestr + 'gbc/')\n",
    "\n",
    "# Data parameters\n",
    "X = np.load('data/zenodo/{}/X_trn.npy'.format(d))\n",
    "y = np.load('data/zenodo/{}/y_trn.npy'.format(d)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "201c1e2a-1e85-4736-b009-fcac789b5d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================\n",
      "100\n",
      "0.8233430308103561 \t 11\n",
      "0 0.7209652066230774 \t 11\t\n",
      "1 0.7217194437980652 \t 11\t\n",
      "2 0.7222219705581665 \t 12\t\n",
      "3 0.718004584312439 \t 11\t\n",
      "4 0.7417144179344177 \t 12\t\n",
      "5 0.7358759045600891 \t 14\t\n",
      "6 0.7359951734542847 \t 11\t\n",
      "7 0.7287700176239014 \t 11\t\n",
      "8 0.7308948040008545 \t 12\t\n",
      "9 0.7268415689468384 \t 15\t\n",
      "10 0.7184034585952759 \t 11\t\n",
      "11 0.7192142605781555 \t 11\t\n",
      "12 0.7209107875823975 \t 11\t\n",
      "13 0.7087037563323975 \t 11\t\n",
      "14 0.7129851579666138 \t 11\t\n",
      "15 0.7363764047622681 \t 11\t\n",
      "16 0.7295403480529785 \t 11\t\n",
      "17 0.7196835279464722 \t 11\t\n",
      "18 0.7183862924575806 \t 11\t\n",
      "19 0.7125200629234314 \t 11\t\n",
      "20 0.7262614369392395 \t 11\t\n",
      "21 0.7214400768280029 \t 16\t\n",
      "22 0.7001100182533264 \t 11\t\n",
      "23 0.7420414090156555 \t 11\t\n",
      "24 0.7349517941474915 \t 11\t\n",
      "25 0.7225614190101624 \t 11\t\n",
      "26 0.7295057773590088 \t 15\t\n",
      "27 0.7269763350486755 \t 13\t\n",
      "28 0.7192058563232422 \t 11\t\n",
      "29 0.7252163887023926 \t 11\t\n",
      "30 0.7308338284492493 \t 11\t\n",
      "31 0.7255899310112 \t 13\t\n",
      "32 0.7219876050949097 \t 11\t\n",
      "33 0.755247950553894 \t 14\t\n",
      "34 0.7198649644851685 \t 11\t\n",
      "35 0.7083088159561157 \t 11\t\n",
      "36 0.7348257303237915 \t 11\t\n",
      "37 0.7233579754829407 \t 11\t\n",
      "38 0.7383167743682861 \t 17\t\n",
      "39 0.7172127366065979 \t 11\t\n",
      "40 0.7174245715141296 \t 11\t\n",
      "41 0.7378077507019043 \t 11\t\n",
      "42 0.7173307538032532 \t 11\t\n",
      "43 0.7129726409912109 \t 11\t\n",
      "44 0.7232579588890076 \t 12\t\n",
      "45 0.7350007891654968 \t 11\t\n",
      "46 0.7142124176025391 \t 17\t\n",
      "47 0.725799560546875 \t 12\t\n",
      "48 0.7091288566589355 \t 11\t\n",
      "49 0.7255232334136963 \t 18\t\n",
      "50 0.7124450206756592 \t 11\t\n",
      "51 0.7103350162506104 \t 12\t\n",
      "52 0.7271595597267151 \t 11\t\n",
      "53 0.7416749000549316 \t 13\t\n",
      "54 0.7273536920547485 \t 11\t\n",
      "55 0.7110562324523926 \t 11\t\n",
      "56 0.7132622599601746 \t 14\t\n",
      "57 0.7158377766609192 \t 17\t\n",
      "58 0.7214462757110596 \t 11\t\n",
      "59 0.7372337579727173 \t 11\t\n",
      "60 0.7306222319602966 \t 13\t\n",
      "61 0.7285439968109131 \t 11\t\n",
      "62 0.7082908153533936 \t 11\t\n",
      "63 0.7288287281990051 \t 11\t\n",
      "64 0.7169317007064819 \t 11\t\n",
      "65 0.7455218434333801 \t 11\t\n",
      "66 0.7217365503311157 \t 11\t\n",
      "67 0.7082273960113525 \t 20\t\n",
      "68 0.7220839858055115 \t 12\t\n",
      "69 0.7207582592964172 \t 11\t\n",
      "70 0.7231526970863342 \t 12\t\n",
      "71 0.7086288332939148 \t 11\t\n",
      "72 0.7267302870750427 \t 11\t\n",
      "73 0.7483752369880676 \t 16\t\n",
      "74 0.7147663235664368 \t 11\t\n",
      "75 0.7182238698005676 \t 11\t\n",
      "76 0.7087045907974243 \t 11\t\n",
      "77 0.7386077046394348 \t 11\t\n",
      "78 0.7225378155708313 \t 11\t\n",
      "79 0.7307295203208923 \t 11\t\n",
      "80 0.7253713011741638 \t 15\t\n",
      "81 0.7232562899589539 \t 11\t\n",
      "82 0.7343586683273315 \t 11\t\n",
      "83 0.7065547108650208 \t 11\t\n",
      "84 0.7288985252380371 \t 11\t\n",
      "85 0.7225865721702576 \t 13\t\n",
      "86 0.7081311941146851 \t 11\t\n",
      "87 0.7169759273529053 \t 12\t\n",
      "88 0.7152965068817139 \t 11\t\n",
      "89 0.7161121964454651 \t 11\t\n",
      "90 0.7299960851669312 \t 16\t\n",
      "91 0.7235885858535767 \t 11\t\n",
      "92 0.7227853536605835 \t 12\t\n",
      "93 0.7195500135421753 \t 12\t\n",
      "94 0.7306280732154846 \t 11\t\n",
      "95 0.719919741153717 \t 11\t\n",
      "96 0.7213841080665588 \t 16\t\n",
      "97 0.7081873416900635 \t 11\t\n",
      "98 0.7199434638023376 \t 11\t\n",
      "99 0.7099135518074036 \t 11\t\n",
      "\n",
      "===================================================\n",
      "1000\n",
      "0.732753623187542 \t 11\n",
      "0 0.6948107481002808 \t 11\t\n",
      "1 0.6993615031242371 \t 11\t\n",
      "2 0.6944943070411682 \t 12\t\n",
      "3 0.6946748495101929 \t 11\t\n",
      "4 0.6958831548690796 \t 11\t\n",
      "5 0.6975688338279724 \t 11\t\n",
      "6 0.6953232288360596 \t 12\t\n",
      "7 0.6968148350715637 \t 11\t\n",
      "8 0.6955082416534424 \t 11\t\n",
      "9 0.6969946622848511 \t 13\t\n",
      "10 0.695650041103363 \t 11\t\n",
      "11 0.6977493762969971 \t 14\t\n",
      "12 0.6953422427177429 \t 12\t\n",
      "13 0.6956336498260498 \t 11\t\n",
      "14 0.6971385478973389 \t 12\t\n",
      "15 0.6974973082542419 \t 11\t\n",
      "16 0.6959450244903564 \t 12\t\n",
      "17 0.699376106262207 \t 13\t\n",
      "18 0.698219895362854 \t 11\t\n",
      "19 0.6937142610549927 \t 11\t\n",
      "20 0.6967792510986328 \t 12\t\n",
      "21 0.6949476003646851 \t 11\t\n",
      "22 0.6937541365623474 \t 11\t\n",
      "23 0.6958654522895813 \t 12\t\n",
      "24 0.6964705586433411 \t 12\t\n",
      "25 0.6971873641014099 \t 12\t\n",
      "26 0.6932254433631897 \t 11\t\n",
      "27 0.6934124231338501 \t 11\t\n",
      "28 0.6953170895576477 \t 11\t\n",
      "29 0.6963600516319275 \t 11\t\n",
      "30 0.6944831013679504 \t 12\t\n",
      "31 0.6955702304840088 \t 14\t\n",
      "32 0.6956347823143005 \t 11\t\n",
      "33 0.6952999830245972 \t 12\t\n",
      "34 0.6937354803085327 \t 11\t\n",
      "35 0.6968916654586792 \t 11\t\n",
      "36 0.6941534280776978 \t 13\t\n",
      "37 0.6955576539039612 \t 12\t\n",
      "38 0.694701611995697 \t 11\t\n",
      "39 0.6948017477989197 \t 11\t\n",
      "40 0.6954548954963684 \t 11\t\n",
      "41 0.6963121294975281 \t 11\t\n",
      "42 0.6934837698936462 \t 11\t\n",
      "43 0.6976433396339417 \t 11\t\n",
      "44 0.6945923566818237 \t 11\t\n",
      "45 0.6932157278060913 \t 12\t\n",
      "46 0.6976507306098938 \t 22\t\n",
      "47 0.6937413811683655 \t 11\t\n",
      "48 0.6949679851531982 \t 11\t\n",
      "49 0.6960007548332214 \t 11\t\n",
      "50 0.6949648261070251 \t 12\t\n",
      "51 0.6974020004272461 \t 13\t\n",
      "52 0.6964055299758911 \t 13\t\n",
      "53 0.6929866075515747 \t 11\t\n",
      "54 0.6970444917678833 \t 11\t\n",
      "55 0.6955022811889648 \t 11\t\n",
      "56 0.6956332921981812 \t 11\t\n",
      "57 0.696212112903595 \t 12\t\n",
      "58 0.6966878771781921 \t 11\t\n",
      "59 0.6953113079071045 \t 11\t\n",
      "60 0.6947870850563049 \t 11\t\n",
      "61 0.6964371204376221 \t 11\t\n",
      "62 0.695368230342865 \t 11\t\n",
      "63 0.6933913230895996 \t 11\t\n",
      "64 0.6957492232322693 \t 11\t\n",
      "65 0.6947917342185974 \t 11\t\n",
      "66 0.694555401802063 \t 11\t\n",
      "67 0.6977879405021667 \t 11\t\n",
      "68 0.6953409910202026 \t 11\t\n",
      "69 0.6963444948196411 \t 12\t\n",
      "70 0.6948388814926147 \t 11\t\n",
      "71 0.6962236762046814 \t 12\t\n",
      "72 0.6947793960571289 \t 13\t\n",
      "73 0.6975152492523193 \t 11\t\n",
      "74 0.6936997175216675 \t 12\t\n",
      "75 0.6937520503997803 \t 12\t\n",
      "76 0.6939128637313843 \t 11\t\n",
      "77 0.6962245106697083 \t 11\t\n",
      "78 0.699064314365387 \t 11\t\n",
      "79 0.6967940330505371 \t 11\t\n",
      "80 0.6955403685569763 \t 11\t\n",
      "81 0.6949403285980225 \t 11\t\n",
      "82 0.6946344971656799 \t 12\t\n",
      "83 0.6954929828643799 \t 20\t\n",
      "84 0.6950691342353821 \t 11\t\n",
      "85 0.6960011124610901 \t 12\t\n",
      "86 0.694873571395874 \t 11\t\n",
      "87 0.6961547136306763 \t 12\t\n",
      "88 0.6959729194641113 \t 12\t\n",
      "89 0.694981575012207 \t 11\t\n",
      "90 0.693794846534729 \t 12\t\n",
      "91 0.6935125589370728 \t 12\t\n",
      "92 0.6950325965881348 \t 12\t\n",
      "93 0.696563720703125 \t 11\t\n",
      "94 0.6944522857666016 \t 11\t\n",
      "95 0.6957774758338928 \t 11\t\n",
      "96 0.6941837668418884 \t 11\t\n",
      "97 0.6968517899513245 \t 12\t\n",
      "98 0.6947436332702637 \t 17\t\n",
      "99 0.6957802176475525 \t 11\t\n",
      "\n",
      "===================================================\n",
      "10000\n",
      "0.6989231345713138 \t 12\n",
      "0 0.6924486756324768 \t 17\t\n",
      "1 0.6924093961715698 \t 12\t\n",
      "2 0.6924736499786377 \t 11\t\n",
      "3 0.6927904486656189 \t 12\t\n",
      "4 0.6922908425331116 \t 12\t\n",
      "5 0.6922243237495422 \t 12\t\n",
      "6 0.6924728751182556 \t 11\t\n",
      "7 0.6922740936279297 \t 12\t\n",
      "8 0.6929473876953125 \t 14\t\n",
      "9 0.6929899454116821 \t 15\t\n",
      "10 0.6929091811180115 \t 14\t\n",
      "11 0.6924600601196289 \t 13\t\n",
      "12 0.6928442120552063 \t 13\t\n",
      "13 0.6925170421600342 \t 13\t\n",
      "14 0.6922069787979126 \t 11\t\n",
      "15 0.6924782395362854 \t 14\t\n",
      "16 0.6928467154502869 \t 11\t\n",
      "17 0.6927529573440552 \t 21\t\n",
      "18 0.693459689617157 \t 16\t\n",
      "19 0.6928234100341797 \t 16\t\n",
      "20 0.692933976650238 \t 12\t\n",
      "21 0.6926903128623962 \t 12\t\n",
      "22 0.692939281463623 \t 12\t\n",
      "23 0.692642092704773 \t 12\t\n",
      "24 0.6928144097328186 \t 17\t\n",
      "25 0.6929450631141663 \t 15\t\n",
      "26 0.692770779132843 \t 14\t\n",
      "27 0.6927943229675293 \t 20\t\n",
      "28 0.6927420496940613 \t 12\t\n",
      "29 0.6925967931747437 \t 18\t\n",
      "30 0.6925374269485474 \t 13\t\n",
      "31 0.6926382780075073 \t 16\t\n",
      "32 0.6927160620689392 \t 13\t\n",
      "33 0.6925074458122253 \t 13\t\n",
      "34 0.6926673054695129 \t 12\t\n",
      "35 0.6926741003990173 \t 19\t\n",
      "36 0.6924881935119629 \t 14\t\n",
      "37 0.6928539276123047 \t 20\t\n",
      "38 0.692908763885498 \t 17\t\n",
      "39 0.6930340528488159 \t 19\t\n",
      "40 0.6930732131004333 \t 15\t\n",
      "41 0.6926918029785156 \t 14\t\n",
      "42 0.6925486326217651 \t 25\t\n",
      "43 0.692918062210083 \t 23\t\n",
      "44 0.6923527717590332 \t 15\t\n",
      "45 0.693048357963562 \t 19\t\n",
      "46 0.6929820775985718 \t 16\t\n",
      "47 0.6925270557403564 \t 21\t\n",
      "48 0.6927841305732727 \t 14\t\n",
      "49 0.6923887133598328 \t 12\t\n",
      "50 0.6926109790802002 \t 35\t\n",
      "51 0.6925920248031616 \t 29\t\n",
      "52 0.6925435066223145 \t 13\t\n",
      "53 0.6924464106559753 \t 13\t\n",
      "54 0.6930832862854004 \t 13\t\n",
      "55 0.6924843788146973 \t 17\t\n",
      "56 0.692584216594696 \t 16\t\n",
      "57 0.6925774812698364 \t 12\t\n",
      "58 0.6927854418754578 \t 11\t\n",
      "59 0.6928701400756836 \t 13\t\n",
      "60 0.6929675936698914 \t 17\t\n",
      "61 0.6930478811264038 \t 14\t\n",
      "62 0.6926272511482239 \t 13\t\n",
      "63 0.6926467418670654 \t 14\t\n",
      "64 0.6922745704650879 \t 12\t\n",
      "65 0.6924946308135986 \t 12\t\n",
      "66 0.6926783323287964 \t 13\t\n",
      "67 0.6926534175872803 \t 12\t\n",
      "68 0.6925182342529297 \t 11\t\n",
      "69 0.6925989985466003 \t 12\t\n",
      "70 0.6924314498901367 \t 16\t\n",
      "71 0.6929001212120056 \t 12\t\n",
      "72 0.6930418014526367 \t 21\t\n",
      "73 0.6927662491798401 \t 20\t\n",
      "74 0.6921901106834412 \t 13\t\n",
      "75 0.6932202577590942 \t 16\t\n",
      "76 0.6925715804100037 \t 18\t\n",
      "77 0.6922255754470825 \t 12\t\n",
      "78 0.6924461722373962 \t 12\t\n",
      "79 0.6926564574241638 \t 11\t\n",
      "80 0.6927596926689148 \t 22\t\n",
      "81 0.6928081512451172 \t 12\t\n",
      "82 0.6923460960388184 \t 11\t\n",
      "83 0.6925914287567139 \t 12\t\n",
      "84 0.6927000284194946 \t 12\t\n",
      "85 0.6926702857017517 \t 11\t\n",
      "86 0.6928176879882812 \t 16\t\n",
      "87 0.6923617720603943 \t 13\t\n",
      "88 0.6928620338439941 \t 15\t\n",
      "89 0.6926019191741943 \t 12\t\n",
      "90 0.6925444602966309 \t 20\t\n",
      "91 0.6927139759063721 \t 14\t\n",
      "92 0.6923533082008362 \t 13\t\n",
      "93 0.6928432583808899 \t 12\t\n",
      "94 0.6930103302001953 \t 11\t\n",
      "95 0.6925451159477234 \t 12\t\n",
      "96 0.6925098896026611 \t 29\t\n",
      "97 0.6922690272331238 \t 19\t\n",
      "98 0.6928988099098206 \t 16\t\n",
      "99 0.6922965049743652 \t 12\t\n",
      "\n",
      "===================================================\n",
      "100000\n",
      "0.6923379176658392 \t 13\n",
      "0 0.6912754774093628 \t 53\t\n",
      "1 0.6913517117500305 \t 33\t\n",
      "2 0.6912661194801331 \t 29\t\n",
      "3 0.6912923455238342 \t 26\t\n",
      "4 0.6912810206413269 \t 23\t\n",
      "5 0.6912311911582947 \t 33\t\n",
      "6 0.6913929581642151 \t 26\t\n",
      "7 0.6912224292755127 \t 36\t\n",
      "8 0.6912862658500671 \t 44\t\n",
      "9 0.6913167834281921 \t 33\t\n",
      "10 0.6912825703620911 \t 23\t\n",
      "11 0.6912262439727783 \t 39\t\n",
      "12 0.6912808418273926 \t 29\t\n",
      "13 0.6913111805915833 \t 26\t\n",
      "14 0.6913869380950928 \t 32\t\n",
      "15 0.6913579106330872 \t 54\t\n",
      "16 0.6914093494415283 \t 34\t\n",
      "17 0.6912820935249329 \t 27\t\n",
      "18 0.6913228034973145 \t 25\t\n",
      "19 0.6913690567016602 \t 27\t\n",
      "20 0.6912108659744263 \t 37\t\n",
      "21 0.6912049055099487 \t 52\t\n",
      "22 0.6912842988967896 \t 30\t\n",
      "23 0.6914896965026855 \t 23\t\n",
      "24 0.6912662386894226 \t 42\t\n",
      "25 0.6913315057754517 \t 26\t\n",
      "26 0.6912417411804199 \t 43\t\n",
      "27 0.6912453174591064 \t 61\t\n",
      "28 0.691326379776001 \t 26\t\n",
      "29 0.6914120316505432 \t 38\t\n",
      "30 0.6913225650787354 \t 29\t\n",
      "31 0.6912387013435364 \t 38\t\n",
      "32 0.6911899447441101 \t 35\t\n",
      "33 0.6912885308265686 \t 43\t\n",
      "34 0.6912946701049805 \t 30\t\n",
      "35 0.6912398934364319 \t 36\t\n",
      "36 0.6912959218025208 \t 31\t\n",
      "37 0.6912549734115601 \t 23\t\n",
      "38 0.6911897659301758 \t 40\t\n",
      "39 0.6913447380065918 \t 32\t\n",
      "40 0.6914200186729431 \t 37\t\n",
      "41 0.6912378668785095 \t 34\t\n",
      "42 0.691296398639679 \t 24\t\n",
      "43 0.6912739276885986 \t 46\t\n",
      "44 0.6912807822227478 \t 44\t\n",
      "45 0.6913665533065796 \t 39\t\n",
      "46 0.6912217140197754 \t 69\t\n",
      "47 0.6913431286811829 \t 40\t\n",
      "48 0.6913269758224487 \t 23\t\n",
      "49 0.6912548542022705 \t 38\t\n",
      "50 0.6912632584571838 \t 31\t\n",
      "51 0.6913728713989258 \t 26\t\n",
      "52 0.6912668943405151 \t 33\t\n",
      "53 0.6912440061569214 \t 34\t\n",
      "54 0.691318690776825 \t 29\t\n",
      "55 0.6912860870361328 \t 28\t\n",
      "56 0.6912294030189514 \t 34\t\n",
      "57 0.6914351582527161 \t 24\t\n",
      "58 0.6912513971328735 \t 36\t\n",
      "59 0.6913132667541504 \t 55\t\n",
      "60 0.6913816928863525 \t 33\t\n",
      "61 0.6912429928779602 \t 34\t\n",
      "62 0.6912510991096497 \t 31\t\n",
      "63 0.6913198232650757 \t 39\t\n",
      "64 0.6913785934448242 \t 24\t\n",
      "65 0.6913554668426514 \t 30\t\n",
      "66 0.6913228631019592 \t 50\t\n",
      "67 0.6912134885787964 \t 27\t\n",
      "68 0.6912742853164673 \t 31\t\n",
      "69 0.6912452578544617 \t 24\t\n",
      "70 0.6912482976913452 \t 29\t\n",
      "71 0.6912686824798584 \t 26\t\n",
      "72 0.6913408637046814 \t 24\t\n",
      "73 0.6911082863807678 \t 27\t\n",
      "74 0.6913893818855286 \t 23\t\n",
      "75 0.6912423968315125 \t 49\t\n",
      "76 0.6912277340888977 \t 23\t\n",
      "77 0.6912469267845154 \t 26\t\n",
      "78 0.6912369728088379 \t 47\t\n",
      "79 0.6912919282913208 \t 28\t\n",
      "80 0.691268265247345 \t 28\t\n",
      "81 0.6913198232650757 \t 29\t\n",
      "82 0.6912564039230347 \t 37\t\n",
      "83 0.6912558674812317 \t 32\t\n",
      "84 0.6914176344871521 \t 35\t\n",
      "85 0.6913585662841797 \t 26\t\n",
      "86 0.6912627220153809 \t 39\t\n",
      "87 0.691285252571106 \t 22\t\n",
      "88 0.6912785172462463 \t 29\t\n",
      "89 0.6912277936935425 \t 23\t\n",
      "90 0.6912897825241089 \t 29\t\n",
      "91 0.6912347078323364 \t 23\t\n",
      "92 0.6911301016807556 \t 28\t\n",
      "93 0.6913565397262573 \t 36\t\n",
      "94 0.6913137435913086 \t 22\t\n",
      "95 0.6913283467292786 \t 31\t\n",
      "96 0.6913344264030457 \t 25\t\n",
      "97 0.6912931203842163 \t 37\t\n",
      "98 0.6912704706192017 \t 25\t\n",
      "99 0.6913352608680725 \t 33\t\n",
      "\n",
      "===================================================\n",
      "1000000\n",
      "0.6910087631967515 \t 27\n",
      "0 0.6914517879486084 \t 59\t\n",
      "1 0.691374659538269 \t 100\t\n",
      "2 0.6913537383079529 \t 100\t\n",
      "3 0.691339373588562 \t 100\t\n",
      "4 0.6915028095245361 \t 35\t\n",
      "5 0.6914808750152588 \t 71\t\n",
      "6 0.691480278968811 \t 55\t\n",
      "7 0.6913572549819946 \t 100\t\n",
      "8 0.6914363503456116 \t 79\t\n",
      "9 0.6914594769477844 \t 48\t\n",
      "10 0.6913997530937195 \t 100\t\n",
      "11 0.6914915442466736 \t 44\t\n",
      "12 0.6913720369338989 \t 100\t\n",
      "13 0.6914977431297302 \t 43\t\n",
      "14 0.6914551258087158 \t 58\t\n",
      "15 0.6914624571800232 \t 63\t\n",
      "16 0.6913964152336121 \t 90\t\n",
      "17 0.6914417147636414 \t 82\t\n",
      "18 0.6914456486701965 \t 70\t\n",
      "19 0.6913131475448608 \t 100\t\n",
      "20 0.6914743781089783 \t 51\t\n",
      "21 0.6914488673210144 \t 61\t\n",
      "22 0.6913557648658752 \t 100\t\n",
      "23 0.6914495229721069 \t 53\t\n",
      "24 0.6914830207824707 \t 51\t\n",
      "25 0.6914791464805603 \t 28\t\n",
      "26 0.6914843320846558 \t 44\t\n",
      "27 0.6914694905281067 \t 49\t\n",
      "28 0.6913907527923584 \t 100\t\n",
      "29 0.6914861798286438 \t 52\t\n",
      "30 0.6914698481559753 \t 81\t\n",
      "31 0.6914659738540649 \t 46\t\n",
      "32 0.6913893818855286 \t 100\t\n",
      "33 0.6914430856704712 \t 64\t\n",
      "34 0.6914613246917725 \t 56\t\n",
      "35 0.6914433836936951 \t 100\t\n",
      "36 0.69145667552948 \t 63\t\n",
      "37 0.6914190053939819 \t 100\t\n",
      "38 0.6914835572242737 \t 52\t\n",
      "39 0.6913391947746277 \t 100\t\n",
      "40 0.6914588212966919 \t 53\t\n",
      "41 0.6913893818855286 \t 100\t\n",
      "42 0.6913878917694092 \t 86\t\n",
      "43 0.6914117336273193 \t 85\t\n",
      "44 0.6914348006248474 \t 82\t\n",
      "45 0.6914949417114258 \t 39\t\n",
      "46 0.6914013624191284 \t 100\t\n",
      "47 0.6913291215896606 \t 100\t\n",
      "48 0.6914423704147339 \t 65\t\n",
      "49 0.6914242506027222 \t 91\t\n",
      "50 0.6914440393447876 \t 57\t\n",
      "51 0.6913997530937195 \t 96\t\n",
      "52 0.6914263963699341 \t 70\t\n",
      "53 0.6914110779762268 \t 76\t\n",
      "54 0.6914202570915222 \t 81\t\n",
      "55 0.6914551258087158 \t 66\t\n",
      "56 0.6914637684822083 \t 46\t\n",
      "57 0.6914544105529785 \t 59\t\n",
      "58 0.6913202404975891 \t 100\t\n",
      "59 0.6914671063423157 \t 59\t\n",
      "60 0.6913393139839172 \t 100\t\n",
      "61 0.6914781928062439 \t 85\t\n",
      "62 0.6914576292037964 \t 44\t\n",
      "63 0.6914551258087158 \t 47\t\n",
      "64 0.6914221048355103 \t 71\t\n",
      "65 0.6913722157478333 \t 100\t\n",
      "66 0.6914160251617432 \t 88\t\n",
      "67 0.6914593577384949 \t 66\t\n",
      "68 0.6914392113685608 \t 66\t\n",
      "69 0.6913735270500183 \t 100\t\n",
      "70 0.6914993524551392 \t 44\t\n",
      "71 0.6914533972740173 \t 75\t\n",
      "72 0.6913595795631409 \t 100\t\n",
      "73 0.6914618611335754 \t 51\t\n",
      "74 0.691403329372406 \t 89\t\n",
      "75 0.6915140748023987 \t 40\t\n",
      "76 0.6914421916007996 \t 100\t\n",
      "77 0.6914421916007996 \t 50\t\n",
      "78 0.69134122133255 \t 100\t\n",
      "79 0.6914525032043457 \t 68\t\n",
      "80 0.6914346218109131 \t 100\t\n",
      "81 0.6914191246032715 \t 82\t\n",
      "82 0.6914622783660889 \t 60\t\n",
      "83 0.6914154887199402 \t 66\t\n",
      "84 0.6914523243904114 \t 61\t\n",
      "85 0.6914631724357605 \t 44\t\n",
      "86 0.6914633512496948 \t 46\t\n",
      "87 0.6914474368095398 \t 59\t\n",
      "88 0.6913397312164307 \t 100\t\n",
      "89 0.6914553642272949 \t 61\t\n",
      "90 0.691474974155426 \t 44\t\n",
      "91 0.6914086937904358 \t 80\t\n",
      "92 0.6913962960243225 \t 100\t\n",
      "93 0.6913930177688599 \t 100\t\n",
      "94 0.6913204789161682 \t 100\t\n",
      "95 0.6914476156234741 \t 63\t\n",
      "96 0.6914207339286804 \t 75\t\n",
      "97 0.691446840763092 \t 69\t\n",
      "98 0.6915112137794495 \t 28\t\n",
      "99 0.6914616227149963 \t 42\t\n",
      "\n",
      "===================================================\n",
      "10000000\n",
      "0.6907671455825418 \t 44\n",
      "0 0.6908367872238159 \t 100\t\n",
      "1 0.6909074187278748 \t 100\t\n",
      "2 0.6908218264579773 \t 100\t\n",
      "3 0.6909608244895935 \t 100\t\n",
      "4 0.6908385753631592 \t 100\t\n",
      "5 0.690834105014801 \t 100\t\n",
      "6 0.6909276843070984 \t 100\t\n",
      "7 0.6908357739448547 \t 100\t\n",
      "8 0.690941572189331 \t 100\t\n",
      "9 0.690987229347229 \t 100\t\n",
      "10 0.6908235549926758 \t 100\t\n",
      "11 0.6910064220428467 \t 100\t\n",
      "12 0.6908401846885681 \t 100\t\n",
      "13 0.6908483505249023 \t 100\t\n",
      "14 0.690890908241272 \t 100\t\n",
      "15 0.6908673644065857 \t 100\t\n",
      "16 0.6908314228057861 \t 100\t\n",
      "17 0.6908740401268005 \t 100\t\n",
      "18 0.6908825039863586 \t 100\t\n",
      "19 0.6909831166267395 \t 100\t\n",
      "20 0.6911001801490784 \t 100\t\n",
      "21 0.6908407211303711 \t 100\t\n",
      "22 0.6909663081169128 \t 100\t\n",
      "23 0.6909382939338684 \t 100\t\n",
      "24 0.6908720135688782 \t 100\t\n",
      "25 0.6910626888275146 \t 100\t\n",
      "26 0.690776526927948 \t 100\t\n",
      "27 0.690853476524353 \t 100\t\n",
      "28 0.6908643245697021 \t 100\t\n",
      "29 0.690881073474884 \t 100\t\n",
      "30 0.6907824873924255 \t 100\t\n",
      "31 0.6908721923828125 \t 100\t\n",
      "32 0.6908005475997925 \t 100\t\n",
      "33 0.6909250020980835 \t 100\t\n",
      "34 0.6909023523330688 \t 100\t\n",
      "35 0.6910008192062378 \t 100\t\n",
      "36 0.6908489465713501 \t 100\t\n",
      "37 0.6908792853355408 \t 100\t\n",
      "38 0.690888524055481 \t 100\t\n",
      "39 0.690946102142334 \t 100\t\n",
      "40 0.6908835172653198 \t 100\t\n",
      "41 0.6908644437789917 \t 100\t\n",
      "42 0.6909077167510986 \t 100\t\n",
      "43 0.6908261179924011 \t 100\t\n",
      "44 0.6909704804420471 \t 100\t\n",
      "45 0.6910956501960754 \t 100\t\n",
      "46 0.6908143162727356 \t 100\t\n",
      "47 0.6908438205718994 \t 100\t\n",
      "48 0.6910609602928162 \t 100\t\n",
      "49 0.6908723711967468 \t 100\t\n",
      "50 0.6908276677131653 \t 100\t\n",
      "51 0.6911851167678833 \t 100\t\n",
      "52 0.6909649968147278 \t 100\t\n",
      "53 0.6908131837844849 \t 100\t\n",
      "54 0.690808892250061 \t 100\t\n",
      "55 0.6908090114593506 \t 100\t\n",
      "56 0.6907991766929626 \t 100\t\n",
      "57 0.6908833980560303 \t 100\t\n",
      "58 0.6908484101295471 \t 100\t\n",
      "59 0.6908249258995056 \t 100\t\n",
      "60 0.6909117102622986 \t 100\t\n",
      "61 0.6908115148544312 \t 100\t\n",
      "62 0.691189706325531 \t 100\t\n",
      "63 0.6908919811248779 \t 100\t\n",
      "64 0.6909708380699158 \t 100\t\n",
      "65 0.6908828020095825 \t 100\t\n",
      "66 0.6909191012382507 \t 100\t\n",
      "67 0.6908730268478394 \t 100\t\n",
      "68 0.6908149123191833 \t 100\t\n",
      "69 0.6912842988967896 \t 100\t\n",
      "70 0.6908084750175476 \t 100\t\n",
      "71 0.6907699704170227 \t 100\t\n",
      "72 0.6908652782440186 \t 100\t\n",
      "73 0.6908326745033264 \t 100\t\n",
      "74 0.6908295750617981 \t 100\t\n",
      "75 0.6909744739532471 \t 100\t\n",
      "76 0.6908137202262878 \t 100\t\n",
      "77 0.6907969117164612 \t 100\t\n",
      "78 0.6913272738456726 \t 100\t\n",
      "79 0.6908600926399231 \t 100\t\n",
      "80 0.6908813118934631 \t 100\t\n",
      "81 0.690834105014801 \t 100\t\n",
      "82 0.690873920917511 \t 100\t\n",
      "83 0.6908621191978455 \t 100\t\n",
      "84 0.6909527778625488 \t 100\t\n",
      "85 0.6908001899719238 \t 100\t\n",
      "86 0.6909260749816895 \t 100\t\n",
      "87 0.6908146739006042 \t 100\t\n",
      "88 0.6908388137817383 \t 100\t\n",
      "89 0.6909295320510864 \t 100\t\n",
      "90 0.6908484697341919 \t 100\t\n",
      "91 0.6908791065216064 \t 100\t\n",
      "92 0.6907953023910522 \t 100\t\n",
      "93 0.6909123063087463 \t 100\t\n",
      "94 0.6908243894577026 \t 100\t\n",
      "95 0.6909257769584656 \t 100\t\n",
      "96 0.6908531785011292 \t 100\t\n",
      "97 0.6907684803009033 \t 100\t\n",
      "98 0.6910619139671326 \t 100\t\n",
      "99 0.6907920837402344 \t 100\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for N in Ns:\n",
    "    print('===================================================\\n{}'.format(N))\n",
    "    # Take the first N samples.\n",
    "    data, m, s = split_data(X[:N], y[:N])\n",
    "    \n",
    "    # Train BDT model (only need to train 1)\n",
    "    bdt_model = XGBClassifier(early_stopping_rounds = 10)\n",
    "    X_trn, X_vld, y_trn, y_vld = data\n",
    "    bdt_model.fit(X_trn, y_trn, eval_set = [(X_vld, y_vld)], verbose = 0)\n",
    "    trace = bdt_model.evals_result()['validation_0']\n",
    "    print(trace['logloss'][-1], '\\t', len(trace['logloss']), end = '\\n')\n",
    "    bdt_model.save_model(bdt_filestr.format(N))\n",
    "\n",
    "    for i in range(reps):\n",
    "        print(i, end = ' ')\n",
    "        # Train BCE model\n",
    "        bce_model, trace = train(data, **bce_params)\n",
    "        bce_model.save_weights(bce_filestr.format(N, i))\n",
    "        \n",
    "        # Train GBC model\n",
    "        gbc_model = GradientBoostingClassifier(validation_fraction = 0.25,\n",
    "                                               n_iter_no_change = 10)\n",
    "        gbc_model.fit(X[:N], y[:N])\n",
    "        dump(gbc_model, gbc_filestr.format(N, i))\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c198e6-0986-41aa-a9a7-d28f318d178d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## $d = 4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c9764d4-4e06-4426-b662-c1551a4c8747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "num = 0\n",
    "reps = 100\n",
    "d = 4\n",
    "Ns = 10**np.arange(2, 8)\n",
    "\n",
    "# Model parameters\n",
    "bce_params = {'loss':bce, 'd': d}\n",
    "\n",
    "filestr = 'models/zenodo/{}/set_{}/'.format(d, num)\n",
    "bce_filestr = filestr + 'bce/model_{}_{}.h5'\n",
    "bdt_filestr = filestr + 'bdt/model_{}.h5'\n",
    "gbc_filestr = filestr + 'gbc/model_{}_{}.h5'\n",
    "\n",
    "if not os.path.isdir(filestr):\n",
    "    os.mkdir(filestr)\n",
    "\n",
    "if not os.path.isdir(filestr + 'bce/'):\n",
    "    os.mkdir(filestr + 'bce/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'bdt/'):\n",
    "    os.mkdir(filestr + 'bdt/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'gbc/'):\n",
    "    os.mkdir(filestr + 'gbc/')\n",
    "\n",
    "# Data parameters\n",
    "X = np.load('data/zenodo/{}/X_trn.npy'.format(d))\n",
    "y = np.load('data/zenodo/{}/y_trn.npy'.format(d)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15382cf7-0ebb-41ab-bc6f-037c5c058056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================\n",
      "100\n",
      "1.0303720071911813 \t 11\n",
      "0 0.7152805924415588 \t 15\t\n",
      "1 0.7930371165275574 \t 21\t\n",
      "2 0.7494280338287354 \t 20\t\n",
      "3 0.7697917222976685 \t 20\t\n",
      "4 0.688759982585907 \t 17\t\n",
      "5 0.7402358055114746 \t 19\t\n",
      "6 0.7901978492736816 \t 21\t\n",
      "7 0.7297656536102295 \t 14\t\n",
      "8 0.7083227634429932 \t 18\t\n",
      "9 0.7640546560287476 \t 22\t\n",
      "10 0.7565174102783203 \t 22\t\n",
      "11 0.7596052289009094 \t 20\t\n",
      "12 0.6614600419998169 \t 14\t\n",
      "13 0.7304542064666748 \t 17\t\n",
      "14 0.722554624080658 \t 19\t\n",
      "15 0.7085067629814148 \t 20\t\n",
      "16 0.7608336806297302 \t 17\t\n",
      "17 0.7507644891738892 \t 21\t\n",
      "18 0.7018964886665344 \t 17\t\n",
      "19 0.7381808757781982 \t 17\t\n",
      "20 0.7348161339759827 \t 21\t\n",
      "21 0.7106039524078369 \t 18\t\n",
      "22 0.7307910323143005 \t 16\t\n",
      "23 0.7920247912406921 \t 22\t\n",
      "24 0.7406968474388123 \t 18\t\n",
      "25 0.7073522210121155 \t 22\t\n",
      "26 0.7665988802909851 \t 21\t\n",
      "27 0.7552981376647949 \t 21\t\n",
      "28 0.7021545171737671 \t 18\t\n",
      "29 0.719815731048584 \t 20\t\n",
      "30 0.7171556949615479 \t 19\t\n",
      "31 0.7294284701347351 \t 22\t\n",
      "32 0.7039250135421753 \t 21\t\n",
      "33 0.6961967945098877 \t 17\t\n",
      "34 0.7264349460601807 \t 20\t\n",
      "35 0.7684410214424133 \t 20\t\n",
      "36 0.7745237946510315 \t 22\t\n",
      "37 0.6816476583480835 \t 15\t\n",
      "38 0.7009669542312622 \t 15\t\n",
      "39 0.7383064031600952 \t 20\t\n",
      "40 0.7510724067687988 \t 26\t\n",
      "41 0.7335491180419922 \t 15\t\n",
      "42 0.751288890838623 \t 21\t\n",
      "43 0.8065552711486816 \t 20\t\n",
      "44 0.7021626830101013 \t 18\t\n",
      "45 0.7011334300041199 \t 17\t\n",
      "46 0.7373791337013245 \t 15\t\n",
      "47 0.7328140139579773 \t 21\t\n",
      "48 0.692608654499054 \t 19\t\n",
      "49 0.7204650044441223 \t 21\t\n",
      "50 0.7117851376533508 \t 18\t\n",
      "51 0.7272735834121704 \t 20\t\n",
      "52 0.7077503800392151 \t 16\t\n",
      "53 0.7408445477485657 \t 19\t\n",
      "54 0.7820680141448975 \t 22\t\n",
      "55 0.6957134008407593 \t 16\t\n",
      "56 0.7309109568595886 \t 21\t\n",
      "57 0.7345155477523804 \t 17\t\n",
      "58 0.6674621105194092 \t 16\t\n",
      "59 0.7325109839439392 \t 17\t\n",
      "60 0.810096263885498 \t 21\t\n",
      "61 0.6969505548477173 \t 16\t\n",
      "62 0.7533986568450928 \t 20\t\n",
      "63 0.7379110455513 \t 24\t\n",
      "64 0.7107946276664734 \t 18\t\n",
      "65 0.701907753944397 \t 13\t\n",
      "66 0.7390509247779846 \t 17\t\n",
      "67 0.693515419960022 \t 17\t\n",
      "68 0.716773271560669 \t 15\t\n",
      "69 0.7366254329681396 \t 17\t\n",
      "70 0.7591906785964966 \t 20\t\n",
      "71 0.7036805152893066 \t 19\t\n",
      "72 0.7186036705970764 \t 18\t\n",
      "73 0.6820605993270874 \t 18\t\n",
      "74 0.7396340370178223 \t 18\t\n",
      "75 0.7170779705047607 \t 16\t\n",
      "76 0.7243867516517639 \t 18\t\n",
      "77 0.748750627040863 \t 20\t\n",
      "78 0.7890963554382324 \t 27\t\n",
      "79 0.7856244444847107 \t 22\t\n",
      "80 0.7391909956932068 \t 17\t\n",
      "81 0.6787291169166565 \t 16\t\n",
      "82 0.7074825763702393 \t 16\t\n",
      "83 0.680866003036499 \t 16\t\n",
      "84 0.7155570387840271 \t 16\t\n",
      "85 0.7213323712348938 \t 17\t\n",
      "86 0.7150792479515076 \t 23\t\n",
      "87 0.6799059510231018 \t 18\t\n",
      "88 0.703216016292572 \t 17\t\n",
      "89 0.7233222723007202 \t 20\t\n",
      "90 0.6683319807052612 \t 15\t\n",
      "91 0.680989146232605 \t 15\t\n",
      "92 0.7083696722984314 \t 19\t\n",
      "93 0.7060077786445618 \t 14\t\n",
      "94 0.7592912316322327 \t 24\t\n",
      "95 0.7356387376785278 \t 21\t\n",
      "96 0.7020652890205383 \t 17\t\n",
      "97 0.792898416519165 \t 21\t\n",
      "98 0.7524667382240295 \t 18\t\n",
      "99 0.7229195237159729 \t 17\t\n",
      "\n",
      "===================================================\n",
      "1000\n",
      "0.756363850235939 \t 11\n",
      "0 0.7166744470596313 \t 11\t\n",
      "1 0.7127248048782349 \t 11\t\n",
      "2 0.7199257612228394 \t 11\t\n",
      "3 0.714540958404541 \t 11\t\n",
      "4 0.7149016857147217 \t 11\t\n",
      "5 0.7096689939498901 \t 11\t\n",
      "6 0.7216070890426636 \t 11\t\n",
      "7 0.7181931734085083 \t 11\t\n",
      "8 0.7129502892494202 \t 11\t\n",
      "9 0.7155858874320984 \t 11\t\n",
      "10 0.7180542349815369 \t 11\t\n",
      "11 0.7198800444602966 \t 11\t\n",
      "12 0.719441831111908 \t 11\t\n",
      "13 0.7226971387863159 \t 11\t\n",
      "14 0.7205359935760498 \t 11\t\n",
      "15 0.7198776006698608 \t 11\t\n",
      "16 0.7192020416259766 \t 11\t\n",
      "17 0.7140291929244995 \t 11\t\n",
      "18 0.7199349999427795 \t 11\t\n",
      "19 0.7190295457839966 \t 11\t\n",
      "20 0.715236246585846 \t 11\t\n",
      "21 0.7138714790344238 \t 11\t\n",
      "22 0.7160800695419312 \t 11\t\n",
      "23 0.7208529114723206 \t 11\t\n",
      "24 0.7181870341300964 \t 11\t\n",
      "25 0.7209402918815613 \t 11\t\n",
      "26 0.7180371284484863 \t 11\t\n",
      "27 0.7177309989929199 \t 11\t\n",
      "28 0.7144150733947754 \t 12\t\n",
      "29 0.7136163711547852 \t 11\t\n",
      "30 0.7242040634155273 \t 11\t\n",
      "31 0.7132611870765686 \t 11\t\n",
      "32 0.7173117399215698 \t 11\t\n",
      "33 0.719989538192749 \t 11\t\n",
      "34 0.719260036945343 \t 11\t\n",
      "35 0.7210038900375366 \t 11\t\n",
      "36 0.7174279093742371 \t 11\t\n",
      "37 0.7224467992782593 \t 11\t\n",
      "38 0.7210299968719482 \t 11\t\n",
      "39 0.714985191822052 \t 11\t\n",
      "40 0.7219000458717346 \t 11\t\n",
      "41 0.7139680981636047 \t 11\t\n",
      "42 0.7206177115440369 \t 11\t\n",
      "43 0.7138333320617676 \t 11\t\n",
      "44 0.7215364575386047 \t 11\t\n",
      "45 0.7228273153305054 \t 11\t\n",
      "46 0.7211605906486511 \t 11\t\n",
      "47 0.7208733558654785 \t 11\t\n",
      "48 0.7123931050300598 \t 11\t\n",
      "49 0.7141233086585999 \t 11\t\n",
      "50 0.7182576656341553 \t 11\t\n",
      "51 0.719505786895752 \t 11\t\n",
      "52 0.7125775218009949 \t 11\t\n",
      "53 0.7235890626907349 \t 11\t\n",
      "54 0.7212292551994324 \t 11\t\n",
      "55 0.7207608222961426 \t 11\t\n",
      "56 0.7081417441368103 \t 11\t\n",
      "57 0.7192239165306091 \t 11\t\n",
      "58 0.7202861309051514 \t 11\t\n",
      "59 0.7220319509506226 \t 11\t\n",
      "60 0.717517077922821 \t 11\t\n",
      "61 0.7213149666786194 \t 11\t\n",
      "62 0.7159662246704102 \t 11\t\n",
      "63 0.7225733399391174 \t 11\t\n",
      "64 0.7201120853424072 \t 11\t\n",
      "65 0.7121908068656921 \t 11\t\n",
      "66 0.7172051668167114 \t 11\t\n",
      "67 0.712906002998352 \t 11\t\n",
      "68 0.716809093952179 \t 11\t\n",
      "69 0.7183510065078735 \t 11\t\n",
      "70 0.7168142199516296 \t 11\t\n",
      "71 0.7161697149276733 \t 11\t\n",
      "72 0.7147840857505798 \t 11\t\n",
      "73 0.7190850973129272 \t 12\t\n",
      "74 0.718190610408783 \t 11\t\n",
      "75 0.7203537821769714 \t 11\t\n",
      "76 0.7095344066619873 \t 11\t\n",
      "77 0.7185862064361572 \t 11\t\n",
      "78 0.7142004370689392 \t 11\t\n",
      "79 0.7199901342391968 \t 11\t\n",
      "80 0.7155150175094604 \t 11\t\n",
      "81 0.7192923426628113 \t 11\t\n",
      "82 0.7171226739883423 \t 11\t\n",
      "83 0.7234264016151428 \t 11\t\n",
      "84 0.7166131138801575 \t 11\t\n",
      "85 0.7213954329490662 \t 11\t\n",
      "86 0.7175154089927673 \t 11\t\n",
      "87 0.7189134359359741 \t 11\t\n",
      "88 0.7152373194694519 \t 11\t\n",
      "89 0.7162336707115173 \t 11\t\n",
      "90 0.7138844132423401 \t 11\t\n",
      "91 0.7151899337768555 \t 12\t\n",
      "92 0.7154902219772339 \t 12\t\n",
      "93 0.7182517051696777 \t 11\t\n",
      "94 0.7166585326194763 \t 12\t\n",
      "95 0.7128669619560242 \t 11\t\n",
      "96 0.7154632210731506 \t 11\t\n",
      "97 0.7183153033256531 \t 11\t\n",
      "98 0.7126626968383789 \t 11\t\n",
      "99 0.7167407274246216 \t 11\t\n",
      "\n",
      "===================================================\n",
      "10000\n",
      "0.691845316144824 \t 12\n",
      "0 0.6878968477249146 \t 26\t\n",
      "1 0.6886090636253357 \t 16\t\n",
      "2 0.6875255703926086 \t 19\t\n",
      "3 0.6880006790161133 \t 24\t\n",
      "4 0.6876842975616455 \t 15\t\n",
      "5 0.6881545782089233 \t 26\t\n",
      "6 0.6887527108192444 \t 19\t\n",
      "7 0.6886850595474243 \t 15\t\n",
      "8 0.6882455348968506 \t 19\t\n",
      "9 0.6880081295967102 \t 15\t\n",
      "10 0.6885698437690735 \t 18\t\n",
      "11 0.6888014674186707 \t 20\t\n",
      "12 0.6885200142860413 \t 17\t\n",
      "13 0.6886568069458008 \t 26\t\n",
      "14 0.6886451840400696 \t 14\t\n",
      "15 0.6881320476531982 \t 25\t\n",
      "16 0.6887345910072327 \t 21\t\n",
      "17 0.6884352564811707 \t 24\t\n",
      "18 0.6876924633979797 \t 23\t\n",
      "19 0.6880607604980469 \t 15\t\n",
      "20 0.6892517805099487 \t 21\t\n",
      "21 0.689801812171936 \t 39\t\n",
      "22 0.6878059506416321 \t 15\t\n",
      "23 0.6882495284080505 \t 18\t\n",
      "24 0.6891164779663086 \t 22\t\n",
      "25 0.688332736492157 \t 28\t\n",
      "26 0.6882922053337097 \t 37\t\n",
      "27 0.6889641880989075 \t 28\t\n",
      "28 0.6892110705375671 \t 35\t\n",
      "29 0.688639760017395 \t 20\t\n",
      "30 0.6879351139068604 \t 22\t\n",
      "31 0.6881197094917297 \t 26\t\n",
      "32 0.6877297163009644 \t 24\t\n",
      "33 0.6880195140838623 \t 17\t\n",
      "34 0.6882275342941284 \t 16\t\n",
      "35 0.6886119246482849 \t 15\t\n",
      "36 0.6883501410484314 \t 28\t\n",
      "37 0.6883700489997864 \t 17\t\n",
      "38 0.6879516243934631 \t 27\t\n",
      "39 0.6885191798210144 \t 19\t\n",
      "40 0.6881618499755859 \t 15\t\n",
      "41 0.688947856426239 \t 15\t\n",
      "42 0.6880530118942261 \t 26\t\n",
      "43 0.6894339323043823 \t 27\t\n",
      "44 0.688439130783081 \t 18\t\n",
      "45 0.688469409942627 \t 21\t\n",
      "46 0.6874358654022217 \t 15\t\n",
      "47 0.6885507106781006 \t 17\t\n",
      "48 0.6877237558364868 \t 16\t\n",
      "49 0.687950074672699 \t 22\t\n",
      "50 0.6881510019302368 \t 20\t\n",
      "51 0.6880414485931396 \t 21\t\n",
      "52 0.689819872379303 \t 28\t\n",
      "53 0.6881412267684937 \t 31\t\n",
      "54 0.6881850361824036 \t 27\t\n",
      "55 0.688300609588623 \t 23\t\n",
      "56 0.6875279545783997 \t 30\t\n",
      "57 0.6882448792457581 \t 19\t\n",
      "58 0.6886829137802124 \t 15\t\n",
      "59 0.6889060735702515 \t 24\t\n",
      "60 0.68852698802948 \t 30\t\n",
      "61 0.6889203190803528 \t 15\t\n",
      "62 0.6883864998817444 \t 21\t\n",
      "63 0.6884980201721191 \t 20\t\n",
      "64 0.6878315210342407 \t 18\t\n",
      "65 0.688197135925293 \t 37\t\n",
      "66 0.689950704574585 \t 28\t\n",
      "67 0.6883050203323364 \t 31\t\n",
      "68 0.6884880661964417 \t 16\t\n",
      "69 0.6875596642494202 \t 25\t\n",
      "70 0.6876122951507568 \t 16\t\n",
      "71 0.6886908411979675 \t 28\t\n",
      "72 0.688791036605835 \t 19\t\n",
      "73 0.6877252459526062 \t 17\t\n",
      "74 0.6884282827377319 \t 30\t\n",
      "75 0.6878074407577515 \t 15\t\n",
      "76 0.6879379153251648 \t 15\t\n",
      "77 0.687788188457489 \t 27\t\n",
      "78 0.687752902507782 \t 31\t\n",
      "79 0.6882764101028442 \t 26\t\n",
      "80 0.6882169842720032 \t 23\t\n",
      "81 0.6888828873634338 \t 18\t\n",
      "82 0.6881334185600281 \t 27\t\n",
      "83 0.6880431771278381 \t 15\t\n",
      "84 0.6884781122207642 \t 28\t\n",
      "85 0.6887097358703613 \t 22\t\n",
      "86 0.6877427697181702 \t 16\t\n",
      "87 0.6880693435668945 \t 26\t\n",
      "88 0.6879911422729492 \t 20\t\n",
      "89 0.6878249645233154 \t 24\t\n",
      "90 0.6889832615852356 \t 25\t\n",
      "91 0.6874247789382935 \t 17\t\n",
      "92 0.6882349848747253 \t 15\t\n",
      "93 0.6889501214027405 \t 24\t\n",
      "94 0.6883372068405151 \t 15\t\n",
      "95 0.6884599328041077 \t 25\t\n",
      "96 0.6880961656570435 \t 22\t\n",
      "97 0.6880884766578674 \t 19\t\n",
      "98 0.6876107454299927 \t 20\t\n",
      "99 0.6880044937133789 \t 15\t\n",
      "\n",
      "===================================================\n",
      "100000\n",
      "0.6812957424539328 \t 20\n",
      "0 0.6821429133415222 \t 56\t\n",
      "1 0.6820333003997803 \t 69\t\n",
      "2 0.6818349957466125 \t 63\t\n",
      "3 0.6821051836013794 \t 66\t\n",
      "4 0.682115375995636 \t 61\t\n",
      "5 0.6821930408477783 \t 54\t\n",
      "6 0.682066023349762 \t 81\t\n",
      "7 0.6819871664047241 \t 72\t\n",
      "8 0.6823326349258423 \t 66\t\n",
      "9 0.6820245981216431 \t 52\t\n",
      "10 0.6820383071899414 \t 51\t\n",
      "11 0.6822550296783447 \t 39\t\n",
      "12 0.6821836829185486 \t 57\t\n",
      "13 0.6819856762886047 \t 62\t\n",
      "14 0.6819664835929871 \t 58\t\n",
      "15 0.6819966435432434 \t 53\t\n",
      "16 0.6820155382156372 \t 55\t\n",
      "17 0.6822097897529602 \t 56\t\n",
      "18 0.6820162534713745 \t 47\t\n",
      "19 0.6819322109222412 \t 52\t\n",
      "20 0.6820418834686279 \t 68\t\n",
      "21 0.682033121585846 \t 48\t\n",
      "22 0.6817386150360107 \t 100\t\n",
      "23 0.6817985773086548 \t 65\t\n",
      "24 0.681878387928009 \t 76\t\n",
      "25 0.6819101572036743 \t 74\t\n",
      "26 0.6820811033248901 \t 70\t\n",
      "27 0.6820062398910522 \t 66\t\n",
      "28 0.6819852590560913 \t 54\t\n",
      "29 0.6819508075714111 \t 56\t\n",
      "30 0.681961715221405 \t 49\t\n",
      "31 0.6819787621498108 \t 48\t\n",
      "32 0.6820548176765442 \t 62\t\n",
      "33 0.6820107698440552 \t 66\t\n",
      "34 0.6819604635238647 \t 58\t\n",
      "35 0.68208909034729 \t 65\t\n",
      "36 0.6820393800735474 \t 68\t\n",
      "37 0.6821997761726379 \t 60\t\n",
      "38 0.682119607925415 \t 62\t\n",
      "39 0.6822013258934021 \t 34\t\n",
      "40 0.6820630431175232 \t 79\t\n",
      "41 0.6821699738502502 \t 76\t\n",
      "42 0.6818900108337402 \t 100\t\n",
      "43 0.6816328167915344 \t 100\t\n",
      "44 0.6819989681243896 \t 78\t\n",
      "45 0.6820361614227295 \t 59\t\n",
      "46 0.6821033358573914 \t 42\t\n",
      "47 0.6821302175521851 \t 44\t\n",
      "48 0.6819772720336914 \t 68\t\n",
      "49 0.6819278001785278 \t 56\t\n",
      "50 0.6821860074996948 \t 47\t\n",
      "51 0.682178258895874 \t 51\t\n",
      "52 0.6821451783180237 \t 65\t\n",
      "53 0.6822564601898193 \t 63\t\n",
      "54 0.6821545362472534 \t 49\t\n",
      "55 0.6822259426116943 \t 46\t\n",
      "56 0.6823378801345825 \t 41\t\n",
      "57 0.6819879412651062 \t 58\t\n",
      "58 0.68219393491745 \t 58\t\n",
      "59 0.6820723414421082 \t 79\t\n",
      "60 0.6822485327720642 \t 63\t\n",
      "61 0.6820321679115295 \t 73\t\n",
      "62 0.6816253066062927 \t 100\t\n",
      "63 0.6820845603942871 \t 50\t\n",
      "64 0.682201623916626 \t 61\t\n",
      "65 0.6820042729377747 \t 64\t\n",
      "66 0.6821167469024658 \t 78\t\n",
      "67 0.6815747022628784 \t 100\t\n",
      "68 0.6817527413368225 \t 61\t\n",
      "69 0.682197630405426 \t 58\t\n",
      "70 0.6822700500488281 \t 55\t\n",
      "71 0.6817107200622559 \t 100\t\n",
      "72 0.6821191310882568 \t 41\t\n",
      "73 0.6819555759429932 \t 57\t\n",
      "74 0.6821191310882568 \t 81\t\n",
      "75 0.6821953058242798 \t 43\t\n",
      "76 0.6818495392799377 \t 100\t\n",
      "77 0.6822765469551086 \t 41\t\n",
      "78 0.6819890141487122 \t 79\t\n",
      "79 0.6819144487380981 \t 53\t\n",
      "80 0.6821165084838867 \t 45\t\n",
      "81 0.681991457939148 \t 53\t\n",
      "82 0.6820200085639954 \t 84\t\n",
      "83 0.6820682287216187 \t 54\t\n",
      "84 0.6822212338447571 \t 63\t\n",
      "85 0.6820718050003052 \t 74\t\n",
      "86 0.682539165019989 \t 43\t\n",
      "87 0.6819038987159729 \t 48\t\n",
      "88 0.6821653842926025 \t 53\t\n",
      "89 0.6822657585144043 \t 67\t\n",
      "90 0.6821179389953613 \t 58\t\n",
      "91 0.6822747588157654 \t 76\t\n",
      "92 0.68229740858078 \t 50\t\n",
      "93 0.682022213935852 \t 69\t\n",
      "94 0.682335376739502 \t 42\t\n",
      "95 0.6821781396865845 \t 55\t\n",
      "96 0.681540846824646 \t 100\t\n",
      "97 0.6822745203971863 \t 54\t\n",
      "98 0.6820491552352905 \t 58\t\n",
      "99 0.6821134090423584 \t 53\t\n",
      "\n",
      "===================================================\n",
      "1000000\n",
      "0.6770499480839255 \t 71\n",
      "0 0.6787963509559631 \t 100\t\n",
      "1 0.6787541508674622 \t 100\t\n",
      "2 0.6788221001625061 \t 100\t\n",
      "3 0.6784877777099609 \t 100\t\n",
      "4 0.6785604953765869 \t 100\t\n",
      "5 0.6786550879478455 \t 100\t\n",
      "6 0.678648054599762 \t 100\t\n",
      "7 0.6787700057029724 \t 100\t\n",
      "8 0.6784223318099976 \t 100\t\n",
      "9 0.6786335110664368 \t 100\t\n",
      "10 0.6788621544837952 \t 100\t\n",
      "11 0.678730845451355 \t 100\t\n",
      "12 0.6788461208343506 \t 100\t\n",
      "13 0.678737998008728 \t 100\t\n",
      "14 0.6790534853935242 \t 100\t\n",
      "15 0.6789115071296692 \t 100\t\n",
      "16 0.6785424947738647 \t 100\t\n",
      "17 0.6786066889762878 \t 100\t\n",
      "18 0.6787896156311035 \t 100\t\n",
      "19 0.6787605285644531 \t 100\t\n",
      "20 0.6788294315338135 \t 100\t\n",
      "21 0.6785271167755127 \t 100\t\n",
      "22 0.6786279082298279 \t 100\t\n",
      "23 0.6788671016693115 \t 100\t\n",
      "24 0.6787713766098022 \t 100\t\n",
      "25 0.6787518858909607 \t 100\t\n",
      "26 0.678478479385376 \t 100\t\n",
      "27 0.6787803769111633 \t 100\t\n",
      "28 0.6786049604415894 \t 100\t\n",
      "29 0.6785382628440857 \t 100\t\n",
      "30 0.6788198947906494 \t 100\t\n",
      "31 0.6786969900131226 \t 100\t\n",
      "32 0.6787611246109009 \t 100\t\n",
      "33 0.6787558197975159 \t 100\t\n",
      "34 0.6787477731704712 \t 100\t\n",
      "35 0.6786507964134216 \t 100\t\n",
      "36 0.6788048148155212 \t 100\t\n",
      "37 0.6787508726119995 \t 100\t\n",
      "38 0.6787245869636536 \t 100\t\n",
      "39 0.6786854863166809 \t 100\t\n",
      "40 0.6785776615142822 \t 100\t\n",
      "41 0.6784966588020325 \t 100\t\n",
      "42 0.678674042224884 \t 100\t\n",
      "43 0.6787523627281189 \t 94\t\n",
      "44 0.6784734725952148 \t 100\t\n",
      "45 0.6786606311798096 \t 100\t\n",
      "46 0.6788427233695984 \t 100\t\n",
      "47 0.6789283752441406 \t 100\t\n",
      "48 0.6788767576217651 \t 100\t\n",
      "49 0.6787727475166321 \t 100\t\n",
      "51 0.6786976456642151 \t 100\t\n",
      "52 0.6786531805992126 \t 100\t\n",
      "53 0.6787914633750916 \t 100\t\n",
      "54 0.6787359714508057 \t 100\t\n",
      "55 0.6786573529243469 \t 100\t\n",
      "56 0.67853844165802 \t 100\t\n",
      "57 0.6787751317024231 \t 100\t\n",
      "58 0.6789833903312683 \t 100\t\n",
      "59 0.6786304116249084 \t 100\t\n",
      "60 0.6787526607513428 \t 100\t\n",
      "61 0.6787706017494202 \t 100\t\n",
      "62 0.6785318851470947 \t 100\t\n",
      "63 0.6785640120506287 \t 100\t\n",
      "64 0.6789607405662537 \t 100\t\n",
      "65 0.678598165512085 \t 100\t\n",
      "66 0.6787300705909729 \t 100\t\n",
      "67 0.6788697242736816 \t 100\t\n",
      "68 0.6786303520202637 \t 100\t\n",
      "69 0.6787620186805725 \t 100\t\n",
      "70 0.6786851286888123 \t 100\t\n",
      "71 0.6787732243537903 \t 100\t\n",
      "72 0.6787470579147339 \t 100\t\n",
      "73 0.6786297559738159 \t 100\t\n",
      "74 0.6786592602729797 \t 100\t\n",
      "75 0.6786634922027588 \t 100\t\n",
      "76 0.6787068843841553 \t 100\t\n",
      "77 0.6788414716720581 \t 100\t\n",
      "78 0.6786222457885742 \t 100\t\n",
      "79 0.678695797920227 \t 100\t\n",
      "80 0.6785991191864014 \t 100\t\n",
      "81 0.678560733795166 \t 100\t\n",
      "82 0.6787131428718567 \t 100\t\n",
      "83 0.6785870790481567 \t 100\t\n",
      "84 0.6785855293273926 \t 100\t\n",
      "85 0.6786553859710693 \t 100\t\n",
      "86 0.6786222457885742 \t 100\t\n",
      "87 0.6787727475166321 \t 100\t\n",
      "88 0.678824245929718 \t 100\t\n",
      "89 0.6786184906959534 \t 100\t\n",
      "90 0.67875736951828 \t 100\t\n",
      "91 0.6786792278289795 \t 100\t\n",
      "92 0.6789093613624573 \t 100\t\n",
      "93 0.6787317395210266 \t 100\t\n",
      "94 0.6788125038146973 \t 100\t\n",
      "95 0.678795337677002 \t 100\t\n",
      "96 0.67864990234375 \t 100\t\n",
      "97 0.6787506937980652 \t 100\t\n",
      "98 0.678737998008728 \t 100\t\n",
      "99 0.6785796284675598 \t 100\t\n",
      "\n",
      "===================================================\n",
      "10000000\n",
      "0.6757857166656779 \t 100\n",
      "0 0.6773370504379272 \t 100\t\n",
      "1 0.6773890256881714 \t 100\t\n",
      "2 0.6775267124176025 \t 100\t\n",
      "3 0.6775228977203369 \t 100\t\n",
      "4 0.6774517893791199 \t 100\t\n",
      "5 0.6776573657989502 \t 100\t\n",
      "6 0.6777623295783997 \t 100\t\n",
      "7 0.6775069236755371 \t 100\t\n",
      "8 0.6774346232414246 \t 100\t\n",
      "9 0.6777529120445251 \t 100\t\n",
      "10 0.6774334907531738 \t 100\t\n",
      "11 0.677515983581543 \t 100\t\n",
      "12 0.6776198744773865 \t 100\t\n",
      "13 0.6774360537528992 \t 100\t\n",
      "14 0.6774284839630127 \t 100\t\n",
      "15 0.6775581240653992 \t 100\t\n",
      "16 0.6777570843696594 \t 100\t\n",
      "17 0.6773850321769714 \t 100\t\n",
      "18 0.6774020791053772 \t 100\t\n",
      "19 0.6775873899459839 \t 100\t\n",
      "20 0.6775277853012085 \t 100\t\n",
      "21 0.6773185133934021 \t 100\t\n",
      "22 0.6773185729980469 \t 100\t\n",
      "23 0.6775904297828674 \t 100\t\n",
      "24 0.6774309277534485 \t 100\t\n",
      "25 0.6778125762939453 \t 100\t\n",
      "26 0.6774646043777466 \t 100\t\n",
      "27 0.6775654554367065 \t 100\t\n",
      "28 0.6775128245353699 \t 100\t\n",
      "29 0.6774923205375671 \t 100\t\n",
      "30 0.6776857376098633 \t 100\t\n",
      "31 0.6775512099266052 \t 100\t\n",
      "32 0.6775441765785217 \t 100\t\n",
      "33 0.6773367524147034 \t 100\t\n",
      "34 0.677562415599823 \t 100\t\n",
      "35 0.6773591041564941 \t 100\t\n",
      "36 0.6774616241455078 \t 100\t\n",
      "37 0.677363932132721 \t 100\t\n",
      "38 0.6772983074188232 \t 100\t\n",
      "39 0.6776465773582458 \t 100\t\n",
      "40 0.677497386932373 \t 100\t\n",
      "41 0.6773096919059753 \t 100\t\n",
      "42 0.6776283979415894 \t 100\t\n",
      "43 0.6779935956001282 \t 100\t\n",
      "44 0.6776748895645142 \t 100\t\n",
      "45 0.67763352394104 \t 100\t\n",
      "46 0.6773386001586914 \t 100\t\n",
      "47 0.6775578260421753 \t 100\t\n",
      "48 0.6774654984474182 \t 100\t\n",
      "49 0.677450954914093 \t 100\t\n",
      "50 0.6773912906646729 \t 100\t\n",
      "51 0.6775532960891724 \t 100\t\n",
      "52 0.6773650050163269 \t 100\t\n",
      "53 0.6775807738304138 \t 100\t\n",
      "54 0.6775400042533875 \t 100\t\n",
      "55 0.6775977611541748 \t 100\t\n",
      "56 0.6774966716766357 \t 100\t\n",
      "57 0.6774272918701172 \t 100\t\n",
      "58 0.6774783730506897 \t 100\t\n",
      "59 0.6773797273635864 \t 100\t\n",
      "60 0.6773067712783813 \t 100\t\n",
      "61 0.6774027943611145 \t 100\t\n",
      "62 0.6772955060005188 \t 100\t\n",
      "63 0.6776471138000488 \t 100\t\n",
      "64 0.6780077219009399 \t 100\t\n",
      "65 0.6775010228157043 \t 100\t\n",
      "66 0.6773729920387268 \t 100\t\n",
      "67 0.6773943901062012 \t 100\t\n",
      "68 0.677547037601471 \t 100\t\n",
      "69 0.6774889230728149 \t 100\t\n",
      "70 0.6774541139602661 \t 100\t\n",
      "71 0.6775093078613281 \t 100\t\n",
      "72 0.6775006055831909 \t 100\t\n",
      "73 0.6773755550384521 \t 100\t\n",
      "74 0.6775882840156555 \t 100\t\n",
      "75 0.6779558062553406 \t 100\t\n",
      "76 0.6772891879081726 \t 100\t\n",
      "77 0.6774569749832153 \t 100\t\n",
      "78 0.6776042580604553 \t 100\t\n",
      "79 0.6773159503936768 \t 100\t\n",
      "80 0.677539587020874 \t 100\t\n",
      "81 0.677402913570404 \t 100\t\n",
      "82 0.6775059700012207 \t 100\t\n",
      "83 0.6781743168830872 \t 100\t\n",
      "84 0.6775573492050171 \t 100\t\n",
      "85 0.6772884726524353 \t 100\t\n",
      "86 0.6775665283203125 \t 100\t\n",
      "87 0.6776536703109741 \t 100\t\n",
      "88 0.6775360703468323 \t 100\t\n",
      "89 0.6774194836616516 \t 100\t\n",
      "90 0.6775185465812683 \t 100\t\n",
      "91 0.6776863932609558 \t 100\t\n",
      "92 0.6777142286300659 \t 100\t\n",
      "93 0.6778095960617065 \t 100\t\n",
      "94 0.6774532198905945 \t 100\t\n",
      "95 0.6777756810188293 \t 100\t\n",
      "96 0.6774687767028809 \t 100\t\n",
      "97 0.6775112748146057 \t 100\t\n",
      "98 0.6774553060531616 \t 100\t\n",
      "99 0.6774296760559082 \t 100\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for N in Ns:\n",
    "    print('===================================================\\n{}'.format(N))\n",
    "    # Take the first N samples.\n",
    "    data, m, s = split_data(X[:N], y[:N])\n",
    "    \n",
    "    # Train BDT model (only need to train 1)\n",
    "    bdt_model = XGBClassifier(early_stopping_rounds = 10)\n",
    "    X_trn, X_vld, y_trn, y_vld = data\n",
    "    bdt_model.fit(X_trn, y_trn, eval_set = [(X_vld, y_vld)], verbose = 0)\n",
    "    trace = bdt_model.evals_result()['validation_0']\n",
    "    print(trace['logloss'][-1], '\\t', len(trace['logloss']), end = '\\n')\n",
    "    bdt_model.save_model(bdt_filestr.format(N))\n",
    "\n",
    "    for i in range(reps):\n",
    "        print(i, end = ' ')\n",
    "        # Train BCE model\n",
    "        bce_model, trace = train(data, **bce_params)\n",
    "        bce_model.save_weights(bce_filestr.format(N, i))\n",
    "        \n",
    "        # Train GBC model\n",
    "        gbc_model = GradientBoostingClassifier(validation_fraction = 0.25,\n",
    "                                               n_iter_no_change = 10)\n",
    "        gbc_model.fit(X[:N], y[:N])\n",
    "        dump(gbc_model, gbc_filestr.format(N, i))\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd8d17d-d03a-4d22-9a8f-537bfb822352",
   "metadata": {
    "tags": []
   },
   "source": [
    "## $d = 8$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d23a752-df10-4f18-bc9f-222e20f10212",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/zenodo/8/X_trn.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     os\u001b[38;5;241m.\u001b[39mmkdir(filestr \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgbc/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Data parameters\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/zenodo/\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m/X_trn.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/zenodo/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/y_trn.npy\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(d))\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/clusterfs/ml4hep/shahzar/miniconda/envs/multifold/lib/python3.10/site-packages/numpy/lib/npyio.py:390\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    388\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 390\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    391\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/zenodo/8/X_trn.npy'"
     ]
    }
   ],
   "source": [
    "# Experiment parameters\n",
    "num = 0\n",
    "reps = 100\n",
    "d = 8\n",
    "Ns = 10**np.arange(2, 8)\n",
    "\n",
    "# Model parameters\n",
    "bce_params = {'loss':bce, 'd': d}\n",
    "\n",
    "filestr = 'models/zenodo/{}/set_{}/'.format(d, num)\n",
    "bce_filestr = filestr + 'bce/model_{}_{}.h5'\n",
    "bdt_filestr = filestr + 'bdt/model_{}.h5'\n",
    "gbc_filestr = filestr + 'gbc/model_{}_{}.h5'\n",
    "\n",
    "if not os.path.isdir(filestr):\n",
    "    os.mkdir(filestr)\n",
    "\n",
    "if not os.path.isdir(filestr + 'bce/'):\n",
    "    os.mkdir(filestr + 'bce/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'bdt/'):\n",
    "    os.mkdir(filestr + 'bdt/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'gbc/'):\n",
    "    os.mkdir(filestr + 'gbc/')\n",
    "\n",
    "# Data parameters\n",
    "X = np.load('data/zenodo/{}/X_trn.npy'.format(d))\n",
    "y = np.load('data/zenodo/{}/y_trn.npy'.format(d)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a5ea4b-dc38-470e-a65d-9c3217dcc73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for N in Ns:\n",
    "    print('===================================================\\n{}'.format(N))\n",
    "    # Take the first N samples.\n",
    "    data, m, s = split_data(X[:N], y[:N])\n",
    "    \n",
    "    # Train BDT model (only need to train 1)\n",
    "    bdt_model = XGBClassifier(early_stopping_rounds = 10)\n",
    "    X_trn, X_vld, y_trn, y_vld = data\n",
    "    bdt_model.fit(X_trn, y_trn, eval_set = [(X_vld, y_vld)], verbose = 0)\n",
    "    trace = bdt_model.evals_result()['validation_0']\n",
    "    print(trace['logloss'][-1], '\\t', len(trace['logloss']), end = '\\n')\n",
    "    bdt_model.save_model(bdt_filestr.format(N))\n",
    "\n",
    "    for i in range(reps):\n",
    "        print(i, end = ' ')\n",
    "        # Train BCE model\n",
    "        bce_model, trace = train(data, **bce_params)\n",
    "        bce_model.save_weights(bce_filestr.format(N, i))\n",
    "        \n",
    "        # Train GBC model\n",
    "        gbc_model = GradientBoostingClassifier(validation_fraction = 0.25,\n",
    "                                               n_iter_no_change = 10)\n",
    "        gbc_model.fit(X[:N], y[:N])\n",
    "        dump(gbc_model, gbc_filestr.format(N, i))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef99106-1889-4016-afdc-36c47f2bb0a2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## $d = 11$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa77dcda-85fc-4e15-afc9-2081a23d9796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "num = 0\n",
    "reps = 100\n",
    "d = 11\n",
    "Ns = 10**np.arange(2, 8)\n",
    "\n",
    "# Model parameters\n",
    "bce_params = {'loss':bce, 'd': d}\n",
    "\n",
    "filestr = 'models/zenodo/{}/set_{}/'.format(d, num)\n",
    "bce_filestr = filestr + 'bce/model_{}_{}.h5'\n",
    "bdt_filestr = filestr + 'bdt/model_{}.h5'\n",
    "gbc_filestr = filestr + 'gbc/model_{}_{}.h5'\n",
    "\n",
    "if not os.path.isdir(filestr):\n",
    "    os.mkdir(filestr)\n",
    "\n",
    "if not os.path.isdir(filestr + 'bce/'):\n",
    "    os.mkdir(filestr + 'bce/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'bdt/'):\n",
    "    os.mkdir(filestr + 'bdt/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'gbc/'):\n",
    "    os.mkdir(filestr + 'gbc/')\n",
    "\n",
    "# Data parameters\n",
    "X = np.load('data/zenodo/{}/X_trn.npy'.format(d))\n",
    "y = np.load('data/zenodo/{}/y_trn.npy'.format(d)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918ba1cb-8b9c-4545-bb0d-9ddbaf89bfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for N in Ns:\n",
    "    print('===================================================\\n{}'.format(N))\n",
    "    # Take the first N samples.\n",
    "    data, m, s = split_data(X[:N], y[:N])\n",
    "    \n",
    "    # Train BDT model (only need to train 1)\n",
    "    bdt_model = XGBClassifier(early_stopping_rounds = 10)\n",
    "    X_trn, X_vld, y_trn, y_vld = data\n",
    "    bdt_model.fit(X_trn, y_trn, eval_set = [(X_vld, y_vld)], verbose = 0)\n",
    "    trace = bdt_model.evals_result()['validation_0']\n",
    "    print(trace['logloss'][-1], '\\t', len(trace['logloss']), end = '\\n')\n",
    "    bdt_model.save_model(bdt_filestr.format(N))\n",
    "\n",
    "    for i in range(reps):\n",
    "        print(i, end = ' ')\n",
    "        # Train BCE model\n",
    "        bce_model, trace = train(data, **bce_params)\n",
    "        bce_model.save_weights(bce_filestr.format(N, i))\n",
    "        \n",
    "        # Train GBC model\n",
    "        gbc_model = GradientBoostingClassifier(validation_fraction = 0.25,\n",
    "                                               n_iter_no_change = 10)\n",
    "        gbc_model.fit(X[:N], y[:N])\n",
    "        dump(gbc_model, gbc_filestr.format(N, i))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multifold",
   "language": "python",
   "name": "multifold"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
