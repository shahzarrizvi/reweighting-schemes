{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7ced574-924b-4fd3-9444-614365da7fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically reload modules after executing each cell.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d2777dc-b825-4800-bc82-d1b23953c263",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# General imports\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from joblib import dump, load \n",
    "\n",
    "# Utility imports\n",
    "from utils.losses import *\n",
    "from utils.plotting import *\n",
    "from utils.training import *\n",
    "\n",
    "np.random.seed(666) # Need to do more to ensure data is the same across runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18c301cf-f760-4479-aeef-27c5a063035f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # pick a number < 4 on ML4HEP; < 3 on Voltan \n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eff551-de89-49c5-a2c0-a1c15ae11183",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Vertical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d526a150-2df1-4284-b6da-d1b4959d5352",
   "metadata": {
    "tags": []
   },
   "source": [
    "## $d = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cae75b-8b8b-44bb-a06d-78c677817809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "num = 0\n",
    "reps = 100\n",
    "d = 1\n",
    "Ns = 10**np.arange(2, 8)\n",
    "\n",
    "# Model parameters\n",
    "bce_params = {'loss':bce, 'd': d}\n",
    "\n",
    "filestr = 'models/trees/{}/set_{}/'.format(d, num)\n",
    "bce_filestr = filestr + 'bce/model_{}_{}.h5'\n",
    "bdt_filestr = filestr + 'bdt/model_{}.h5'\n",
    "gbc_filestr = filestr + 'gbc/model_{}_{}.h5'\n",
    "\n",
    "if not os.path.isdir(filestr):\n",
    "    os.mkdir(filestr)\n",
    "\n",
    "if not os.path.isdir(filestr + 'bce/'):\n",
    "    os.mkdir(filestr + 'bce/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'bdt/'):\n",
    "    os.mkdir(filestr + 'bdt/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'gbc/'):\n",
    "    os.mkdir(filestr + 'gbc/')\n",
    "\n",
    "# Data parameters\n",
    "X = np.load('data/trees/{}/X_trn.npy'.format(d)).reshape(-1, 1)\n",
    "y = np.load('data/trees/{}/y_trn.npy'.format(d)).astype('float32')\n",
    "\n",
    "bkgd = stats.norm(-0.1, 1)\n",
    "sgnl = stats.norm(+0.1, 1)\n",
    "\n",
    "lr = make_lr(bkgd, sgnl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c89e58-6e56-4b84-9385-049327bed049",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10**6\n",
    "data, m, s = split_data(X[:N], y[:N])\n",
    "\n",
    "bce_lrs = [None] * reps\n",
    "gbc_lrs = [None] * reps\n",
    "for i in range(reps):\n",
    "    print(i, end = ' ')\n",
    "    bce_model = create_model(**bce_params)\n",
    "    bce_model.load_weights(bce_filestr.format(N, i))\n",
    "    bce_lrs[i] = odds_lr(bce_model, m, s)\n",
    "\n",
    "    gbc_model = load(gbc_filestr.format(N, i))\n",
    "    gbc_lrs[i] = tree_lr(gbc_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5856c227-271c-440f-95d0-3e93becf92e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.linspace(-6, 6, 1201).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ca7c19-d2bc-4bc6-b872-8a44b142272f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bce_preds = get_preds(bce_lrs, xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19a6431-0b68-4d35-a8c3-8c37c3ea4db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc_preds = get_preds(gbc_lrs, xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b130a1cc-e58a-4743-9016-e06387cf46bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_bce = bce_preds.mean(axis = 0)\n",
    "avg_gbc = gbc_preds.mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09e0a31-4089-402d-ab48-f62fb0d35123",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_plot([bce_preds, gbc_preds], ['BCE', 'GBC'], lr, xs.reshape(-1), \n",
    "           figsize = (w, h), title = '\\it Likelihood Ratio Models', \n",
    "           filename = 'plots/lr_models.png') "
   ]
  },
  {
   "cell_type": "raw",
   "id": "c2945cb9-735d-4029-a433-32efe7aa5f8b",
   "metadata": {},
   "source": [
    "for N in Ns:\n",
    "    print('===================================================\\n{}'.format(N))\n",
    "    # Take the first N samples.\n",
    "    data, m, s = split_data(X[:N], y[:N])\n",
    "    \n",
    "    # Train BDT model (only need to train 1)\n",
    "    #bdt_model = XGBClassifier(early_stopping_rounds = 10)\n",
    "    #X_trn, X_vld, y_trn, y_vld = data\n",
    "    #bdt_model.fit(X_trn, y_trn, eval_set = [(X_vld, y_vld)], verbose = 0)\n",
    "    #trace = bdt_model.evals_result()['validation_0']\n",
    "    #print(trace['logloss'][-1], '\\t', len(trace['logloss']), end = '\\n')\n",
    "    #bdt_model.save_model(bdt_filestr.format(N))\n",
    "\n",
    "    for i in range(reps):\n",
    "        print(i, end = ' ')\n",
    "        # Train BCE model\n",
    "        #bce_model, trace = train(data, **bce_params)\n",
    "        #bce_model.save_weights(bce_filestr.format(N, i))\n",
    "        \n",
    "        # Train GBC model\n",
    "        gbc_model = GradientBoostingClassifier(validation_fraction = 0.25,\n",
    "                                               n_iter_no_change = 10)\n",
    "        gbc_model.fit(X[:N], y[:N])\n",
    "        dump(gbc_model, gbc_filestr.format(N, i))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78d6730-4b33-40bd-8abb-acf1be1d2f30",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## $d = 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12422843-2cf2-4280-b1b4-aae3127e5cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "num = 0\n",
    "reps = 100\n",
    "d = 2\n",
    "Ns = 10**np.arange(2, 8)\n",
    "\n",
    "# Model parameters\n",
    "bce_params = {'loss':bce, 'd': d}\n",
    "\n",
    "filestr = 'models/trees/{}/set_{}/'.format(d, num)\n",
    "bce_filestr = filestr + 'bce/model_{}_{}.h5'\n",
    "bdt_filestr = filestr + 'bdt/model_{}.h5'\n",
    "gbc_filestr = filestr + 'gbc/model_{}_{}.h5'\n",
    "\n",
    "if not os.path.isdir(filestr):\n",
    "    os.mkdir(filestr)\n",
    "\n",
    "if not os.path.isdir(filestr + 'bce/'):\n",
    "    os.mkdir(filestr + 'bce/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'bdt/'):\n",
    "    os.mkdir(filestr + 'bdt/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'gbc/'):\n",
    "    os.mkdir(filestr + 'gbc/')\n",
    "\n",
    "# Data parameters\n",
    "X = np.load('data/trees/{}/X_trn.npy'.format(d))\n",
    "y = np.load('data/trees/{}/y_trn.npy'.format(d)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1a1f4c-7d62-4617-9eaf-1c2ab3f56bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for N in Ns:\n",
    "    print('===================================================\\n{}'.format(N))\n",
    "    # Take the first N samples.\n",
    "    data, m, s = split_data(X[:N], y[:N])\n",
    "    \n",
    "    # Train BDT model (only need to train 1)\n",
    "    #bdt_model = XGBClassifier(early_stopping_rounds = 10)\n",
    "    #X_trn, X_vld, y_trn, y_vld = data\n",
    "    #bdt_model.fit(X_trn, y_trn, eval_set = [(X_vld, y_vld)], verbose = 0)\n",
    "    #trace = bdt_model.evals_result()['validation_0']\n",
    "    #print(trace['logloss'][-1], '\\t', len(trace['logloss']), end = '\\n')\n",
    "    #bdt_model.save_model(bdt_filestr.format(N))\n",
    "\n",
    "    for i in range(reps):\n",
    "        print(i, end = ' ')\n",
    "        # Train BCE model\n",
    "        #bce_model, trace = train(data, **bce_params)\n",
    "        #bce_model.save_weights(bce_filestr.format(N, i))\n",
    "        \n",
    "        # Train GBC model\n",
    "        gbc_model = GradientBoostingClassifier(validation_fraction = 0.25,\n",
    "                                               n_iter_no_change = 10)\n",
    "        gbc_model.fit(X[:N], y[:N])\n",
    "        dump(gbc_model, gbc_filestr.format(N, i))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d942469-7ef3-4907-87eb-c9296e3409bb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## $d=4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9f85bd-b60b-473c-bcc8-373a9d120f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "num = 0\n",
    "reps = 100\n",
    "d = 4\n",
    "Ns = 10**np.arange(2, 8)\n",
    "\n",
    "# Model parameters\n",
    "bce_params = {'loss':bce, 'd': d}\n",
    "\n",
    "filestr = 'models/trees/{}/set_{}/'.format(d, num)\n",
    "bce_filestr = filestr + 'bce/model_{}_{}.h5'\n",
    "bdt_filestr = filestr + 'bdt/model_{}.h5'\n",
    "gbc_filestr = filestr + 'gbc/model_{}_{}.h5'\n",
    "\n",
    "if not os.path.isdir(filestr):\n",
    "    os.mkdir(filestr)\n",
    "\n",
    "if not os.path.isdir(filestr + 'bce/'):\n",
    "    os.mkdir(filestr + 'bce/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'bdt/'):\n",
    "    os.mkdir(filestr + 'bdt/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'gbc/'):\n",
    "    os.mkdir(filestr + 'gbc/')\n",
    "\n",
    "# Data parameters\n",
    "X = np.load('data/trees/{}/X_trn.npy'.format(d))\n",
    "y = np.load('data/trees/{}/y_trn.npy'.format(d)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb3e63d-4966-43eb-af89-f6ae1d3370eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for N in Ns:\n",
    "    print('===================================================\\n{}'.format(N))\n",
    "    # Take the first N samples.\n",
    "    data, m, s = split_data(X[:N], y[:N])\n",
    "    \n",
    "    # Train BDT model (only need to train 1)\n",
    "    #bdt_model = XGBClassifier(early_stopping_rounds = 10)\n",
    "    #X_trn, X_vld, y_trn, y_vld = data\n",
    "    #bdt_model.fit(X_trn, y_trn, eval_set = [(X_vld, y_vld)], verbose = 0)\n",
    "    #trace = bdt_model.evals_result()['validation_0']\n",
    "    #print(trace['logloss'][-1], '\\t', len(trace['logloss']), end = '\\n')\n",
    "    #bdt_model.save_model(bdt_filestr.format(N))\n",
    "\n",
    "    for i in range(91, reps):\n",
    "        print(i, end = ' ')\n",
    "        # Train BCE model\n",
    "        #bce_model, trace = train(data, **bce_params)\n",
    "        #bce_model.save_weights(bce_filestr.format(N, i))\n",
    "        \n",
    "        # Train GBC model\n",
    "        gbc_model = GradientBoostingClassifier(validation_fraction = 0.25,\n",
    "                                               n_iter_no_change = 10)\n",
    "        gbc_model.fit(X[:N], y[:N])\n",
    "        dump(gbc_model, gbc_filestr.format(N, i))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1b039a-fa91-4a35-8233-c4a017babc44",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## $d=8$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe81baf6-0352-4b70-bcc8-ff37baeac13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "num = 0\n",
    "reps = 100\n",
    "d = 8\n",
    "Ns = 10**np.arange(2, 8)\n",
    "\n",
    "# Model parameters\n",
    "bce_params = {'loss':bce, 'd': d}\n",
    "\n",
    "filestr = 'models/trees/{}/set_{}/'.format(d, num)\n",
    "bce_filestr = filestr + 'bce/model_{}_{}.h5'\n",
    "bdt_filestr = filestr + 'bdt/model_{}.h5'\n",
    "gbc_filestr = filestr + 'gbc/model_{}_{}.h5'\n",
    "\n",
    "if not os.path.isdir(filestr):\n",
    "    os.mkdir(filestr)\n",
    "\n",
    "if not os.path.isdir(filestr + 'bce/'):\n",
    "    os.mkdir(filestr + 'bce/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'bdt/'):\n",
    "    os.mkdir(filestr + 'bdt/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'gbc/'):\n",
    "    os.mkdir(filestr + 'gbc/')\n",
    "\n",
    "# Data parameters\n",
    "X = np.load('data/trees/{}/X_trn.npy'.format(d))\n",
    "y = np.load('data/trees/{}/y_trn.npy'.format(d)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e72def4-b63c-46e2-92d5-1d724b5d079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for N in Ns:\n",
    "    print('===================================================\\n{}'.format(N))\n",
    "    # Take the first N samples.\n",
    "    data, m, s = split_data(X[:N], y[:N])\n",
    "    \n",
    "    # Train BDT model (only need to train 1)\n",
    "    #bdt_model = XGBClassifier(early_stopping_rounds = 10)\n",
    "    #X_trn, X_vld, y_trn, y_vld = data\n",
    "    #bdt_model.fit(X_trn, y_trn, eval_set = [(X_vld, y_vld)], verbose = 0)\n",
    "    #trace = bdt_model.evals_result()['validation_0']\n",
    "    #print(trace['logloss'][-1], '\\t', len(trace['logloss']), end = '\\n')\n",
    "    #bdt_model.save_model(bdt_filestr.format(N))\n",
    "\n",
    "    for i in range(reps):\n",
    "        print(i, end = ' ')\n",
    "        # Train BCE model\n",
    "        #bce_model, trace = train(data, **bce_params)\n",
    "        #bce_model.save_weights(bce_filestr.format(N, i))\n",
    "        \n",
    "        # Train GBC model\n",
    "        gbc_model = GradientBoostingClassifier(validation_fraction = 0.25,\n",
    "                                               n_iter_no_change = 10)\n",
    "        gbc_model.fit(X[:N], y[:N])\n",
    "        dump(gbc_model, gbc_filestr.format(N, i))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7b33f6-be09-404b-bbca-7e567a651b1b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## $d=16$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed78be7a-1c20-4cde-bd24-d5a46012377e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "num = 0\n",
    "reps = 100\n",
    "d = 16\n",
    "Ns = 10**np.arange(2, 8)\n",
    "\n",
    "# Model parameters\n",
    "bce_params = {'loss':bce, 'd': d}\n",
    "\n",
    "filestr = 'models/trees/{}/set_{}/'.format(d, num)\n",
    "bce_filestr = filestr + 'bce/model_{}_{}.h5'\n",
    "bdt_filestr = filestr + 'bdt/model_{}.h5'\n",
    "gbc_filestr = filestr + 'gbc/model_{}_{}.h5'\n",
    "\n",
    "if not os.path.isdir(filestr):\n",
    "    os.mkdir(filestr)\n",
    "\n",
    "if not os.path.isdir(filestr + 'bce/'):\n",
    "    os.mkdir(filestr + 'bce/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'bdt/'):\n",
    "    os.mkdir(filestr + 'bdt/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'gbc/'):\n",
    "    os.mkdir(filestr + 'gbc/')\n",
    "\n",
    "# Data parameters\n",
    "X = np.load('data/trees/{}/X_trn.npy'.format(d))\n",
    "y = np.load('data/trees/{}/y_trn.npy'.format(d)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5262859-deb7-413f-b674-9c84a157ad73",
   "metadata": {},
   "outputs": [],
   "source": [
    "for N in Ns:\n",
    "    print('===================================================\\n{}'.format(N))\n",
    "    # Take the first N samples.\n",
    "    data, m, s = split_data(X[:N], y[:N])\n",
    "    \n",
    "    # Train BDT model (only need to train 1)\n",
    "    #bdt_model = XGBClassifier(early_stopping_rounds = 10)\n",
    "    #X_trn, X_vld, y_trn, y_vld = data\n",
    "    #bdt_model.fit(X_trn, y_trn, eval_set = [(X_vld, y_vld)], verbose = 0)\n",
    "    #trace = bdt_model.evals_result()['validation_0']\n",
    "    #print(trace['logloss'][-1], '\\t', len(trace['logloss']), end = '\\n')\n",
    "    #bdt_model.save_model(bdt_filestr.format(N))\n",
    "\n",
    "    for i in range(reps):\n",
    "        print(i, end = ' ')\n",
    "        # Train BCE model\n",
    "        #bce_model, trace = train(data, **bce_params)\n",
    "        #bce_model.save_weights(bce_filestr.format(N, i))\n",
    "        \n",
    "        # Train GBC model\n",
    "        gbc_model = GradientBoostingClassifier(validation_fraction = 0.25,\n",
    "                                               n_iter_no_change = 10)\n",
    "        gbc_model.fit(X[:N], y[:N])\n",
    "        dump(gbc_model, gbc_filestr.format(N, i))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ca2309-9077-432f-933c-77be9ede35e1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## $d = 32$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f08136-c5dc-4649-989b-4f4cea166cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "num = 0\n",
    "reps = 100\n",
    "d = 32\n",
    "Ns = 10**np.arange(2, 8)\n",
    "\n",
    "# Model parameters\n",
    "bce_params = {'loss':bce, 'd': d}\n",
    "\n",
    "filestr = 'models/trees/{}/set_{}/'.format(d, num)\n",
    "bce_filestr = filestr + 'bce/model_{}_{}.h5'\n",
    "bdt_filestr = filestr + 'bdt/model_{}.h5'\n",
    "gbc_filestr = filestr + 'gbc/model_{}_{}.h5'\n",
    "\n",
    "if not os.path.isdir(filestr):\n",
    "    os.mkdir(filestr)\n",
    "\n",
    "if not os.path.isdir(filestr + 'bce/'):\n",
    "    os.mkdir(filestr + 'bce/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'bdt/'):\n",
    "    os.mkdir(filestr + 'bdt/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'gbc/'):\n",
    "    os.mkdir(filestr + 'gbc/')\n",
    "\n",
    "# Data parameters\n",
    "X = np.load('data/trees/{}/X_trn.npy'.format(d))\n",
    "y = np.load('data/trees/{}/y_trn.npy'.format(d)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f9e44b-a9fb-49c9-923d-9c414e2c6a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "for N in Ns:\n",
    "    print('===================================================\\n{}'.format(N))\n",
    "    # Take the first N samples.\n",
    "    data, m, s = split_data(X[:N], y[:N])\n",
    "    \n",
    "    # Train BDT model (only need to train 1)\n",
    "    #bdt_model = XGBClassifier(early_stopping_rounds = 10)\n",
    "    #X_trn, X_vld, y_trn, y_vld = data\n",
    "    #bdt_model.fit(X_trn, y_trn, eval_set = [(X_vld, y_vld)], verbose = 0)\n",
    "    #trace = bdt_model.evals_result()['validation_0']\n",
    "    #print(trace['logloss'][-1], '\\t', len(trace['logloss']), end = '\\n')\n",
    "    #bdt_model.save_model(bdt_filestr.format(N))\n",
    "\n",
    "    for i in range(reps):\n",
    "        print(i, end = ' ')\n",
    "        # Train BCE model\n",
    "        #bce_model, trace = train(data, **bce_params)\n",
    "        #bce_model.save_weights(bce_filestr.format(N, i))\n",
    "        \n",
    "        # Train GBC model\n",
    "        gbc_model = GradientBoostingClassifier(validation_fraction = 0.25,\n",
    "                                               n_iter_no_change = 10)\n",
    "        gbc_model.fit(X[:N], y[:N])\n",
    "        dump(gbc_model, gbc_filestr.format(N, i))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ed990a-9a86-46df-a2b5-45d88cdb0e88",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Zenodo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c7beb4-a137-4bfc-a7a5-5cbc00b6b5ee",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## $d = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "982a7583-5b0d-48b8-b4f1-c37fda6ba098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "num = 0\n",
    "reps = 100\n",
    "d = 1\n",
    "Ns = 10**np.arange(2, 8)\n",
    "\n",
    "# Model parameters\n",
    "bce_params = {'loss':bce, 'd': d}\n",
    "\n",
    "filestr = 'models/zenodo/{}/set_{}/'.format(d, num)\n",
    "bce_filestr = filestr + 'bce/model_{}_{}.h5'\n",
    "bdt_filestr = filestr + 'bdt/model_{}.h5'\n",
    "gbc_filestr = filestr + 'gbc/model_{}_{}.h5'\n",
    "\n",
    "if not os.path.isdir(filestr):\n",
    "    os.mkdir(filestr)\n",
    "\n",
    "if not os.path.isdir(filestr + 'bce/'):\n",
    "    os.mkdir(filestr + 'bce/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'bdt/'):\n",
    "    os.mkdir(filestr + 'bdt/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'gbc/'):\n",
    "    os.mkdir(filestr + 'gbc/')\n",
    "\n",
    "# Data parameters\n",
    "X = np.load('data/zenodo/{}/X_trn.npy'.format(d)).reshape(-1, 1)\n",
    "y = np.load('data/zenodo/{}/y_trn.npy'.format(d)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "faa8fdd1-bf3c-49dd-9e78-a2051e91857e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================\n",
      "10000000\n",
      "0.6910528750873685 \t 45\n",
      "87 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-10 19:32:06.351600: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-10 19:32:07.004419: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22243 MB memory:  -> device: 0, name: Quadro RTX 6000, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6913653612136841 \t 100\t\n",
      "88 0.6912950277328491 \t 100\t\n",
      "89 0.6913024187088013 \t 100\t\n",
      "90 0.6912943720817566 \t 100\t\n",
      "91 0.6913145780563354 \t 100\t\n",
      "92 0.691287100315094 \t 100\t\n",
      "93 0.6912788152694702 \t 100\t\n",
      "94 0.6912984848022461 \t 100\t\n",
      "95 0.6912990808486938 \t 100\t\n",
      "96 0.6913532018661499 \t 100\t\n",
      "97 0.6912767887115479 \t 100\t\n",
      "98 0.6912504434585571 \t 100\t\n",
      "99 0.6913511753082275 \t 100\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Ns = [10**7]\n",
    "for N in Ns:\n",
    "    print('===================================================\\n{}'.format(N))\n",
    "    # Take the first N samples.\n",
    "    data, m, s = split_data(X[:N], y[:N])\n",
    "    \n",
    "    # Train BDT model (only need to train 1)\n",
    "    bdt_model = XGBClassifier(early_stopping_rounds = 10)\n",
    "    X_trn, X_vld, y_trn, y_vld = data\n",
    "    bdt_model.fit(X_trn, y_trn, eval_set = [(X_vld, y_vld)], verbose = 0)\n",
    "    trace = bdt_model.evals_result()['validation_0']\n",
    "    print(trace['logloss'][-1], '\\t', len(trace['logloss']), end = '\\n')\n",
    "    bdt_model.save_model(bdt_filestr.format(N))\n",
    "\n",
    "    for i in range(87, reps):\n",
    "        print(i, end = ' ')\n",
    "        # Train BCE model\n",
    "        bce_model, trace = train(data, **bce_params)\n",
    "        bce_model.save_weights(bce_filestr.format(N, i))\n",
    "        \n",
    "        # Train GBC model\n",
    "        gbc_model = GradientBoostingClassifier(validation_fraction = 0.25,\n",
    "                                               n_iter_no_change = 10)\n",
    "        gbc_model.fit(X[:N], y[:N])\n",
    "        dump(gbc_model, gbc_filestr.format(N, i))\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf622c3-cfce-4a54-b8d5-2dca4a76c167",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## $d = 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c07e462f-21a5-4517-a390-57d7e4e498fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "num = 0\n",
    "reps = 100\n",
    "d = 2\n",
    "Ns = 10**np.arange(2, 8)\n",
    "\n",
    "# Model parameters\n",
    "bce_params = {'loss':bce, 'd': d}\n",
    "\n",
    "filestr = 'models/zenodo/{}/set_{}/'.format(d, num)\n",
    "bce_filestr = filestr + 'bce/model_{}_{}.h5'\n",
    "bdt_filestr = filestr + 'bdt/model_{}.h5'\n",
    "gbc_filestr = filestr + 'gbc/model_{}_{}.h5'\n",
    "\n",
    "if not os.path.isdir(filestr):\n",
    "    os.mkdir(filestr)\n",
    "\n",
    "if not os.path.isdir(filestr + 'bce/'):\n",
    "    os.mkdir(filestr + 'bce/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'bdt/'):\n",
    "    os.mkdir(filestr + 'bdt/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'gbc/'):\n",
    "    os.mkdir(filestr + 'gbc/')\n",
    "\n",
    "# Data parameters\n",
    "X = np.load('data/zenodo/{}/X_trn.npy'.format(d))\n",
    "y = np.load('data/zenodo/{}/y_trn.npy'.format(d)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "201c1e2a-1e85-4736-b009-fcac789b5d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================\n",
      "100\n",
      "0.8233430308103561 \t 11\n",
      "0 0.7209652066230774 \t 11\t\n",
      "1 0.7217194437980652 \t 11\t\n",
      "2 0.7222219705581665 \t 12\t\n",
      "3 0.718004584312439 \t 11\t\n",
      "4 0.7417144179344177 \t 12\t\n",
      "5 0.7358759045600891 \t 14\t\n",
      "6 0.7359951734542847 \t 11\t\n",
      "7 0.7287700176239014 \t 11\t\n",
      "8 0.7308948040008545 \t 12\t\n",
      "9 0.7268415689468384 \t 15\t\n",
      "10 0.7184034585952759 \t 11\t\n",
      "11 0.7192142605781555 \t 11\t\n",
      "12 0.7209107875823975 \t 11\t\n",
      "13 0.7087037563323975 \t 11\t\n",
      "14 0.7129851579666138 \t 11\t\n",
      "15 0.7363764047622681 \t 11\t\n",
      "16 0.7295403480529785 \t 11\t\n",
      "17 0.7196835279464722 \t 11\t\n",
      "18 0.7183862924575806 \t 11\t\n",
      "19 0.7125200629234314 \t 11\t\n",
      "20 0.7262614369392395 \t 11\t\n",
      "21 0.7214400768280029 \t 16\t\n",
      "22 0.7001100182533264 \t 11\t\n",
      "23 0.7420414090156555 \t 11\t\n",
      "24 0.7349517941474915 \t 11\t\n",
      "25 0.7225614190101624 \t 11\t\n",
      "26 0.7295057773590088 \t 15\t\n",
      "27 0.7269763350486755 \t 13\t\n",
      "28 0.7192058563232422 \t 11\t\n",
      "29 0.7252163887023926 \t 11\t\n",
      "30 0.7308338284492493 \t 11\t\n",
      "31 0.7255899310112 \t 13\t\n",
      "32 0.7219876050949097 \t 11\t\n",
      "33 0.755247950553894 \t 14\t\n",
      "34 0.7198649644851685 \t 11\t\n",
      "35 0.7083088159561157 \t 11\t\n",
      "36 0.7348257303237915 \t 11\t\n",
      "37 0.7233579754829407 \t 11\t\n",
      "38 0.7383167743682861 \t 17\t\n",
      "39 0.7172127366065979 \t 11\t\n",
      "40 0.7174245715141296 \t 11\t\n",
      "41 0.7378077507019043 \t 11\t\n",
      "42 0.7173307538032532 \t 11\t\n",
      "43 0.7129726409912109 \t 11\t\n",
      "44 0.7232579588890076 \t 12\t\n",
      "45 0.7350007891654968 \t 11\t\n",
      "46 0.7142124176025391 \t 17\t\n",
      "47 0.725799560546875 \t 12\t\n",
      "48 0.7091288566589355 \t 11\t\n",
      "49 0.7255232334136963 \t 18\t\n",
      "50 0.7124450206756592 \t 11\t\n",
      "51 0.7103350162506104 \t 12\t\n",
      "52 0.7271595597267151 \t 11\t\n",
      "53 0.7416749000549316 \t 13\t\n",
      "54 0.7273536920547485 \t 11\t\n",
      "55 0.7110562324523926 \t 11\t\n",
      "56 0.7132622599601746 \t 14\t\n",
      "57 0.7158377766609192 \t 17\t\n",
      "58 0.7214462757110596 \t 11\t\n",
      "59 0.7372337579727173 \t 11\t\n",
      "60 0.7306222319602966 \t 13\t\n",
      "61 0.7285439968109131 \t 11\t\n",
      "62 0.7082908153533936 \t 11\t\n",
      "63 0.7288287281990051 \t 11\t\n",
      "64 0.7169317007064819 \t 11\t\n",
      "65 0.7455218434333801 \t 11\t\n",
      "66 0.7217365503311157 \t 11\t\n",
      "67 0.7082273960113525 \t 20\t\n",
      "68 0.7220839858055115 \t 12\t\n",
      "69 0.7207582592964172 \t 11\t\n",
      "70 0.7231526970863342 \t 12\t\n",
      "71 0.7086288332939148 \t 11\t\n",
      "72 0.7267302870750427 \t 11\t\n",
      "73 0.7483752369880676 \t 16\t\n",
      "74 0.7147663235664368 \t 11\t\n",
      "75 0.7182238698005676 \t 11\t\n",
      "76 0.7087045907974243 \t 11\t\n",
      "77 0.7386077046394348 \t 11\t\n",
      "78 0.7225378155708313 \t 11\t\n",
      "79 0.7307295203208923 \t 11\t\n",
      "80 0.7253713011741638 \t 15\t\n",
      "81 0.7232562899589539 \t 11\t\n",
      "82 0.7343586683273315 \t 11\t\n",
      "83 0.7065547108650208 \t 11\t\n",
      "84 0.7288985252380371 \t 11\t\n",
      "85 0.7225865721702576 \t 13\t\n",
      "86 0.7081311941146851 \t 11\t\n",
      "87 0.7169759273529053 \t 12\t\n",
      "88 0.7152965068817139 \t 11\t\n",
      "89 0.7161121964454651 \t 11\t\n",
      "90 0.7299960851669312 \t 16\t\n",
      "91 0.7235885858535767 \t 11\t\n",
      "92 0.7227853536605835 \t 12\t\n",
      "93 0.7195500135421753 \t 12\t\n",
      "94 0.7306280732154846 \t 11\t\n",
      "95 0.719919741153717 \t 11\t\n",
      "96 0.7213841080665588 \t 16\t\n",
      "97 0.7081873416900635 \t 11\t\n",
      "98 0.7199434638023376 \t 11\t\n",
      "99 0.7099135518074036 \t 11\t\n",
      "\n",
      "===================================================\n",
      "1000\n",
      "0.732753623187542 \t 11\n",
      "0 0.6948107481002808 \t 11\t\n",
      "1 0.6993615031242371 \t 11\t\n",
      "2 0.6944943070411682 \t 12\t\n",
      "3 0.6946748495101929 \t 11\t\n",
      "4 0.6958831548690796 \t 11\t\n",
      "5 0.6975688338279724 \t 11\t\n",
      "6 0.6953232288360596 \t 12\t\n",
      "7 0.6968148350715637 \t 11\t\n",
      "8 0.6955082416534424 \t 11\t\n",
      "9 0.6969946622848511 \t 13\t\n",
      "10 0.695650041103363 \t 11\t\n",
      "11 0.6977493762969971 \t 14\t\n",
      "12 0.6953422427177429 \t 12\t\n",
      "13 0.6956336498260498 \t 11\t\n",
      "14 0.6971385478973389 \t 12\t\n",
      "15 0.6974973082542419 \t 11\t\n",
      "16 0.6959450244903564 \t 12\t\n",
      "17 0.699376106262207 \t 13\t\n",
      "18 0.698219895362854 \t 11\t\n",
      "19 0.6937142610549927 \t 11\t\n",
      "20 0.6967792510986328 \t 12\t\n",
      "21 0.6949476003646851 \t 11\t\n",
      "22 0.6937541365623474 \t 11\t\n",
      "23 0.6958654522895813 \t 12\t\n",
      "24 0.6964705586433411 \t 12\t\n",
      "25 0.6971873641014099 \t 12\t\n",
      "26 0.6932254433631897 \t 11\t\n",
      "27 0.6934124231338501 \t 11\t\n",
      "28 0.6953170895576477 \t 11\t\n",
      "29 0.6963600516319275 \t 11\t\n",
      "30 0.6944831013679504 \t 12\t\n",
      "31 0.6955702304840088 \t 14\t\n",
      "32 0.6956347823143005 \t 11\t\n",
      "33 0.6952999830245972 \t 12\t\n",
      "34 0.6937354803085327 \t 11\t\n",
      "35 0.6968916654586792 \t 11\t\n",
      "36 0.6941534280776978 \t 13\t\n",
      "37 0.6955576539039612 \t 12\t\n",
      "38 0.694701611995697 \t 11\t\n",
      "39 0.6948017477989197 \t 11\t\n",
      "40 0.6954548954963684 \t 11\t\n",
      "41 0.6963121294975281 \t 11\t\n",
      "42 0.6934837698936462 \t 11\t\n",
      "43 0.6976433396339417 \t 11\t\n",
      "44 0.6945923566818237 \t 11\t\n",
      "45 0.6932157278060913 \t 12\t\n",
      "46 0.6976507306098938 \t 22\t\n",
      "47 0.6937413811683655 \t 11\t\n",
      "48 0.6949679851531982 \t 11\t\n",
      "49 0.6960007548332214 \t 11\t\n",
      "50 0.6949648261070251 \t 12\t\n",
      "51 0.6974020004272461 \t 13\t\n",
      "52 0.6964055299758911 \t 13\t\n",
      "53 0.6929866075515747 \t 11\t\n",
      "54 0.6970444917678833 \t 11\t\n",
      "55 0.6955022811889648 \t 11\t\n",
      "56 0.6956332921981812 \t 11\t\n",
      "57 0.696212112903595 \t 12\t\n",
      "58 0.6966878771781921 \t 11\t\n",
      "59 0.6953113079071045 \t 11\t\n",
      "60 0.6947870850563049 \t 11\t\n",
      "61 0.6964371204376221 \t 11\t\n",
      "62 0.695368230342865 \t 11\t\n",
      "63 0.6933913230895996 \t 11\t\n",
      "64 0.6957492232322693 \t 11\t\n",
      "65 0.6947917342185974 \t 11\t\n",
      "66 0.694555401802063 \t 11\t\n",
      "67 0.6977879405021667 \t 11\t\n",
      "68 0.6953409910202026 \t 11\t\n",
      "69 0.6963444948196411 \t 12\t\n",
      "70 0.6948388814926147 \t 11\t\n",
      "71 0.6962236762046814 \t 12\t\n",
      "72 0.6947793960571289 \t 13\t\n",
      "73 0.6975152492523193 \t 11\t\n",
      "74 0.6936997175216675 \t 12\t\n",
      "75 0.6937520503997803 \t 12\t\n",
      "76 0.6939128637313843 \t 11\t\n",
      "77 0.6962245106697083 \t 11\t\n",
      "78 0.699064314365387 \t 11\t\n",
      "79 0.6967940330505371 \t 11\t\n",
      "80 0.6955403685569763 \t 11\t\n",
      "81 0.6949403285980225 \t 11\t\n",
      "82 0.6946344971656799 \t 12\t\n",
      "83 0.6954929828643799 \t 20\t\n",
      "84 0.6950691342353821 \t 11\t\n",
      "85 0.6960011124610901 \t 12\t\n",
      "86 0.694873571395874 \t 11\t\n",
      "87 0.6961547136306763 \t 12\t\n",
      "88 0.6959729194641113 \t 12\t\n",
      "89 0.694981575012207 \t 11\t\n",
      "90 0.693794846534729 \t 12\t\n",
      "91 0.6935125589370728 \t 12\t\n",
      "92 0.6950325965881348 \t 12\t\n",
      "93 0.696563720703125 \t 11\t\n",
      "94 0.6944522857666016 \t 11\t\n",
      "95 0.6957774758338928 \t 11\t\n",
      "96 0.6941837668418884 \t 11\t\n",
      "97 0.6968517899513245 \t 12\t\n",
      "98 0.6947436332702637 \t 17\t\n",
      "99 0.6957802176475525 \t 11\t\n",
      "\n",
      "===================================================\n",
      "10000\n",
      "0.6989231345713138 \t 12\n",
      "0 0.6924486756324768 \t 17\t\n",
      "1 0.6924093961715698 \t 12\t\n",
      "2 0.6924736499786377 \t 11\t\n",
      "3 0.6927904486656189 \t 12\t\n",
      "4 0.6922908425331116 \t 12\t\n",
      "5 0.6922243237495422 \t 12\t\n",
      "6 0.6924728751182556 \t 11\t\n",
      "7 0.6922740936279297 \t 12\t\n",
      "8 0.6929473876953125 \t 14\t\n",
      "9 0.6929899454116821 \t 15\t\n",
      "10 0.6929091811180115 \t 14\t\n",
      "11 0.6924600601196289 \t 13\t\n",
      "12 0.6928442120552063 \t 13\t\n",
      "13 0.6925170421600342 \t 13\t\n",
      "14 0.6922069787979126 \t 11\t\n",
      "15 0.6924782395362854 \t 14\t\n",
      "16 0.6928467154502869 \t 11\t\n",
      "17 0.6927529573440552 \t 21\t\n",
      "18 0.693459689617157 \t 16\t\n",
      "19 0.6928234100341797 \t 16\t\n",
      "20 0.692933976650238 \t 12\t\n",
      "21 0.6926903128623962 \t 12\t\n",
      "22 0.692939281463623 \t 12\t\n",
      "23 0.692642092704773 \t 12\t\n",
      "24 0.6928144097328186 \t 17\t\n",
      "25 0.6929450631141663 \t 15\t\n",
      "26 0.692770779132843 \t 14\t\n",
      "27 0.6927943229675293 \t 20\t\n",
      "28 0.6927420496940613 \t 12\t\n",
      "29 0.6925967931747437 \t 18\t\n",
      "30 0.6925374269485474 \t 13\t\n",
      "31 0.6926382780075073 \t 16\t\n",
      "32 0.6927160620689392 \t 13\t\n",
      "33 0.6925074458122253 \t 13\t\n",
      "34 0.6926673054695129 \t 12\t\n",
      "35 0.6926741003990173 \t 19\t\n",
      "36 0.6924881935119629 \t 14\t\n",
      "37 0.6928539276123047 \t 20\t\n",
      "38 0.692908763885498 \t 17\t\n",
      "39 0.6930340528488159 \t 19\t\n",
      "40 0.6930732131004333 \t 15\t\n",
      "41 0.6926918029785156 \t 14\t\n",
      "42 0.6925486326217651 \t 25\t\n",
      "43 0.692918062210083 \t 23\t\n",
      "44 0.6923527717590332 \t 15\t\n",
      "45 0.693048357963562 \t 19\t\n",
      "46 0.6929820775985718 \t 16\t\n",
      "47 0.6925270557403564 \t 21\t\n",
      "48 0.6927841305732727 \t 14\t\n",
      "49 0.6923887133598328 \t 12\t\n",
      "50 0.6926109790802002 \t 35\t\n",
      "51 0.6925920248031616 \t 29\t\n",
      "52 0.6925435066223145 \t 13\t\n",
      "53 0.6924464106559753 \t 13\t\n",
      "54 0.6930832862854004 \t 13\t\n",
      "55 0.6924843788146973 \t 17\t\n",
      "56 0.692584216594696 \t 16\t\n",
      "57 0.6925774812698364 \t 12\t\n",
      "58 0.6927854418754578 \t 11\t\n",
      "59 0.6928701400756836 \t 13\t\n",
      "60 0.6929675936698914 \t 17\t\n",
      "61 0.6930478811264038 \t 14\t\n",
      "62 0.6926272511482239 \t 13\t\n",
      "63 0.6926467418670654 \t 14\t\n",
      "64 0.6922745704650879 \t 12\t\n",
      "65 0.6924946308135986 \t 12\t\n",
      "66 0.6926783323287964 \t 13\t\n",
      "67 0.6926534175872803 \t 12\t\n",
      "68 0.6925182342529297 \t 11\t\n",
      "69 0.6925989985466003 \t 12\t\n",
      "70 0.6924314498901367 \t 16\t\n",
      "71 0.6929001212120056 \t 12\t\n",
      "72 0.6930418014526367 \t 21\t\n",
      "73 0.6927662491798401 \t 20\t\n",
      "74 0.6921901106834412 \t 13\t\n",
      "75 0.6932202577590942 \t 16\t\n",
      "76 0.6925715804100037 \t 18\t\n",
      "77 0.6922255754470825 \t 12\t\n",
      "78 0.6924461722373962 \t 12\t\n",
      "79 0.6926564574241638 \t 11\t\n",
      "80 0.6927596926689148 \t 22\t\n",
      "81 0.6928081512451172 \t 12\t\n",
      "82 0.6923460960388184 \t 11\t\n",
      "83 0.6925914287567139 \t 12\t\n",
      "84 0.6927000284194946 \t 12\t\n",
      "85 0.6926702857017517 \t 11\t\n",
      "86 0.6928176879882812 \t 16\t\n",
      "87 0.6923617720603943 \t 13\t\n",
      "88 0.6928620338439941 \t 15\t\n",
      "89 0.6926019191741943 \t 12\t\n",
      "90 0.6925444602966309 \t 20\t\n",
      "91 0.6927139759063721 \t 14\t\n",
      "92 0.6923533082008362 \t 13\t\n",
      "93 0.6928432583808899 \t 12\t\n",
      "94 0.6930103302001953 \t 11\t\n",
      "95 0.6925451159477234 \t 12\t\n",
      "96 0.6925098896026611 \t 29\t\n",
      "97 0.6922690272331238 \t 19\t\n",
      "98 0.6928988099098206 \t 16\t\n",
      "99 0.6922965049743652 \t 12\t\n",
      "\n",
      "===================================================\n",
      "100000\n",
      "0.6923379176658392 \t 13\n",
      "0 0.6912754774093628 \t 53\t\n",
      "1 0.6913517117500305 \t 33\t\n",
      "2 0.6912661194801331 \t 29\t\n",
      "3 0.6912923455238342 \t 26\t\n",
      "4 0.6912810206413269 \t 23\t\n",
      "5 0.6912311911582947 \t 33\t\n",
      "6 0.6913929581642151 \t 26\t\n",
      "7 0.6912224292755127 \t 36\t\n",
      "8 0.6912862658500671 \t 44\t\n",
      "9 0.6913167834281921 \t 33\t\n",
      "10 0.6912825703620911 \t 23\t\n",
      "11 0.6912262439727783 \t 39\t\n",
      "12 0.6912808418273926 \t 29\t\n",
      "13 0.6913111805915833 \t 26\t\n",
      "14 0.6913869380950928 \t 32\t\n",
      "15 0.6913579106330872 \t 54\t\n",
      "16 0.6914093494415283 \t 34\t\n",
      "17 0.6912820935249329 \t 27\t\n",
      "18 0.6913228034973145 \t 25\t\n",
      "19 0.6913690567016602 \t 27\t\n",
      "20 0.6912108659744263 \t 37\t\n",
      "21 0.6912049055099487 \t 52\t\n",
      "22 0.6912842988967896 \t 30\t\n",
      "23 0.6914896965026855 \t 23\t\n",
      "24 0.6912662386894226 \t 42\t\n",
      "25 0.6913315057754517 \t 26\t\n",
      "26 0.6912417411804199 \t 43\t\n",
      "27 0.6912453174591064 \t 61\t\n",
      "28 0.691326379776001 \t 26\t\n",
      "29 0.6914120316505432 \t 38\t\n",
      "30 0.6913225650787354 \t 29\t\n",
      "31 0.6912387013435364 \t 38\t\n",
      "32 0.6911899447441101 \t 35\t\n",
      "33 0.6912885308265686 \t 43\t\n",
      "34 0.6912946701049805 \t 30\t\n",
      "35 0.6912398934364319 \t 36\t\n",
      "36 0.6912959218025208 \t 31\t\n",
      "37 0.6912549734115601 \t 23\t\n",
      "38 0.6911897659301758 \t 40\t\n",
      "39 0.6913447380065918 \t 32\t\n",
      "40 0.6914200186729431 \t 37\t\n",
      "41 0.6912378668785095 \t 34\t\n",
      "42 0.691296398639679 \t 24\t\n",
      "43 0.6912739276885986 \t 46\t\n",
      "44 0.6912807822227478 \t 44\t\n",
      "45 0.6913665533065796 \t 39\t\n",
      "46 0.6912217140197754 \t 69\t\n",
      "47 0.6913431286811829 \t 40\t\n",
      "48 0.6913269758224487 \t 23\t\n",
      "49 0.6912548542022705 \t 38\t\n",
      "50 0.6912632584571838 \t 31\t\n",
      "51 0.6913728713989258 \t 26\t\n",
      "52 0.6912668943405151 \t 33\t\n",
      "53 0.6912440061569214 \t 34\t\n",
      "54 0.691318690776825 \t 29\t\n",
      "55 0.6912860870361328 \t 28\t\n",
      "56 0.6912294030189514 \t 34\t\n",
      "57 0.6914351582527161 \t 24\t\n",
      "58 0.6912513971328735 \t 36\t\n",
      "59 0.6913132667541504 \t 55\t\n",
      "60 0.6913816928863525 \t 33\t\n",
      "61 0.6912429928779602 \t 34\t\n",
      "62 0.6912510991096497 \t 31\t\n",
      "63 0.6913198232650757 \t 39\t\n",
      "64 0.6913785934448242 \t 24\t\n",
      "65 0.6913554668426514 \t 30\t\n",
      "66 0.6913228631019592 \t 50\t\n",
      "67 0.6912134885787964 \t 27\t\n",
      "68 0.6912742853164673 \t 31\t\n",
      "69 0.6912452578544617 \t 24\t\n",
      "70 0.6912482976913452 \t 29\t\n",
      "71 0.6912686824798584 \t 26\t\n",
      "72 0.6913408637046814 \t 24\t\n",
      "73 0.6911082863807678 \t 27\t\n",
      "74 0.6913893818855286 \t 23\t\n",
      "75 0.6912423968315125 \t 49\t\n",
      "76 0.6912277340888977 \t 23\t\n",
      "77 0.6912469267845154 \t 26\t\n",
      "78 0.6912369728088379 \t 47\t\n",
      "79 0.6912919282913208 \t 28\t\n",
      "80 0.691268265247345 \t 28\t\n",
      "81 0.6913198232650757 \t 29\t\n",
      "82 0.6912564039230347 \t 37\t\n",
      "83 0.6912558674812317 \t 32\t\n",
      "84 0.6914176344871521 \t 35\t\n",
      "85 0.6913585662841797 \t 26\t\n",
      "86 0.6912627220153809 \t 39\t\n",
      "87 0.691285252571106 \t 22\t\n",
      "88 0.6912785172462463 \t 29\t\n",
      "89 0.6912277936935425 \t 23\t\n",
      "90 0.6912897825241089 \t 29\t\n",
      "91 0.6912347078323364 \t 23\t\n",
      "92 0.6911301016807556 \t 28\t\n",
      "93 0.6913565397262573 \t 36\t\n",
      "94 0.6913137435913086 \t 22\t\n",
      "95 0.6913283467292786 \t 31\t\n",
      "96 0.6913344264030457 \t 25\t\n",
      "97 0.6912931203842163 \t 37\t\n",
      "98 0.6912704706192017 \t 25\t\n",
      "99 0.6913352608680725 \t 33\t\n",
      "\n",
      "===================================================\n",
      "1000000\n",
      "0.6910087631967515 \t 27\n",
      "0 0.6914517879486084 \t 59\t\n",
      "1 0.691374659538269 \t 100\t\n",
      "2 0.6913537383079529 \t 100\t\n",
      "3 0.691339373588562 \t 100\t\n",
      "4 0.6915028095245361 \t 35\t\n",
      "5 0.6914808750152588 \t 71\t\n",
      "6 0.691480278968811 \t 55\t\n",
      "7 0.6913572549819946 \t 100\t\n",
      "8 0.6914363503456116 \t 79\t\n",
      "9 0.6914594769477844 \t 48\t\n",
      "10 0.6913997530937195 \t 100\t\n",
      "11 0.6914915442466736 \t 44\t\n",
      "12 0.6913720369338989 \t 100\t\n",
      "13 0.6914977431297302 \t 43\t\n",
      "14 0.6914551258087158 \t 58\t\n",
      "15 0.6914624571800232 \t 63\t\n",
      "16 0.6913964152336121 \t 90\t\n",
      "17 0.6914417147636414 \t 82\t\n",
      "18 0.6914456486701965 \t 70\t\n",
      "19 0.6913131475448608 \t 100\t\n",
      "20 0.6914743781089783 \t 51\t\n",
      "21 0.6914488673210144 \t 61\t\n",
      "22 0.6913557648658752 \t 100\t\n",
      "23 0.6914495229721069 \t 53\t\n",
      "24 0.6914830207824707 \t 51\t\n",
      "25 0.6914791464805603 \t 28\t\n",
      "26 0.6914843320846558 \t 44\t\n",
      "27 0.6914694905281067 \t 49\t\n",
      "28 0.6913907527923584 \t 100\t\n",
      "29 0.6914861798286438 \t 52\t\n",
      "30 0.6914698481559753 \t 81\t\n",
      "31 0.6914659738540649 \t 46\t\n",
      "32 0.6913893818855286 \t 100\t\n",
      "33 0.6914430856704712 \t 64\t\n",
      "34 0.6914613246917725 \t 56\t\n",
      "35 0.6914433836936951 \t 100\t\n",
      "36 0.69145667552948 \t 63\t\n",
      "37 0.6914190053939819 \t 100\t\n",
      "38 0.6914835572242737 \t 52\t\n",
      "39 0.6913391947746277 \t 100\t\n",
      "40 0.6914588212966919 \t 53\t\n",
      "41 0.6913893818855286 \t 100\t\n",
      "42 0.6913878917694092 \t 86\t\n",
      "43 0.6914117336273193 \t 85\t\n",
      "44 0.6914348006248474 \t 82\t\n",
      "45 0.6914949417114258 \t 39\t\n",
      "46 0.6914013624191284 \t 100\t\n",
      "47 0.6913291215896606 \t 100\t\n",
      "48 0.6914423704147339 \t 65\t\n",
      "49 0.6914242506027222 \t 91\t\n",
      "50 0.6914440393447876 \t 57\t\n",
      "51 0.6913997530937195 \t 96\t\n",
      "52 0.6914263963699341 \t 70\t\n",
      "53 0.6914110779762268 \t 76\t\n",
      "54 0.6914202570915222 \t 81\t\n",
      "55 0.6914551258087158 \t 66\t\n",
      "56 0.6914637684822083 \t 46\t\n",
      "57 0.6914544105529785 \t 59\t\n",
      "58 0.6913202404975891 \t 100\t\n",
      "59 0.6914671063423157 \t 59\t\n",
      "60 0.6913393139839172 \t 100\t\n",
      "61 0.6914781928062439 \t 85\t\n",
      "62 0.6914576292037964 \t 44\t\n",
      "63 0.6914551258087158 \t 47\t\n",
      "64 0.6914221048355103 \t 71\t\n",
      "65 0.6913722157478333 \t 100\t\n",
      "66 0.6914160251617432 \t 88\t\n",
      "67 0.6914593577384949 \t 66\t\n",
      "68 0.6914392113685608 \t 66\t\n",
      "69 0.6913735270500183 \t 100\t\n",
      "70 0.6914993524551392 \t 44\t\n",
      "71 0.6914533972740173 \t 75\t\n",
      "72 0.6913595795631409 \t 100\t\n",
      "73 0.6914618611335754 \t 51\t\n",
      "74 0.691403329372406 \t 89\t\n",
      "75 0.6915140748023987 \t 40\t\n",
      "76 0.6914421916007996 \t 100\t\n",
      "77 0.6914421916007996 \t 50\t\n",
      "78 0.69134122133255 \t 100\t\n",
      "79 0.6914525032043457 \t 68\t\n",
      "80 0.6914346218109131 \t 100\t\n",
      "81 0.6914191246032715 \t 82\t\n",
      "82 0.6914622783660889 \t 60\t\n",
      "83 0.6914154887199402 \t 66\t\n",
      "84 0.6914523243904114 \t 61\t\n",
      "85 0.6914631724357605 \t 44\t\n",
      "86 0.6914633512496948 \t 46\t\n",
      "87 0.6914474368095398 \t 59\t\n",
      "88 0.6913397312164307 \t 100\t\n",
      "89 0.6914553642272949 \t 61\t\n",
      "90 0.691474974155426 \t 44\t\n",
      "91 0.6914086937904358 \t 80\t\n",
      "92 0.6913962960243225 \t 100\t\n",
      "93 0.6913930177688599 \t 100\t\n",
      "94 0.6913204789161682 \t 100\t\n",
      "95 0.6914476156234741 \t 63\t\n",
      "96 0.6914207339286804 \t 75\t\n",
      "97 0.691446840763092 \t 69\t\n",
      "98 0.6915112137794495 \t 28\t\n",
      "99 0.6914616227149963 \t 42\t\n",
      "\n",
      "===================================================\n",
      "10000000\n",
      "0.6907671455825418 \t 44\n",
      "0 0.6908367872238159 \t 100\t\n",
      "1 0.6909074187278748 \t 100\t\n",
      "2 0.6908218264579773 \t 100\t\n",
      "3 0.6909608244895935 \t 100\t\n",
      "4 0.6908385753631592 \t 100\t\n",
      "5 0.690834105014801 \t 100\t\n",
      "6 0.6909276843070984 \t 100\t\n",
      "7 0.6908357739448547 \t 100\t\n",
      "8 0.690941572189331 \t 100\t\n",
      "9 0.690987229347229 \t 100\t\n",
      "10 0.6908235549926758 \t 100\t\n",
      "11 0.6910064220428467 \t 100\t\n",
      "12 0.6908401846885681 \t 100\t\n",
      "13 0.6908483505249023 \t 100\t\n",
      "14 0.690890908241272 \t 100\t\n",
      "15 0.6908673644065857 \t 100\t\n",
      "16 0.6908314228057861 \t 100\t\n",
      "17 0.6908740401268005 \t 100\t\n",
      "18 0.6908825039863586 \t 100\t\n",
      "19 0.6909831166267395 \t 100\t\n",
      "20 0.6911001801490784 \t 100\t\n",
      "21 0.6908407211303711 \t 100\t\n",
      "22 0.6909663081169128 \t 100\t\n",
      "23 0.6909382939338684 \t 100\t\n",
      "24 0.6908720135688782 \t 100\t\n",
      "25 0.6910626888275146 \t 100\t\n",
      "26 0.690776526927948 \t 100\t\n",
      "27 0.690853476524353 \t 100\t\n",
      "28 0.6908643245697021 \t 100\t\n",
      "29 0.690881073474884 \t 100\t\n",
      "30 0.6907824873924255 \t 100\t\n",
      "31 0.6908721923828125 \t 100\t\n",
      "32 0.6908005475997925 \t 100\t\n",
      "33 0.6909250020980835 \t 100\t\n",
      "34 0.6909023523330688 \t 100\t\n",
      "35 0.6910008192062378 \t 100\t\n",
      "36 0.6908489465713501 \t 100\t\n",
      "37 0.6908792853355408 \t 100\t\n",
      "38 0.690888524055481 \t 100\t\n",
      "39 0.690946102142334 \t 100\t\n",
      "40 0.6908835172653198 \t 100\t\n",
      "41 0.6908644437789917 \t 100\t\n",
      "42 0.6909077167510986 \t 100\t\n",
      "43 0.6908261179924011 \t 100\t\n",
      "44 0.6909704804420471 \t 100\t\n",
      "45 0.6910956501960754 \t 100\t\n",
      "46 0.6908143162727356 \t 100\t\n",
      "47 0.6908438205718994 \t 100\t\n",
      "48 0.6910609602928162 \t 100\t\n",
      "49 0.6908723711967468 \t 100\t\n",
      "50 0.6908276677131653 \t 100\t\n",
      "51 0.6911851167678833 \t 100\t\n",
      "52 0.6909649968147278 \t 100\t\n",
      "53 0.6908131837844849 \t 100\t\n",
      "54 0.690808892250061 \t 100\t\n",
      "55 0.6908090114593506 \t 100\t\n",
      "56 0.6907991766929626 \t 100\t\n",
      "57 0.6908833980560303 \t 100\t\n",
      "58 0.6908484101295471 \t 100\t\n",
      "59 0.6908249258995056 \t 100\t\n",
      "60 0.6909117102622986 \t 100\t\n",
      "61 0.6908115148544312 \t 100\t\n",
      "62 0.691189706325531 \t 100\t\n",
      "63 0.6908919811248779 \t 100\t\n",
      "64 0.6909708380699158 \t 100\t\n",
      "65 0.6908828020095825 \t 100\t\n",
      "66 0.6909191012382507 \t 100\t\n",
      "67 0.6908730268478394 \t 100\t\n",
      "68 0.6908149123191833 \t 100\t\n",
      "69 0.6912842988967896 \t 100\t\n",
      "70 0.6908084750175476 \t 100\t\n",
      "71 0.6907699704170227 \t 100\t\n",
      "72 0.6908652782440186 \t 100\t\n",
      "73 0.6908326745033264 \t 100\t\n",
      "74 0.6908295750617981 \t 100\t\n",
      "75 0.6909744739532471 \t 100\t\n",
      "76 0.6908137202262878 \t 100\t\n",
      "77 0.6907969117164612 \t 100\t\n",
      "78 0.6913272738456726 \t 100\t\n",
      "79 0.6908600926399231 \t 100\t\n",
      "80 0.6908813118934631 \t 100\t\n",
      "81 0.690834105014801 \t 100\t\n",
      "82 0.690873920917511 \t 100\t\n",
      "83 0.6908621191978455 \t 100\t\n",
      "84 0.6909527778625488 \t 100\t\n",
      "85 0.6908001899719238 \t 100\t\n",
      "86 0.6909260749816895 \t 100\t\n",
      "87 0.6908146739006042 \t 100\t\n",
      "88 0.6908388137817383 \t 100\t\n",
      "89 0.6909295320510864 \t 100\t\n",
      "90 0.6908484697341919 \t 100\t\n",
      "91 0.6908791065216064 \t 100\t\n",
      "92 0.6907953023910522 \t 100\t\n",
      "93 0.6909123063087463 \t 100\t\n",
      "94 0.6908243894577026 \t 100\t\n",
      "95 0.6909257769584656 \t 100\t\n",
      "96 0.6908531785011292 \t 100\t\n",
      "97 0.6907684803009033 \t 100\t\n",
      "98 0.6910619139671326 \t 100\t\n",
      "99 0.6907920837402344 \t 100\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for N in Ns:\n",
    "    print('===================================================\\n{}'.format(N))\n",
    "    # Take the first N samples.\n",
    "    data, m, s = split_data(X[:N], y[:N])\n",
    "    \n",
    "    # Train BDT model (only need to train 1)\n",
    "    bdt_model = XGBClassifier(early_stopping_rounds = 10)\n",
    "    X_trn, X_vld, y_trn, y_vld = data\n",
    "    bdt_model.fit(X_trn, y_trn, eval_set = [(X_vld, y_vld)], verbose = 0)\n",
    "    trace = bdt_model.evals_result()['validation_0']\n",
    "    print(trace['logloss'][-1], '\\t', len(trace['logloss']), end = '\\n')\n",
    "    bdt_model.save_model(bdt_filestr.format(N))\n",
    "\n",
    "    for i in range(reps):\n",
    "        print(i, end = ' ')\n",
    "        # Train BCE model\n",
    "        bce_model, trace = train(data, **bce_params)\n",
    "        bce_model.save_weights(bce_filestr.format(N, i))\n",
    "        \n",
    "        # Train GBC model\n",
    "        gbc_model = GradientBoostingClassifier(validation_fraction = 0.25,\n",
    "                                               n_iter_no_change = 10)\n",
    "        gbc_model.fit(X[:N], y[:N])\n",
    "        dump(gbc_model, gbc_filestr.format(N, i))\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c198e6-0986-41aa-a9a7-d28f318d178d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## $d = 4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c9764d4-4e06-4426-b662-c1551a4c8747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "num = 0\n",
    "reps = 100\n",
    "d = 4\n",
    "Ns = 10**np.arange(2, 8)\n",
    "\n",
    "# Model parameters\n",
    "bce_params = {'loss':bce, 'd': d}\n",
    "\n",
    "filestr = 'models/zenodo/{}/set_{}/'.format(d, num)\n",
    "bce_filestr = filestr + 'bce/model_{}_{}.h5'\n",
    "bdt_filestr = filestr + 'bdt/model_{}.h5'\n",
    "gbc_filestr = filestr + 'gbc/model_{}_{}.h5'\n",
    "\n",
    "if not os.path.isdir(filestr):\n",
    "    os.mkdir(filestr)\n",
    "\n",
    "if not os.path.isdir(filestr + 'bce/'):\n",
    "    os.mkdir(filestr + 'bce/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'bdt/'):\n",
    "    os.mkdir(filestr + 'bdt/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'gbc/'):\n",
    "    os.mkdir(filestr + 'gbc/')\n",
    "\n",
    "# Data parameters\n",
    "X = np.load('data/zenodo/{}/X_trn.npy'.format(d))\n",
    "y = np.load('data/zenodo/{}/y_trn.npy'.format(d)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15382cf7-0ebb-41ab-bc6f-037c5c058056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================\n",
      "100\n",
      "0.7837334740161895 \t 13\n",
      "0 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-18 16:07:34.260425: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-18 16:07:34.868212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21667 MB memory:  -> device: 0, name: Quadro RTX 6000, pci bus id: 0000:c1:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8204054236412048 \t 11\t\n",
      "1 0.7890867590904236 \t 11\t\n",
      "2 0.8298936486244202 \t 11\t\n",
      "3 0.8266319036483765 \t 11\t\n",
      "4 0.8053611516952515 \t 11\t\n",
      "5 0.8082427978515625 \t 11\t\n",
      "6 0.8236465454101562 \t 11\t\n",
      "7 0.804288387298584 \t 11\t\n",
      "8 0.8184380531311035 \t 11\t\n",
      "9 0.7777573466300964 \t 11\t\n",
      "10 0.8180947303771973 \t 14\t\n",
      "11 0.8111706376075745 \t 13\t\n",
      "12 0.8100696802139282 \t 11\t\n",
      "13 0.8593564033508301 \t 11\t\n",
      "14 0.7928666472434998 \t 12\t\n",
      "15 0.8123726844787598 \t 11\t\n",
      "16 0.8099817037582397 \t 11\t\n",
      "17 0.7708479166030884 \t 11\t\n",
      "18 0.8036649823188782 \t 11\t\n",
      "19 0.7887219786643982 \t 11\t\n",
      "20 0.8273570537567139 \t 11\t\n",
      "21 0.84206223487854 \t 11\t\n",
      "22 0.8296345472335815 \t 11\t\n",
      "23 0.7844417095184326 \t 11\t\n",
      "24 0.796212911605835 \t 11\t\n",
      "25 0.7815646529197693 \t 11\t\n",
      "26 0.799448549747467 \t 11\t\n",
      "27 0.7801907062530518 \t 11\t\n",
      "28 0.7804624438285828 \t 11\t\n",
      "29 0.7819011211395264 \t 11\t\n",
      "30 0.7736698389053345 \t 11\t\n",
      "31 0.7897666096687317 \t 11\t\n",
      "32 0.8057467937469482 \t 11\t\n",
      "33 0.8083918690681458 \t 11\t\n",
      "34 0.8441940546035767 \t 13\t\n",
      "35 0.785098671913147 \t 11\t\n",
      "36 0.8052678108215332 \t 11\t\n",
      "37 0.8053714036941528 \t 11\t\n",
      "38 0.7798768877983093 \t 11\t\n",
      "39 0.790418267250061 \t 11\t\n",
      "40 0.7772995233535767 \t 11\t\n",
      "41 0.809476912021637 \t 11\t\n",
      "42 0.8333086371421814 \t 11\t\n",
      "43 0.8302426338195801 \t 11\t\n",
      "44 0.7949475049972534 \t 11\t\n",
      "45 0.7967641949653625 \t 12\t\n",
      "46 0.8262503147125244 \t 11\t\n",
      "47 0.793521523475647 \t 11\t\n",
      "48 0.8009354472160339 \t 11\t\n",
      "49 0.784511387348175 \t 11\t\n",
      "50 0.7855613827705383 \t 11\t\n",
      "51 0.8108348250389099 \t 11\t\n",
      "52 0.7450740337371826 \t 11\t\n",
      "53 0.8181349039077759 \t 11\t\n",
      "54 0.8169206976890564 \t 11\t\n",
      "55 0.7906982898712158 \t 11\t\n",
      "56 0.795329749584198 \t 11\t\n",
      "57 0.7505297660827637 \t 11\t\n",
      "58 0.7916496396064758 \t 11\t\n",
      "59 0.7727056741714478 \t 11\t\n",
      "60 0.859657883644104 \t 11\t\n",
      "61 0.8450374603271484 \t 12\t\n",
      "62 0.7798874378204346 \t 11\t\n",
      "63 0.7935946583747864 \t 11\t\n",
      "64 0.794853687286377 \t 11\t\n",
      "65 0.7914589047431946 \t 11\t\n",
      "66 0.8013489246368408 \t 11\t\n",
      "67 0.7974743843078613 \t 11\t\n",
      "68 0.7890946865081787 \t 11\t\n",
      "69 0.8263256549835205 \t 11\t\n",
      "70 0.8155501484870911 \t 11\t\n",
      "71 0.8134240508079529 \t 11\t\n",
      "72 0.8141863942146301 \t 11\t\n",
      "73 0.7775053381919861 \t 11\t\n",
      "74 0.829776406288147 \t 11\t\n",
      "75 0.7924681901931763 \t 11\t\n",
      "76 0.802060067653656 \t 11\t\n",
      "77 0.7592391967773438 \t 11\t\n",
      "78 0.7952890992164612 \t 11\t\n",
      "79 0.7928294539451599 \t 11\t\n",
      "80 0.7800913453102112 \t 11\t\n",
      "81 0.8218032121658325 \t 11\t\n",
      "82 0.8076631426811218 \t 11\t\n",
      "83 0.7679484486579895 \t 12\t\n",
      "84 0.7727135419845581 \t 11\t\n",
      "85 0.7813810110092163 \t 11\t\n",
      "86 0.7846176624298096 \t 11\t\n",
      "87 0.8485292196273804 \t 11\t\n",
      "88 0.8218444585800171 \t 11\t\n",
      "89 0.797107458114624 \t 11\t\n",
      "90 0.8200079202651978 \t 11\t\n",
      "91 0.8128251433372498 \t 11\t\n",
      "92 0.8292012810707092 \t 11\t\n",
      "93 0.7926942706108093 \t 11\t\n",
      "94 0.8011361956596375 \t 11\t\n",
      "95 0.7852655053138733 \t 11\t\n",
      "96 0.7803458571434021 \t 11\t\n",
      "97 0.8181306719779968 \t 11\t\n",
      "98 0.8661310076713562 \t 13\t\n",
      "99 0.8003262281417847 \t 11\t\n",
      "\n",
      "===================================================\n",
      "1000\n",
      "0.7252078629732132 \t 11\n",
      "0 0.6936125755310059 \t 12\t\n",
      "1 0.6936299800872803 \t 12\t\n",
      "2 0.6949576139450073 \t 13\t\n",
      "3 0.697951078414917 \t 13\t\n",
      "4 0.6949973702430725 \t 18\t\n",
      "5 0.6930015683174133 \t 14\t\n",
      "6 0.6953288316726685 \t 13\t\n",
      "7 0.6926441192626953 \t 13\t\n",
      "8 0.69706791639328 \t 19\t\n",
      "9 0.6949500441551208 \t 16\t\n",
      "10 0.6945840716362 \t 14\t\n",
      "11 0.6926944255828857 \t 15\t\n",
      "12 0.6929241418838501 \t 11\t\n",
      "13 0.7019059062004089 \t 17\t\n",
      "14 0.6922672986984253 \t 12\t\n",
      "15 0.6962660551071167 \t 12\t\n",
      "16 0.6987137198448181 \t 12\t\n",
      "17 0.6934656500816345 \t 13\t\n",
      "18 0.6946630477905273 \t 12\t\n",
      "19 0.7042530179023743 \t 16\t\n",
      "20 0.6935827732086182 \t 15\t\n",
      "21 0.6921363472938538 \t 12\t\n",
      "22 0.6992020010948181 \t 15\t\n",
      "23 0.6974561214447021 \t 15\t\n",
      "24 0.6928786635398865 \t 16\t\n",
      "25 0.6995429396629333 \t 17\t\n",
      "26 0.6936965584754944 \t 11\t\n",
      "27 0.6987013220787048 \t 13\t\n",
      "28 0.6969060897827148 \t 11\t\n",
      "29 0.6976659893989563 \t 12\t\n",
      "30 0.70069819688797 \t 17\t\n",
      "31 0.6938474774360657 \t 16\t\n",
      "32 0.6891395449638367 \t 12\t\n",
      "33 0.69581538438797 \t 16\t\n",
      "34 0.6977509260177612 \t 13\t\n",
      "35 0.6938485503196716 \t 13\t\n",
      "36 0.6996407508850098 \t 14\t\n",
      "37 0.6957701444625854 \t 12\t\n",
      "38 0.6975582242012024 \t 18\t\n",
      "39 0.704237699508667 \t 13\t\n",
      "40 0.6946613788604736 \t 16\t\n",
      "41 0.6958508491516113 \t 16\t\n",
      "42 0.6972367167472839 \t 16\t\n",
      "43 0.6950939297676086 \t 12\t\n",
      "44 0.6937232613563538 \t 12\t\n",
      "45 0.6954637765884399 \t 16\t\n",
      "46 0.6935266852378845 \t 15\t\n",
      "47 0.6967982053756714 \t 15\t\n",
      "48 0.7009583115577698 \t 14\t\n",
      "49 0.690031111240387 \t 13\t\n",
      "50 0.6985979676246643 \t 14\t\n",
      "51 0.6907979846000671 \t 13\t\n",
      "52 0.6939205527305603 \t 16\t\n",
      "53 0.7036994099617004 \t 17\t\n",
      "54 0.6903968453407288 \t 12\t\n",
      "55 0.6917757987976074 \t 14\t\n",
      "56 0.7032597064971924 \t 16\t\n",
      "57 0.6899185180664062 \t 12\t\n",
      "58 0.7008880972862244 \t 16\t\n",
      "59 0.6948696374893188 \t 11\t\n",
      "60 0.6952492594718933 \t 14\t\n",
      "61 0.6970105767250061 \t 17\t\n",
      "62 0.6942837238311768 \t 13\t\n",
      "63 0.691117525100708 \t 14\t\n",
      "64 0.6971195936203003 \t 12\t\n",
      "65 0.6933749914169312 \t 15\t\n",
      "66 0.6996002197265625 \t 17\t\n",
      "67 0.6975210309028625 \t 12\t\n",
      "68 0.6918497085571289 \t 14\t\n",
      "69 0.699000358581543 \t 14\t\n",
      "70 0.6961633563041687 \t 14\t\n",
      "71 0.695619523525238 \t 13\t\n",
      "72 0.6963954567909241 \t 14\t\n",
      "73 0.699366569519043 \t 11\t\n",
      "74 0.696990966796875 \t 16\t\n",
      "75 0.6929174065589905 \t 16\t\n",
      "76 0.6967819333076477 \t 16\t\n",
      "77 0.6918360590934753 \t 12\t\n",
      "78 0.6993442177772522 \t 18\t\n",
      "79 0.692343533039093 \t 14\t\n",
      "80 0.696673572063446 \t 11\t\n",
      "81 0.691750168800354 \t 12\t\n",
      "82 0.6976472735404968 \t 11\t\n",
      "83 0.7000327110290527 \t 15\t\n",
      "84 0.6986604928970337 \t 16\t\n",
      "85 0.6923120021820068 \t 13\t\n",
      "86 0.6955634951591492 \t 11\t\n",
      "87 0.6993205547332764 \t 14\t\n",
      "88 0.6952512264251709 \t 16\t\n",
      "89 0.69500732421875 \t 12\t\n",
      "90 0.6959624290466309 \t 13\t\n",
      "91 0.6958038806915283 \t 12\t\n",
      "92 0.6987820863723755 \t 13\t\n",
      "93 0.6952090263366699 \t 13\t\n",
      "94 0.6922728419303894 \t 11\t\n",
      "95 0.7004172205924988 \t 18\t\n",
      "96 0.6971783638000488 \t 11\t\n",
      "97 0.6975451111793518 \t 20\t\n",
      "98 0.6957589983940125 \t 19\t\n",
      "99 0.6960256099700928 \t 16\t\n",
      "\n",
      "===================================================\n",
      "10000\n",
      "0.6906229829221964 \t 12\n",
      "0 0.6877633333206177 \t 14\t\n",
      "1 0.688042938709259 \t 17\t\n",
      "2 0.6881917119026184 \t 17\t\n",
      "3 0.6878841519355774 \t 19\t\n",
      "4 0.6881489157676697 \t 16\t\n",
      "5 0.6875877380371094 \t 21\t\n",
      "6 0.687435507774353 \t 16\t\n",
      "7 0.6875403523445129 \t 16\t\n",
      "8 0.6881069540977478 \t 19\t\n",
      "9 0.6873060464859009 \t 14\t\n",
      "10 0.687564492225647 \t 16\t\n",
      "11 0.687198281288147 \t 14\t\n",
      "12 0.6878767013549805 \t 15\t\n",
      "13 0.686945378780365 \t 16\t\n",
      "14 0.6871371269226074 \t 21\t\n",
      "15 0.6875664591789246 \t 16\t\n",
      "16 0.6872410178184509 \t 16\t\n",
      "17 0.6875327229499817 \t 16\t\n",
      "18 0.6877092719078064 \t 17\t\n",
      "19 0.68790602684021 \t 16\t\n",
      "20 0.6884491443634033 \t 17\t\n",
      "21 0.6873450875282288 \t 18\t\n",
      "22 0.6872638463973999 \t 16\t\n",
      "23 0.6876946091651917 \t 21\t\n",
      "24 0.688246488571167 \t 16\t\n",
      "25 0.6873134970664978 \t 15\t\n",
      "26 0.687904417514801 \t 14\t\n",
      "27 0.6877172589302063 \t 15\t\n",
      "28 0.687606930732727 \t 17\t\n",
      "29 0.6880176663398743 \t 20\t\n",
      "30 0.6875030398368835 \t 14\t\n",
      "31 0.6878475546836853 \t 17\t\n",
      "32 0.6877772212028503 \t 19\t\n",
      "33 0.6876024603843689 \t 22\t\n",
      "34 0.687307596206665 \t 16\t\n",
      "35 0.6874256134033203 \t 17\t\n",
      "36 0.6874215006828308 \t 17\t\n",
      "37 0.6880313754081726 \t 18\t\n",
      "38 0.6868169903755188 \t 16\t\n",
      "39 0.6872206330299377 \t 18\t\n",
      "40 0.6880189180374146 \t 20\t\n",
      "41 0.6880242824554443 \t 15\t\n",
      "42 0.6867265701293945 \t 16\t\n",
      "43 0.6875550746917725 \t 14\t\n",
      "44 0.6874852776527405 \t 15\t\n",
      "45 0.6878872513771057 \t 19\t\n",
      "46 0.6872943639755249 \t 17\t\n",
      "47 0.6881622672080994 \t 16\t\n",
      "48 0.6870254874229431 \t 16\t\n",
      "49 0.6869862079620361 \t 16\t\n",
      "50 0.6874688863754272 \t 15\t\n",
      "51 0.6874242424964905 \t 16\t\n",
      "52 0.6872918009757996 \t 19\t\n",
      "53 0.6872017979621887 \t 15\t\n",
      "54 0.6871196031570435 \t 15\t\n",
      "55 0.6880770921707153 \t 15\t\n",
      "56 0.6873559951782227 \t 15\t\n",
      "57 0.6875868439674377 \t 15\t\n",
      "58 0.6873089671134949 \t 13\t\n",
      "59 0.6875566244125366 \t 15\t\n",
      "60 0.6874427199363708 \t 17\t\n",
      "61 0.6868972778320312 \t 16\t\n",
      "62 0.6876674890518188 \t 15\t\n",
      "63 0.6878382563591003 \t 20\t\n",
      "64 0.6871509552001953 \t 14\t\n",
      "65 0.6875631213188171 \t 17\t\n",
      "66 0.6879384517669678 \t 16\t\n",
      "67 0.6878066658973694 \t 15\t\n",
      "68 0.6879878044128418 \t 16\t\n",
      "69 0.6875663995742798 \t 16\t\n",
      "70 0.6879789233207703 \t 19\t\n",
      "71 0.6876246333122253 \t 12\t\n",
      "72 0.6871881484985352 \t 15\t\n",
      "73 0.6874719858169556 \t 15\t\n",
      "74 0.6877831220626831 \t 18\t\n",
      "75 0.6875131726264954 \t 16\t\n",
      "76 0.6876333951950073 \t 17\t\n",
      "77 0.6872532963752747 \t 17\t\n",
      "78 0.6875070333480835 \t 16\t\n",
      "79 0.6868135333061218 \t 15\t\n",
      "80 0.6884959936141968 \t 15\t\n",
      "81 0.6875432729721069 \t 18\t\n",
      "82 0.687721848487854 \t 16\t\n",
      "83 0.6877135634422302 \t 13\t\n",
      "84 0.6877977848052979 \t 18\t\n",
      "85 0.6872053742408752 \t 17\t\n",
      "86 0.6870799660682678 \t 15\t\n",
      "87 0.687343418598175 \t 17\t\n",
      "88 0.6881048083305359 \t 16\t\n",
      "89 0.6871880292892456 \t 14\t\n",
      "90 0.6876788139343262 \t 15\t\n",
      "91 0.6872175931930542 \t 16\t\n",
      "92 0.6875322461128235 \t 16\t\n",
      "93 0.6878838539123535 \t 17\t\n",
      "94 0.6870493292808533 \t 14\t\n",
      "95 0.6874533891677856 \t 15\t\n",
      "96 0.6875125169754028 \t 15\t\n",
      "97 0.6879373788833618 \t 16\t\n",
      "98 0.6877915263175964 \t 16\t\n",
      "99 0.6881090998649597 \t 14\t\n",
      "\n",
      "===================================================\n",
      "100000\n",
      "0.681302691504173 \t 30\n",
      "0 0.684138834476471 \t 86\t\n",
      "1 0.6841579079627991 \t 72\t\n",
      "2 0.6841262578964233 \t 100\t\n",
      "3 0.684075653553009 \t 100\t\n",
      "4 0.6839693784713745 \t 100\t\n",
      "5 0.6842202544212341 \t 77\t\n",
      "6 0.6851423978805542 \t 32\t\n",
      "7 0.6840304136276245 \t 77\t\n",
      "8 0.6841589212417603 \t 100\t\n",
      "9 0.6838929653167725 \t 100\t\n",
      "10 0.6838283538818359 \t 100\t\n",
      "11 0.6836478114128113 \t 100\t\n",
      "12 0.6842419505119324 \t 66\t\n",
      "13 0.6837995052337646 \t 100\t\n",
      "14 0.6841735243797302 \t 100\t\n",
      "15 0.6838070154190063 \t 84\t\n",
      "16 0.6838139295578003 \t 100\t\n",
      "17 0.6839792132377625 \t 79\t\n",
      "18 0.6838789582252502 \t 72\t\n",
      "19 0.6839474439620972 \t 83\t\n",
      "20 0.6848726272583008 \t 41\t\n",
      "21 0.6847196817398071 \t 44\t\n",
      "22 0.6847708821296692 \t 41\t\n",
      "23 0.6848452091217041 \t 37\t\n",
      "24 0.6839777827262878 \t 100\t\n",
      "25 0.6836311221122742 \t 100\t\n",
      "26 0.6840554475784302 \t 100\t\n",
      "27 0.6840011477470398 \t 100\t\n",
      "28 0.6844451427459717 \t 71\t\n",
      "29 0.6841761469841003 \t 83\t\n",
      "30 0.683834433555603 \t 100\t\n",
      "31 0.6842084527015686 \t 66\t\n",
      "32 0.6839929819107056 \t 81\t\n",
      "33 0.6837779879570007 \t 100\t\n",
      "34 0.6836681962013245 \t 97\t\n",
      "35 0.6841707825660706 \t 74\t\n",
      "36 0.6840823292732239 \t 87\t\n",
      "37 0.6838535070419312 \t 96\t\n",
      "38 0.6839640736579895 \t 100\t\n",
      "39 0.6836017966270447 \t 92\t\n",
      "40 0.6837836503982544 \t 100\t\n",
      "41 0.6839116215705872 \t 91\t\n",
      "42 0.6842054724693298 \t 75\t\n",
      "43 0.6844797134399414 \t 56\t\n",
      "44 0.6848554611206055 \t 33\t\n",
      "45 0.6841790080070496 \t 86\t\n",
      "46 0.6843119263648987 \t 72\t\n",
      "47 0.6841834187507629 \t 84\t\n",
      "48 0.684158205986023 \t 80\t\n",
      "49 0.6839814186096191 \t 100\t\n",
      "50 0.684083878993988 \t 100\t\n",
      "51 0.6843463778495789 \t 74\t\n",
      "52 0.6843212246894836 \t 74\t\n",
      "53 0.684298574924469 \t 90\t\n",
      "54 0.6841467022895813 \t 81\t\n",
      "55 0.6837989687919617 \t 100\t\n",
      "56 0.6837320327758789 \t 87\t\n",
      "57 0.6842790842056274 \t 84\t\n",
      "58 0.684188961982727 \t 100\t\n",
      "59 0.6841548681259155 \t 73\t\n",
      "60 0.6838735342025757 \t 100\t\n",
      "61 0.6841114163398743 \t 80\t\n",
      "62 0.6850249767303467 \t 31\t\n",
      "63 0.6841720938682556 \t 84\t\n",
      "64 0.6840576529502869 \t 100\t\n",
      "65 0.6840181350708008 \t 85\t\n",
      "66 0.6842557787895203 \t 82\t\n",
      "67 0.6842617392539978 \t 64\t\n",
      "68 0.6840178370475769 \t 95\t\n",
      "69 0.6838722825050354 \t 98\t\n",
      "70 0.6842806339263916 \t 100\t\n",
      "71 0.6839167475700378 \t 100\t\n",
      "72 0.6842327117919922 \t 100\t\n",
      "73 0.6846752166748047 \t 55\t\n",
      "74 0.6840006113052368 \t 100\t\n",
      "75 0.6838254928588867 \t 83\t\n",
      "76 0.6837883591651917 \t 100\t\n",
      "77 0.6836646199226379 \t 100\t\n",
      "78 0.684912919998169 \t 38\t\n",
      "79 0.6843834519386292 \t 53\t\n",
      "80 0.683784544467926 \t 89\t\n",
      "81 0.6841945052146912 \t 79\t\n",
      "82 0.6842281818389893 \t 80\t\n",
      "83 0.6845722794532776 \t 57\t\n",
      "84 0.6845253109931946 \t 77\t\n",
      "85 0.6838318109512329 \t 85\t\n",
      "86 0.6841924786567688 \t 100\t\n",
      "87 0.6845263838768005 \t 44\t\n",
      "88 0.6839832663536072 \t 93\t\n",
      "89 0.6841117143630981 \t 79\t\n",
      "90 0.6851892471313477 \t 30\t\n",
      "91 0.6845483779907227 \t 64\t\n",
      "92 0.6841033101081848 \t 99\t\n",
      "93 0.6840853691101074 \t 72\t\n",
      "94 0.6841013431549072 \t 87\t\n",
      "95 0.6838981509208679 \t 100\t\n",
      "96 0.6849673390388489 \t 24\t\n",
      "97 0.6838308572769165 \t 99\t\n",
      "98 0.683893620967865 \t 100\t\n",
      "99 0.6836962699890137 \t 100\t\n",
      "\n",
      "===================================================\n",
      "1000000\n",
      "0.6770447024530344 \t 59\n",
      "0 0.6806052327156067 \t 100\t\n",
      "1 0.6807597279548645 \t 100\t\n",
      "2 0.6811131834983826 \t 100\t\n",
      "3 0.6808289289474487 \t 100\t\n",
      "4 0.6808929443359375 \t 100\t\n",
      "5 0.6809486150741577 \t 100\t\n",
      "6 0.6809713840484619 \t 100\t\n",
      "7 0.6808906197547913 \t 100\t\n",
      "8 0.6805509924888611 \t 100\t\n",
      "9 0.6809086203575134 \t 100\t\n",
      "10 0.6807851195335388 \t 100\t\n",
      "11 0.6811898946762085 \t 100\t"
     ]
    }
   ],
   "source": [
    "for N in Ns:\n",
    "    print('===================================================\\n{}'.format(N))\n",
    "    # Take the first N samples.\n",
    "    data, m, s = split_data(X[:N], y[:N])\n",
    "    \n",
    "    # Train BDT model (only need to train 1)\n",
    "    bdt_model = XGBClassifier(early_stopping_rounds = 10)\n",
    "    X_trn, X_vld, y_trn, y_vld = data\n",
    "    bdt_model.fit(X_trn, y_trn, eval_set = [(X_vld, y_vld)], verbose = 0)\n",
    "    trace = bdt_model.evals_result()['validation_0']\n",
    "    print(trace['logloss'][-1], '\\t', len(trace['logloss']), end = '\\n')\n",
    "    bdt_model.save_model(bdt_filestr.format(N))\n",
    "\n",
    "    for i in range(reps):\n",
    "        print(i, end = ' ')\n",
    "        # Train BCE model\n",
    "        bce_model, trace = train(data, **bce_params)\n",
    "        bce_model.save_weights(bce_filestr.format(N, i))\n",
    "        \n",
    "        # Train GBC model\n",
    "        gbc_model = GradientBoostingClassifier(validation_fraction = 0.25,\n",
    "                                               n_iter_no_change = 10)\n",
    "        gbc_model.fit(X[:N], y[:N])\n",
    "        dump(gbc_model, gbc_filestr.format(N, i))\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd8d17d-d03a-4d22-9a8f-537bfb822352",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## $d = 8$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d23a752-df10-4f18-bc9f-222e20f10212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "num = 0\n",
    "reps = 100\n",
    "d = 8\n",
    "Ns = 10**np.arange(2, 8)\n",
    "\n",
    "# Model parameters\n",
    "bce_params = {'loss':bce, 'd': d}\n",
    "\n",
    "filestr = 'models/zenodo/{}/set_{}/'.format(d, num)\n",
    "bce_filestr = filestr + 'bce/model_{}_{}.h5'\n",
    "bdt_filestr = filestr + 'bdt/model_{}.h5'\n",
    "gbc_filestr = filestr + 'gbc/model_{}_{}.h5'\n",
    "\n",
    "if not os.path.isdir(filestr):\n",
    "    os.mkdir(filestr)\n",
    "\n",
    "if not os.path.isdir(filestr + 'bce/'):\n",
    "    os.mkdir(filestr + 'bce/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'bdt/'):\n",
    "    os.mkdir(filestr + 'bdt/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'gbc/'):\n",
    "    os.mkdir(filestr + 'gbc/')\n",
    "\n",
    "# Data parameters\n",
    "X = np.load('data/zenodo/{}/X_trn.npy'.format(d))\n",
    "y = np.load('data/zenodo/{}/y_trn.npy'.format(d)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a5ea4b-dc38-470e-a65d-9c3217dcc73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================\n",
      "100\n",
      "0.9518972730636597 \t 11\n",
      "0 0.709384024143219 \t 18\t\n",
      "1 0.6903370022773743 \t 11\t\n",
      "2 0.7108402848243713 \t 27\t\n",
      "3 0.6931536793708801 \t 31\t\n",
      "4 0.7491366863250732 \t 14\t\n",
      "5 0.7155307531356812 \t 19\t\n",
      "6 0.7148614525794983 \t 30\t\n",
      "7 0.6891833543777466 \t 11\t\n",
      "8 0.6990330219268799 \t 12\t\n",
      "9 0.7314398884773254 \t 13\t\n",
      "10 0.6853344440460205 \t 14\t\n",
      "11 0.6971005201339722 \t 30\t\n",
      "12 0.7055714130401611 \t 23\t\n",
      "13 0.7031133770942688 \t 11\t\n",
      "14 0.6690502762794495 \t 11\t\n",
      "15 0.7138792276382446 \t 24\t\n",
      "16 0.6743512153625488 \t 14\t\n",
      "17 0.7229437232017517 \t 19\t\n",
      "18 0.7388466596603394 \t 27\t\n",
      "19 0.7298656702041626 \t 18\t\n",
      "20 0.7127333879470825 \t 14\t\n",
      "21 0.6981571316719055 \t 12\t\n",
      "22 0.7467792630195618 \t 11\t\n",
      "23 0.7420686483383179 \t 13\t\n",
      "24 0.7143241167068481 \t 12\t\n",
      "25 0.7168168425559998 \t 20\t\n",
      "26 0.7774333357810974 \t 25\t\n",
      "27 0.7238531708717346 \t 11\t\n",
      "28 0.7375357151031494 \t 15\t\n",
      "29 0.6954509615898132 \t 12\t\n",
      "30 0.7315837740898132 \t 13\t\n",
      "31 0.7061780691146851 \t 12\t\n",
      "32 0.7016160488128662 \t 14\t\n",
      "33 0.6699199080467224 \t 33\t\n",
      "34 0.7319837212562561 \t 29\t\n",
      "35 0.6994376182556152 \t 11\t\n",
      "36 0.6985298991203308 \t 11\t\n",
      "37 0.6926833987236023 \t 19\t\n",
      "38 0.7123429775238037 \t 11\t\n",
      "39 0.7307416796684265 \t 20\t\n",
      "40 0.6952579021453857 \t 19\t\n",
      "41 0.782962441444397 \t 15\t\n",
      "42 0.6791807413101196 \t 21\t\n",
      "43 0.7025118470191956 \t 12\t\n",
      "44 0.6870179772377014 \t 14\t\n",
      "45 0.6905309557914734 \t 21\t\n",
      "46 0.7015548944473267 \t 14\t\n",
      "47 0.7520163655281067 \t 29\t\n",
      "48 0.7206225395202637 \t 13\t\n",
      "49 0.7416555881500244 \t 16\t\n",
      "50 0.709735095500946 \t 26\t\n",
      "51 0.7089530229568481 \t 11\t\n",
      "52 0.7230337262153625 \t 16\t\n",
      "53 0.6718707084655762 \t 22\t\n",
      "54 0.7250135540962219 \t 17\t\n",
      "55 0.6820834279060364 \t 11\t\n",
      "56 0.7074435949325562 \t 12\t\n",
      "57 0.688452959060669 \t 20\t\n",
      "58 0.7314817905426025 \t 11\t\n",
      "59 0.7170465588569641 \t 14\t\n",
      "60 0.702341616153717 \t 13\t\n",
      "61 0.666179895401001 \t 11\t\n",
      "62 0.7127292156219482 \t 11\t\n",
      "63 0.7010020613670349 \t 11\t\n",
      "64 0.7164983153343201 \t 11\t\n",
      "65 0.7060964703559875 \t 15\t\n",
      "66 0.7190104722976685 \t 12\t\n",
      "67 0.7139549255371094 \t 18\t\n",
      "68 0.7013193368911743 \t 17\t\n",
      "69 0.6594247221946716 \t 16\t\n",
      "70 0.746380627155304 \t 13\t\n",
      "71 0.6900457739830017 \t 14\t\n",
      "72 0.6925997138023376 \t 14\t\n",
      "73 0.7272960543632507 \t 16\t\n",
      "74 0.6920022368431091 \t 20\t\n",
      "75 0.7644491791725159 \t 16\t\n",
      "76 0.7016369700431824 \t 14\t\n",
      "77 0.7044133543968201 \t 11\t\n",
      "78 0.7323266863822937 \t 20\t\n",
      "79 0.7196840047836304 \t 23\t\n",
      "80 0.782949686050415 \t 14\t\n",
      "81 0.7717683911323547 \t 13\t\n",
      "82 0.7059258818626404 \t 13\t\n",
      "83 0.706663191318512 \t 12\t\n",
      "84 0.7186335921287537 \t 20\t\n",
      "85 0.7201023101806641 \t 13\t\n",
      "86 0.7331074476242065 \t 11\t\n",
      "87 0.71026211977005 \t 18\t\n",
      "88 0.7546181678771973 \t 12\t\n",
      "89 0.7066775560379028 \t 14\t\n",
      "90 0.7191600203514099 \t 12\t\n",
      "91 0.7146489024162292 \t 17\t\n",
      "92 0.7370787262916565 \t 18\t\n",
      "93 0.7350326776504517 \t 26\t\n",
      "94 0.7397605776786804 \t 12\t\n",
      "95 0.7058631777763367 \t 16\t\n",
      "96 0.6842207312583923 \t 14\t\n",
      "97 0.7300035953521729 \t 16\t\n",
      "98 0.681471586227417 \t 14\t\n",
      "99 0.7250635027885437 \t 12\t\n",
      "\n",
      "===================================================\n",
      "1000\n",
      "0.6078300537653267 \t 23\n",
      "0 0.6143078804016113 \t 24\t\n",
      "1 0.620725691318512 \t 32\t\n",
      "2 0.6287177205085754 \t 23\t\n",
      "3 0.633126974105835 \t 25\t\n",
      "4 0.6325826048851013 \t 21\t\n",
      "5 0.6381672620773315 \t 26\t\n",
      "6 0.6333668231964111 \t 35\t\n",
      "7 0.6304200291633606 \t 17\t\n",
      "8 0.6450865268707275 \t 20\t\n",
      "9 0.6292452812194824 \t 22\t\n",
      "10 0.6404582262039185 \t 27\t\n",
      "11 0.6440305113792419 \t 19\t\n",
      "12 0.6261605024337769 \t 21\t\n",
      "13 0.6314833164215088 \t 25\t\n",
      "14 0.6504704356193542 \t 34\t\n",
      "15 0.6394928693771362 \t 30\t\n",
      "16 0.6225827932357788 \t 20\t\n",
      "17 0.6324000954627991 \t 18\t\n",
      "18 0.6275679469108582 \t 22\t\n",
      "19 0.6359745860099792 \t 27\t\n",
      "20 0.6310551762580872 \t 26\t\n",
      "21 0.631714403629303 \t 27\t\n",
      "22 0.6405708193778992 \t 18\t\n",
      "23 0.6292231678962708 \t 20\t\n",
      "24 0.6426370739936829 \t 20\t\n",
      "25 0.6288697719573975 \t 22\t\n",
      "26 0.634560227394104 \t 28\t\n",
      "27 0.6429110169410706 \t 25\t\n",
      "28 0.6189242601394653 \t 21\t\n",
      "29 0.6417050361633301 \t 20\t\n",
      "30 0.6289882063865662 \t 19\t\n",
      "31 0.6273749470710754 \t 22\t\n",
      "32 0.637649416923523 \t 20\t\n",
      "33 0.6286323070526123 \t 17\t\n",
      "34 0.6297308206558228 \t 22\t\n",
      "35 0.6257867217063904 \t 18\t\n",
      "36 0.6392427682876587 \t 27\t\n",
      "37 0.6324108242988586 \t 32\t\n",
      "38 0.6316590905189514 \t 20\t\n",
      "39 0.6251860857009888 \t 21\t\n",
      "40 0.6327833533287048 \t 30\t\n",
      "41 0.6367542743682861 \t 22\t\n",
      "42 0.6256266832351685 \t 23\t\n",
      "43 0.6441572308540344 \t 28\t\n",
      "44 0.6332496404647827 \t 32\t\n",
      "45 0.6296461820602417 \t 18\t\n",
      "46 0.6410207152366638 \t 29\t\n",
      "47 0.6234330534934998 \t 21\t\n",
      "48 0.6370786428451538 \t 25\t\n",
      "49 0.6254344582557678 \t 19\t\n",
      "50 0.6398974657058716 \t 23\t\n",
      "51 0.6361767053604126 \t 21\t\n",
      "52 0.63812255859375 \t 22\t\n",
      "53 0.6343259215354919 \t 28\t\n",
      "54 0.619536280632019 \t 25\t\n",
      "55 0.6296439170837402 \t 20\t\n",
      "56 0.632016122341156 \t 28\t\n",
      "57 0.6398107409477234 \t 23\t\n",
      "58 0.6438387632369995 \t 22\t\n",
      "59 0.6358059644699097 \t 24\t\n",
      "60 0.6254759430885315 \t 25\t\n",
      "61 0.626513659954071 \t 23\t\n",
      "62 0.6299804449081421 \t 22\t\n",
      "63 0.6312261819839478 \t 19\t\n",
      "64 0.6359591484069824 \t 38\t\n",
      "65 0.6476545333862305 \t 19\t\n",
      "66 0.6317407488822937 \t 29\t\n",
      "67 0.6295847296714783 \t 20\t\n",
      "68 0.6386283040046692 \t 20\t\n",
      "69 0.6321872472763062 \t 24\t\n",
      "70 0.6170649528503418 \t 35\t\n",
      "71 0.6296983361244202 \t 20\t\n",
      "72 0.6327931880950928 \t 27\t\n",
      "73 0.640516459941864 \t 23\t\n",
      "74 0.6446596384048462 \t 31\t\n",
      "75 0.6260227560997009 \t 20\t\n",
      "76 0.6346306204795837 \t 33\t\n",
      "77 0.6360145211219788 \t 24\t\n",
      "78 0.6521380543708801 \t 25\t\n",
      "79 0.6344636678695679 \t 24\t\n",
      "80 0.632315456867218 \t 22\t\n",
      "81 0.6322817206382751 \t 21\t\n",
      "82 0.625903844833374 \t 22\t\n",
      "83 0.6194499731063843 \t 28\t\n",
      "84 0.624226450920105 \t 25\t\n",
      "85 0.6372271776199341 \t 21\t\n",
      "86 0.6211719512939453 \t 25\t\n",
      "87 0.6392719149589539 \t 23\t\n",
      "88 0.6239962577819824 \t 25\t\n",
      "89 0.6310061812400818 \t 19\t\n",
      "90 0.6333414316177368 \t 29\t\n",
      "91 0.6248311400413513 \t 29\t\n",
      "92 0.6277614831924438 \t 47\t\n",
      "93 0.6245312690734863 \t 19\t\n",
      "94 0.6340160965919495 \t 23\t\n",
      "95 0.6258096098899841 \t 26\t\n",
      "96 0.6309432983398438 \t 22\t\n",
      "97 0.6292186379432678 \t 23\t\n",
      "98 0.625302255153656 \t 32\t\n",
      "99 0.6352430582046509 \t 20\t\n",
      "\n",
      "===================================================\n",
      "10000\n",
      "0.45482013131691346 \t 72\n",
      "0 0.3375754952430725 \t 100\t\n",
      "1 0.34345996379852295 \t 100\t\n",
      "2 0.35310736298561096 \t 100\t\n",
      "3 0.3409672975540161 \t 100\t\n",
      "4 0.34305688738822937 \t 100\t\n",
      "5 0.33375462889671326 \t 100\t\n",
      "6 0.3504858613014221 \t 100\t\n",
      "7 0.345089852809906 \t 100\t\n",
      "8 0.3435576558113098 \t 100\t\n",
      "9 0.34008949995040894 \t 100\t\n",
      "10 0.33529365062713623 \t 100\t\n",
      "11 0.35062175989151 \t 100\t\n",
      "12 0.3368602991104126 \t 100\t\n",
      "13 0.3377721309661865 \t 100\t\n",
      "14 0.3486829698085785 \t 100\t\n",
      "15 0.34377673268318176 \t 100\t\n",
      "16 0.33917734026908875 \t 100\t\n",
      "17 0.354815274477005 \t 100\t\n",
      "18 0.3455086052417755 \t 100\t\n",
      "19 0.338392972946167 \t 100\t\n",
      "20 0.3413899838924408 \t 100\t\n",
      "21 0.3436417579650879 \t 100\t\n",
      "22 0.34349891543388367 \t 100\t\n",
      "23 0.3347087502479553 \t 100\t\n",
      "24 0.3598804771900177 \t 100\t\n",
      "25 0.35836976766586304 \t 100\t\n",
      "26 0.339444100856781 \t 100\t\n",
      "27 0.3476371169090271 \t 100\t\n",
      "28 0.3387044370174408 \t 100\t\n",
      "29 0.3438219726085663 \t 100\t\n",
      "30 0.3471173644065857 \t 100\t\n",
      "31 0.35294151306152344 \t 100\t\n",
      "32 0.34124755859375 \t 100\t\n",
      "33 0.3494538366794586 \t 100\t\n",
      "34 0.334171324968338 \t 100\t\n",
      "35 0.34351328015327454 \t 100\t\n",
      "36 0.3357486426830292 \t 100\t\n",
      "37 0.33958670496940613 \t 100\t\n",
      "38 0.345028281211853 \t 100\t\n",
      "39 0.3508177101612091 \t 100\t\n",
      "40 0.34833309054374695 \t 100\t\n",
      "41 0.3487376570701599 \t 100\t\n",
      "42 0.342899888753891 \t 100\t\n",
      "43 0.34222549200057983 \t 100\t\n",
      "44 0.3611273765563965 \t 100\t\n",
      "45 0.33961498737335205 \t 100\t\n",
      "46 0.33226630091667175 \t 100\t\n",
      "47 0.34033048152923584 \t 100\t\n",
      "48 0.34251850843429565 \t 100\t\n",
      "49 0.3389784097671509 \t 100\t\n",
      "50 0.34236252307891846 \t 100\t\n",
      "51 0.331944078207016 \t 100\t\n",
      "52 0.34003815054893494 \t 100\t\n",
      "53 0.3464592397212982 \t 100\t\n",
      "54 0.3474883437156677 \t 100\t\n",
      "55 0.3437809944152832 \t 100\t\n",
      "56 0.34498122334480286 \t 100\t\n",
      "57 0.3553899824619293 \t 100\t\n",
      "58 0.344035267829895 \t 100\t\n",
      "59 0.3440849483013153 \t 100\t\n",
      "60 0.35240253806114197 \t 100\t\n",
      "61 0.35063350200653076 \t 100\t\n",
      "62 0.34006938338279724 \t 100\t\n",
      "63 0.3380580544471741 \t 100\t\n",
      "64 0.3491705358028412 \t 100\t\n",
      "65 0.3551029562950134 \t 100\t\n",
      "66 0.35443028807640076 \t 100\t\n",
      "67 0.33537596464157104 \t 100\t\n",
      "68 0.34675461053848267 \t 100\t\n",
      "69 0.349773108959198 \t 100\t\n",
      "70 0.3445517122745514 \t 100\t\n",
      "71 0.3445325195789337 \t 100\t\n",
      "72 0.3521602749824524 \t 100\t\n",
      "73 0.3508756756782532 \t 100\t\n",
      "74 0.349027156829834 \t 100\t\n",
      "75 0.33847999572753906 \t 100\t\n",
      "76 0.3448728919029236 \t 100\t\n",
      "77 0.33709657192230225 \t 100\t\n",
      "78 0.3423582911491394 \t 100\t\n",
      "79 0.3513525426387787 \t 100\t\n",
      "80 0.34295713901519775 \t 100\t\n",
      "81 0.34855151176452637 \t 100\t\n",
      "82 0.3385098874568939 \t 100\t\n",
      "83 0.33887216448783875 \t 100\t\n",
      "84 0.3377937376499176 \t 100\t\n",
      "85 0.3554981052875519 \t 100\t\n",
      "86 0.34159040451049805 \t 100\t\n",
      "87 0.338562935590744 \t 100\t\n",
      "88 0.33804553747177124 \t 100\t\n",
      "89 0.3344878554344177 \t 100\t\n",
      "90 0.33962690830230713 \t 100\t\n",
      "91 0.34280550479888916 \t 100\t\n",
      "92 0.3538309335708618 \t 100\t\n",
      "93 0.3416011929512024 \t 100\t\n",
      "94 0.35289472341537476 \t 100\t\n",
      "95 0.3498065769672394 \t 100\t\n",
      "96 0.35162419080734253 \t 100\t\n",
      "97 0.35087624192237854 \t 100\t\n",
      "98 0.33630338311195374 \t 100\t\n",
      "99 0.3423784673213959 \t 100\t\n",
      "\n",
      "===================================================\n",
      "100000\n",
      "0.37043665670823495 \t 100\n",
      "0 0.27765870094299316 \t 100\t\n",
      "1 0.28142327070236206 \t 100\t\n",
      "2 0.2818315327167511 \t 100\t\n",
      "3 0.2765744626522064 \t 100\t\n",
      "4 0.282291442155838 \t 100\t\n",
      "5 0.284334659576416 \t 100\t\n",
      "6 0.27973219752311707 \t 100\t\n",
      "7 0.28218889236450195 \t 100\t\n",
      "8 0.28031110763549805 \t 100\t\n",
      "9 0.28040453791618347 \t 100\t\n",
      "10 0.27808746695518494 \t 100\t\n",
      "11 0.2829422950744629 \t 100\t\n",
      "12 0.2810603380203247 \t 100\t\n",
      "13 0.28183713555336 \t 100\t\n",
      "14 0.28496792912483215 \t 100\t\n",
      "15 0.2802692651748657 \t 100\t\n",
      "16 0.28084200620651245 \t 100\t\n",
      "17 0.2790227234363556 \t 100\t\n",
      "18 0.28015777468681335 \t 100\t\n",
      "19 0.28455522656440735 \t 100\t\n",
      "20 0.2827293872833252 \t 100\t\n",
      "21 0.2800883650779724 \t 100\t\n",
      "22 0.2789590656757355 \t 100\t\n",
      "23 0.2821722626686096 \t 100\t\n",
      "24 0.2786351144313812 \t 100\t\n",
      "25 0.28049492835998535 \t 100\t\n",
      "26 0.2861282229423523 \t 100\t\n",
      "27 0.2823595106601715 \t 100\t\n",
      "28 0.2792856693267822 \t 100\t\n",
      "29 0.28210997581481934 \t 100\t\n",
      "30 0.2823292911052704 \t 100\t\n",
      "31 0.28169652819633484 \t 100\t\n",
      "32 0.2785604000091553 \t 100\t\n",
      "33 0.27795928716659546 \t 100\t\n",
      "34 0.2823483943939209 \t 100\t\n",
      "35 0.28498828411102295 \t 100\t\n",
      "36 0.2826118469238281 \t 100\t\n",
      "37 0.2827877402305603 \t 100\t\n",
      "38 0.2816968262195587 \t 100\t\n",
      "39 0.2783501148223877 \t 100\t\n",
      "40 0.2762593626976013 \t 100\t\n",
      "41 0.28176113963127136 \t 100\t\n",
      "42 0.2783263027667999 \t 100\t\n",
      "43 0.2786933481693268 \t 100\t\n",
      "44 0.28083133697509766 \t 100\t\n",
      "45 0.28114762902259827 \t 100\t\n",
      "46 0.2800314724445343 \t 100\t\n",
      "47 0.27783530950546265 \t 100\t\n",
      "48 0.2803078591823578 \t 100\t\n",
      "49 0.2806079685688019 \t 100\t\n",
      "50 0.2782266139984131 \t 100\t\n",
      "51 0.2822965681552887 \t 100\t\n",
      "52 0.2816442847251892 \t 100\t\n",
      "53 0.2792738676071167 \t 100\t\n",
      "54 0.28194600343704224 \t 100\t\n",
      "55 0.28124645352363586 \t 100\t\n",
      "56 0.27970778942108154 \t 100\t\n",
      "57 0.28174808621406555 \t 100\t\n",
      "58 0.2775520384311676 \t 100\t\n",
      "59 0.27893656492233276 \t 100\t\n",
      "60 0.2808620035648346 \t 100\t\n",
      "61 0.27792853116989136 \t 100\t\n",
      "62 0.28551191091537476 \t 100\t\n",
      "63 0.28103575110435486 \t 100\t\n",
      "64 0.2755742073059082 \t 100\t\n",
      "65 0.28078174591064453 \t 100\t\n",
      "66 0.28127434849739075 \t 100\t\n",
      "67 0.2842487096786499 \t 100\t\n",
      "68 0.2811514139175415 \t 100\t\n",
      "69 0.2805972695350647 \t 100\t\n",
      "70 0.28218406438827515 \t 100\t\n",
      "71 0.2806485891342163 \t 100\t\n",
      "72 0.28240084648132324 \t 100\t\n",
      "73 0.2834166884422302 \t 100\t\n",
      "74 0.28053709864616394 \t 100\t\n",
      "75 0.2790660262107849 \t 100\t\n",
      "76 0.2818184196949005 \t 100\t\n",
      "77 0.2769820988178253 \t 100\t\n",
      "78 0.27952301502227783 \t 100\t\n",
      "79 0.2823329269886017 \t 100\t\n",
      "80 0.28383925557136536 \t 100\t\n",
      "81 0.2827589213848114 \t 100\t\n",
      "82 0.2826564311981201 \t 100\t\n",
      "83 0.2785206437110901 \t 100\t\n",
      "84 0.27474913001060486 \t 100\t\n",
      "85 0.2833174467086792 \t 100\t\n",
      "86 0.2821369767189026 \t 100\t\n",
      "87 0.28124475479125977 \t 100\t\n",
      "88 0.2804253101348877 \t 100\t\n",
      "89 0.2819846570491791 \t 100\t\n",
      "90 0.28123024106025696 \t 100\t\n",
      "91 0.28110212087631226 \t 100\t\n",
      "92 0.2776033282279968 \t 100\t\n",
      "93 0.28328800201416016 \t 100\t\n",
      "94 0.28273963928222656 \t 100\t\n",
      "95 0.27784276008605957 \t 100\t\n",
      "96 0.27946752309799194 \t 100\t\n",
      "97 0.2811012864112854 \t 100\t\n",
      "98 0.2815941870212555 \t 100\t\n",
      "99 0.278921514749527 \t 100\t\n",
      "\n",
      "===================================================\n",
      "1000000\n",
      "0.3563645810351724 \t 100\n",
      "0 0.26082101464271545 \t 100\t\n",
      "1 0.25919562578201294 \t 100\t\n",
      "2 0.26098132133483887 \t 100\t\n",
      "3 0.2582319676876068 \t 100\t\n",
      "4 0.26207053661346436 \t 100\t\n",
      "5 0.2613554894924164 \t 100\t\n",
      "6 0.26026812195777893 \t 100\t\n",
      "7 0.2612721920013428 \t 100\t\n",
      "8 0.26114386320114136 \t 100\t\n",
      "9 0.2628743350505829 \t 100\t\n",
      "10 0.26131874322891235 \t 100\t\n",
      "11 0.2590778172016144 \t 100\t\n",
      "12 0.2589606046676636 \t 100\t\n",
      "13 0.2601695954799652 \t 100\t\n",
      "14 0.25881534814834595 \t 100\t\n",
      "15 0.25779715180397034 \t 100\t"
     ]
    }
   ],
   "source": [
    "for N in Ns:\n",
    "    print('===================================================\\n{}'.format(N))\n",
    "    # Take the first N samples.\n",
    "    data, m, s = split_data(X[:N], y[:N])\n",
    "    \n",
    "    # Train BDT model (only need to train 1)\n",
    "    bdt_model = XGBClassifier(early_stopping_rounds = 10)\n",
    "    X_trn, X_vld, y_trn, y_vld = data\n",
    "    bdt_model.fit(X_trn, y_trn, eval_set = [(X_vld, y_vld)], verbose = 0)\n",
    "    trace = bdt_model.evals_result()['validation_0']\n",
    "    print(trace['logloss'][-1], '\\t', len(trace['logloss']), end = '\\n')\n",
    "    bdt_model.save_model(bdt_filestr.format(N))\n",
    "\n",
    "    for i in range(reps):\n",
    "        print(i, end = ' ')\n",
    "        # Train BCE model\n",
    "        bce_model, trace = train(data, **bce_params)\n",
    "        bce_model.save_weights(bce_filestr.format(N, i))\n",
    "        \n",
    "        # Train GBC model\n",
    "        gbc_model = GradientBoostingClassifier(validation_fraction = 0.25,\n",
    "                                               n_iter_no_change = 10)\n",
    "        gbc_model.fit(X[:N], y[:N])\n",
    "        dump(gbc_model, gbc_filestr.format(N, i))\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef99106-1889-4016-afdc-36c47f2bb0a2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## $d = 11$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa77dcda-85fc-4e15-afc9-2081a23d9796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "num = 0\n",
    "reps = 100\n",
    "d = 11\n",
    "Ns = 10**np.arange(2, 8)\n",
    "\n",
    "# Model parameters\n",
    "bce_params = {'loss':bce, 'd': d}\n",
    "\n",
    "filestr = 'models/zenodo/{}/set_{}/'.format(d, num)\n",
    "bce_filestr = filestr + 'bce/model_{}_{}.h5'\n",
    "bdt_filestr = filestr + 'bdt/model_{}.h5'\n",
    "gbc_filestr = filestr + 'gbc/model_{}_{}.h5'\n",
    "\n",
    "if not os.path.isdir(filestr):\n",
    "    os.mkdir(filestr)\n",
    "\n",
    "if not os.path.isdir(filestr + 'bce/'):\n",
    "    os.mkdir(filestr + 'bce/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'bdt/'):\n",
    "    os.mkdir(filestr + 'bdt/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'gbc/'):\n",
    "    os.mkdir(filestr + 'gbc/')\n",
    "\n",
    "# Data parameters\n",
    "X = np.load('data/zenodo/{}/X_trn.npy'.format(d))\n",
    "y = np.load('data/zenodo/{}/y_trn.npy'.format(d)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918ba1cb-8b9c-4545-bb0d-9ddbaf89bfed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================\n",
      "100\n",
      "0.7719044119119645 \t 11\n",
      "0 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-22 15:35:49.821434: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-22 15:35:50.452955: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22824 MB memory:  -> device: 0, name: Quadro RTX 6000, pci bus id: 0000:c1:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.87884521484375 \t 14\t1 0.7917284965515137 \t 16\t2 0.8255318403244019 \t 11\t3 0.9300541877746582 \t 15\t4 0.7780203819274902 \t 20\t5 0.9699823260307312 \t 12\t6 0.8516292572021484 \t 14\t7 0.7486464977264404 \t 19\t8 0.9657136797904968 \t 11\t9 0.9924226403236389 \t 12\t10 0.8113024830818176 \t 16\t11 0.9820797443389893 \t 11\t12 0.9461304545402527 \t 12\t13 1.0104331970214844 \t 11\t14 0.7584745287895203 \t 17\t15 1.0052802562713623 \t 12\t16 0.956510603427887 \t 12\t17 0.969941258430481 \t 11\t18 0.8772228360176086 \t 14\t19 0.9899536967277527 \t 14\t20 0.8368549346923828 \t 11\t21 1.0103875398635864 \t 22\t22 0.9701393842697144 \t 11\t23 0.8077855706214905 \t 17\t24 0.7590672373771667 \t 16\t25 1.1377935409545898 \t 11\t26 0.8945189118385315 \t 24\t27 0.8300597667694092 \t 11\t28 0.9364996552467346 \t 12\t29 1.0152965784072876 \t 12\t30 0.9023878574371338 \t 11\t31 0.7208293676376343 \t 16\t32 0.9746490716934204 \t 14\t33 0.8506117463111877 \t 11\t34 0.814598798751831 \t 11\t35 0.9330777525901794 \t 11\t36 0.8216873407363892 \t 11\t37 0.8917598724365234 \t 11\t38 0.7959210872650146 \t 14\t39 0.7421556711196899 \t 21\t40 0.8216590881347656 \t 12\t41 0.9617167115211487 \t 13\t42 0.7428314089775085 \t 11\t43 0.8309754729270935 \t 13\t44 0.7214946150779724 \t 14\t45 0.8015955090522766 \t 11\t46 0.9535140991210938 \t 11\t47 0.9203053116798401 \t 11\t48 0.946358323097229 \t 11\t49 0.9091984033584595 \t 16\t50 0.797420859336853 \t 14\t51 0.9010583758354187 \t 13\t52 0.702628493309021 \t 11\t53 0.9063597917556763 \t 11\t54 1.0241895914077759 \t 14\t55 0.891918957233429 \t 17\t56 0.9876717925071716 \t 11\t57 0.9686765074729919 \t 11\t58 0.8504368662834167 \t 11\t59 0.9287845492362976 \t 11\t60 0.7179742455482483 \t 11\t61 0.8636058568954468 \t 12\t62 0.8768308162689209 \t 13\t63 0.7958691120147705 \t 12\t64 0.7968878149986267 \t 13\t65 0.828499436378479 \t 11\t66 0.9344432950019836 \t 14\t67 0.9012404084205627 \t 11\t68 0.8925836086273193 \t 12\t69 0.8689378499984741 \t 11\t70 0.9517550468444824 \t 11\t71 0.9729159474372864 \t 14\t72 0.7142103314399719 \t 11\t73 0.8418023586273193 \t 12\t74 0.7639959454536438 \t 11\t75 0.9098703861236572 \t 15\t76 0.954170823097229 \t 11\t77 0.8319486975669861 \t 12\t78 0.9202926754951477 \t 15\t79 0.8907086253166199 \t 11\t80 0.9853643774986267 \t 11\t81 0.9254144430160522 \t 11\t82 0.9211791157722473 \t 11\t83 0.7729415893554688 \t 12\t84 0.8634361028671265 \t 13\t85 0.8012763857841492 \t 11\t86 1.0464211702346802 \t 12\t87 0.8520194292068481 \t 11\t88 0.7451659440994263 \t 11\t89 0.8380892276763916 \t 15\t90 1.0077908039093018 \t 19\t91 0.8903601765632629 \t 14\t92 1.0264613628387451 \t 12\t93 0.750845193862915 \t 17\t94 0.993755042552948 \t 11\t95 1.002344012260437 \t 11\t96 0.9744788408279419 \t 11\t97 0.7883859872817993 \t 11\t98 0.8356701135635376 \t 13\t99 0.8983502984046936 \t 15\t\n",
      "===================================================\n",
      "1000\n",
      "0.7094682978019118 \t 20\n",
      "0 0.6722387075424194 \t 27\t1 0.6689797043800354 \t 32\t2 0.6773018836975098 \t 28\t3 0.6606743335723877 \t 25\t4 0.6571248769760132 \t 25\t5 0.6559275388717651 \t 35\t6 0.6788261532783508 \t 26\t7 0.6498763561248779 \t 25\t8 0.6441377401351929 \t 32\t9 0.677136242389679 \t 24\t10 0.6470930576324463 \t 29\t11 0.6605169773101807 \t 24\t12 0.6469618082046509 \t 31\t13 0.6497113108634949 \t 27\t14 0.6621766090393066 \t 28\t15 0.6513429880142212 \t 36\t16 0.6731461882591248 \t 25\t17 0.6535953283309937 \t 31\t18 0.660567045211792 \t 32\t19 0.6571175456047058 \t 29\t20 0.6750013828277588 \t 53\t21 0.653853178024292 \t 26\t22 0.6662027835845947 \t 25\t23 0.6676543951034546 \t 26\t24 0.6510090231895447 \t 35\t25 0.6601638197898865 \t 23\t26 0.6614006161689758 \t 29\t27 0.6620181798934937 \t 28\t28 0.6609740257263184 \t 26\t29 0.6606072783470154 \t 26\t30 0.6631003618240356 \t 21\t31 0.6621955633163452 \t 25\t32 0.6596550941467285 \t 28\t33 0.6483243703842163 \t 25\t34 0.6574022173881531 \t 40\t35 0.6523955464363098 \t 29\t36 0.6542636752128601 \t 30\t37 0.6729111671447754 \t 23\t38 0.6719410419464111 \t 27\t39 0.6677197217941284 \t 24\t40 0.6465209126472473 \t 26\t41 0.6575194001197815 \t 20\t42 0.6476414203643799 \t 24\t43 0.6628771424293518 \t 25\t44 0.6758676171302795 \t 29\t45 0.6464366316795349 \t 31\t46 0.6580356359481812 \t 29\t47 0.6519291996955872 \t 21\t48 0.6710989475250244 \t 26\t49 0.657051146030426 \t 25\t50 0.677067756652832 \t 34\t51 0.6665804386138916 \t 26\t52 0.6728890538215637 \t 24\t53 0.6546735167503357 \t 25\t54 0.6441622376441956 \t 26\t55 0.6642394065856934 \t 29\t56 0.6744073629379272 \t 34\t57 0.6614033579826355 \t 30\t58 0.6543450951576233 \t 26\t59 0.654757559299469 \t 24\t60 0.6631303429603577 \t 28\t61 0.6741456389427185 \t 28\t62 0.6728100180625916 \t 29\t63 0.6632481813430786 \t 31\t64 0.6646341681480408 \t 32\t65 0.6542304754257202 \t 33\t66 0.6543090343475342 \t 26\t67 0.6783856153488159 \t 35\t68 0.6558132171630859 \t 44\t69 0.6690055131912231 \t 33\t70 0.6815427541732788 \t 29\t71 0.665800929069519 \t 24\t72 0.6474840044975281 \t 34\t73 0.639597475528717 \t 28\t74 0.6705564260482788 \t 29\t75 0.6560782194137573 \t 32\t76 0.6529382467269897 \t 34\t77 0.681637704372406 \t 36\t78 0.6747735738754272 \t 30\t79 0.662204384803772 \t 25\t80 0.6606557369232178 \t 27\t81 0.6862902045249939 \t 22\t82 0.6715842485427856 \t 33\t83 0.6761935949325562 \t 32\t84 0.6574466824531555 \t 29\t85 0.6568532586097717 \t 30\t86 0.6545553803443909 \t 35\t87 0.661827027797699 \t 27\t88 0.6615698337554932 \t 22\t89 0.653268575668335 \t 34\t90 0.6824020147323608 \t 28\t91 0.6580986380577087 \t 24\t92 0.6545767784118652 \t 34\t93 0.6605249047279358 \t 34\t94 0.6514653563499451 \t 27\t95 0.641165554523468 \t 32\t96 0.6508120894432068 \t 30\t97 0.6433682441711426 \t 31\t98 0.67371666431427 \t 21\t99 0.6556326746940613 \t 27\t\n",
      "===================================================\n",
      "10000\n",
      "0.5280892907662171 \t 72\n",
      "0 0.47019702196121216 \t 100\t1 0.48349112272262573 \t 100\t2 0.48299697041511536 \t 100\t3 0.48136773705482483 \t 100\t4 0.4758509695529938 \t 100\t5 0.48987188935279846 \t 100\t6 0.4785129427909851 \t 100\t7 0.486820787191391 \t 100\t8 0.4802681505680084 \t 100\t9 0.4881398379802704 \t 100\t10 0.4950389564037323 \t 100\t11 0.4922853112220764 \t 100\t12 0.4695083498954773 \t 100\t13 0.4919675886631012 \t 100\t14 0.48836368322372437 \t 100\t15 0.47646889090538025 \t 100\t16 0.4895756244659424 \t 100\t17 0.4910379946231842 \t 100\t18 0.47581616044044495 \t 100\t19 0.49123111367225647 \t 100\t20 0.4955168068408966 \t 100\t21 0.4840056598186493 \t 100\t22 0.4736069440841675 \t 100\t23 0.4673789441585541 \t 100\t24 0.4792850613594055 \t 100\t25 0.4817273020744324 \t 100\t26 0.49731209874153137 \t 100\t27 0.4966579079627991 \t 100\t28 0.4802120625972748 \t 100\t29 0.4852530360221863 \t 100\t30 0.4811705946922302 \t 100\t31 0.48770156502723694 \t 100\t32 0.5021776556968689 \t 100\t33 0.485232412815094 \t 100\t34 0.48985394835472107 \t 100\t35 0.4766692817211151 \t 100\t36 0.48397591710090637 \t 100\t37 0.4719199240207672 \t 100\t38 0.49473750591278076 \t 100\t39 0.4934055805206299 \t 100\t40 0.4914543032646179 \t 100\t41 0.4856790006160736 \t 100\t42 0.4830557107925415 \t 100\t43 0.47719061374664307 \t 100\t44 0.5005353093147278 \t 100\t45 0.4865838289260864 \t 100\t46 0.48959776759147644 \t 100\t47 0.4945559501647949 \t 100\t48 0.4858781397342682 \t 100\t49 0.4907591938972473 \t 100\t50 0.48058629035949707 \t 100\t51 0.49044427275657654 \t 100\t52 0.47405293583869934 \t 100\t53 0.4955354928970337 \t 100\t54 0.4693714380264282 \t 100\t55 0.49100762605667114 \t 100\t56 0.4916466772556305 \t 100\t57 0.4666234254837036 \t 100\t58 0.4801645576953888 \t 100\t59 0.47594505548477173 \t 100\t60 0.48841527104377747 \t 100\t61 0.48412802815437317 \t 100\t62 0.48516279458999634 \t 100\t63 0.4764009714126587 \t 100\t64 0.4803406298160553 \t 100\t65 0.48014000058174133 \t 100\t66 0.48149141669273376 \t 100\t67 0.4782984256744385 \t 100\t68 0.48618730902671814 \t 100\t69 0.4905708134174347 \t 100\t70 0.4774641692638397 \t 100\t71 0.49900126457214355 \t 100\t72 0.4843981862068176 \t 100\t73 0.4832099974155426 \t 100\t74 0.4904571771621704 \t 100\t75 0.4780483543872833 \t 100\t76 0.48221299052238464 \t 100\t77 0.4832228422164917 \t 100\t78 0.4774859845638275 \t 100\t79 0.49206602573394775 \t 100\t80 0.47258076071739197 \t 100\t81 0.4973559081554413 \t 100\t82 0.46928271651268005 \t 100\t83 0.4771725535392761 \t 100\t84 0.4964715838432312 \t 100\t85 0.4848010241985321 \t 100\t86 0.48330897092819214 \t 100\t87 0.4928881824016571 \t 100\t88 0.49070850014686584 \t 100\t89 0.48563334345817566 \t 100\t90 0.47576263546943665 \t 100\t91 0.49320244789123535 \t 100\t92 0.4935210049152374 \t 100\t93 0.5000730752944946 \t 100\t94 0.4843043088912964 \t 100\t95 0.4814314842224121 \t 100\t96 0.4671115577220917 \t 100\t97 0.48880141973495483 \t 100\t98 0.504584789276123 \t 100\t99 0.47127479314804077 \t 100\t\n",
      "===================================================\n",
      "100000\n",
      "0.4461917473495542 \t 100\n",
      "0 0.3644181787967682 \t 100\t1 0.3600011467933655 \t 100\t2 0.36531683802604675 \t 100\t3 0.3611809015274048 \t 100\t4 0.35539165139198303 \t 100\t5 0.3626059293746948 \t 100\t6 0.3639710247516632 \t 100\t7 0.3673926293849945 \t 100\t8 0.36482352018356323 \t 100\t9 0.36626967787742615 \t 100\t10 0.3660123944282532 \t 100\t11 0.36234477162361145 \t 100\t12 0.36196598410606384 \t 100\t13 0.36351606249809265 \t 100\t14 0.3625117540359497 \t 100\t15 0.367462158203125 \t 100\t16 0.36061468720436096 \t 100\t17 0.3658145070075989 \t 100\t18 0.35718896985054016 \t 100\t19 0.3653941750526428 \t 100\t20 0.3631216287612915 \t 100\t21 0.35754886269569397 \t 100\t22 0.3702569901943207 \t 100\t23 0.3559291660785675 \t 100\t24 0.35992416739463806 \t 100\t25 0.3640587031841278 \t 100\t26 0.3585160970687866 \t 100\t27 0.35852712392807007 \t 100\t28 0.3615987002849579 \t 100\t29 0.36349576711654663 \t 100\t30 0.3675290644168854 \t 100\t31 0.3647509813308716 \t 100\t32 0.3668815493583679 \t 100\t33 0.3598543703556061 \t 100\t34 0.36291202902793884 \t 100\t35 0.3574603199958801 \t 100\t36 0.3625105321407318 \t 100\t37 0.3504262864589691 \t 100\t38 0.3600781261920929 \t 100\t39 0.3590574562549591 \t 100\t40 0.36143726110458374 \t 100\t41 0.36485999822616577 \t 100\t42 0.36986956000328064 \t 100\t43 0.36275991797447205 \t 100\t44 0.36449819803237915 \t 100\t45 0.3628026247024536 \t 100\t46 0.36446449160575867 \t 100\t47 0.3599141538143158 \t 100\t48 0.3630525767803192 \t 100\t49 0.3564189374446869 \t 100\t50 0.36289188265800476 \t 100\t51 0.3575593829154968 \t 100\t52 0.36872613430023193 \t 100\t53 0.3595823049545288 \t 100\t54 0.35969406366348267 \t 100\t55 0.3594602644443512 \t 100\t56 0.361625611782074 \t 100\t57 0.3630446195602417 \t 100\t58 0.35665661096572876 \t 100\t59 0.3690457344055176 \t 100\t60 0.36171695590019226 \t 100\t61 0.36678609251976013 \t 100\t62 0.36159032583236694 \t 100\t63 0.3611771762371063 \t 100\t64 0.36484023928642273 \t 100\t65 0.3687971532344818 \t 100\t66 0.3608786463737488 \t 100\t67 0.35569772124290466 \t 100\t68 0.3604181110858917 \t 100\t69 0.36261385679244995 \t 100\t70 0.36830851435661316 \t 100\t71 0.3660088777542114 \t 100\t72 0.3636425733566284 \t 100\t73 0.3612821102142334 \t 100\t74 0.36187684535980225 \t 100\t75 0.36253222823143005 \t 100\t76 0.36121100187301636 \t 100\t77 0.3623436391353607 \t 100\t78 0.3580155372619629 \t 100\t79 0.35861554741859436 \t 100\t80 0.3592739701271057 \t 100\t81 0.3653985857963562 \t 100\t82 0.3613329827785492 \t 100\t83 0.3593426048755646 \t 100\t84 0.3641994893550873 \t 100\t85 0.3643311858177185 \t 100\t86 0.3621704578399658 \t 100\t87 0.3676147758960724 \t 100\t88 0.35715922713279724 \t 100\t89 0.3644525408744812 \t 100\t90 0.36072787642478943 \t 100\t91 0.36034703254699707 \t 100\t92 0.35830315947532654 \t 100\t93 0.35862281918525696 \t 100\t94 0.3575303256511688 \t 100\t95 0.35860905051231384 \t 100\t96 0.3620847761631012 \t 100\t97 0.36166226863861084 \t 100\t98 0.3720736801624298 \t 100\t99 0.363272100687027 \t 100\t\n",
      "===================================================\n",
      "1000000\n",
      "0.4335367039648644 \t 100\n",
      "0 0.32004302740097046 \t 100\t1 0.31597769260406494 \t 100\t2 0.317335307598114 \t 100\t3 0.31744420528411865 \t 100\t4 0.32058095932006836 \t 100\t5 0.3210573196411133 \t 100\t6 0.3171750009059906 \t 100\t7 0.32018938660621643 \t 100\t8 0.3160645663738251 \t 100\t"
     ]
    }
   ],
   "source": [
    "for N in Ns:\n",
    "    print('===================================================\\n{}'.format(N))\n",
    "    # Take the first N samples.\n",
    "    data, m, s = split_data(X[:N], y[:N])\n",
    "    \n",
    "    # Train BDT model (only need to train 1)\n",
    "    bdt_model = XGBClassifier(early_stopping_rounds = 10)\n",
    "    X_trn, X_vld, y_trn, y_vld = data\n",
    "    bdt_model.fit(X_trn, y_trn, eval_set = [(X_vld, y_vld)], verbose = 0)\n",
    "    trace = bdt_model.evals_result()['validation_0']\n",
    "    print(trace['logloss'][-1], '\\t', len(trace['logloss']), end = '\\n')\n",
    "    bdt_model.save_model(bdt_filestr.format(N))\n",
    "\n",
    "    for i in range(reps):\n",
    "        print(i, end = ' ')\n",
    "        # Train BCE model\n",
    "        bce_model, trace = train(data, **bce_params)\n",
    "        bce_model.save_weights(bce_filestr.format(N, i))\n",
    "        \n",
    "        # Train GBC model\n",
    "        gbc_model = GradientBoostingClassifier(validation_fraction = 0.25,\n",
    "                                               n_iter_no_change = 10)\n",
    "        gbc_model.fit(X[:N], y[:N])\n",
    "        dump(gbc_model, gbc_filestr.format(N, i))\n",
    "        print()\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multifold",
   "language": "python",
   "name": "multifold"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
