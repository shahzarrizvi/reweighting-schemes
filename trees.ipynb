{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7ced574-924b-4fd3-9444-614365da7fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically reload modules after executing each cell.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d2777dc-b825-4800-bc82-d1b23953c263",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# General imports\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from joblib import dump, load \n",
    "\n",
    "# Utility imports\n",
    "from utils.losses import *\n",
    "from utils.plotting import *\n",
    "from utils.training import *\n",
    "\n",
    "np.random.seed(666) # Need to do more to ensure data is the same across runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18c301cf-f760-4479-aeef-27c5a063035f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # pick a number < 4 on ML4HEP; < 3 on Voltan \n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eff551-de89-49c5-a2c0-a1c15ae11183",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Vertical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d526a150-2df1-4284-b6da-d1b4959d5352",
   "metadata": {
    "tags": []
   },
   "source": [
    "## $d = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cae75b-8b8b-44bb-a06d-78c677817809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "num = 0\n",
    "reps = 100\n",
    "d = 1\n",
    "Ns = 10**np.arange(2, 8)\n",
    "\n",
    "# Model parameters\n",
    "bce_params = {'loss':bce, 'd': d}\n",
    "\n",
    "filestr = 'models/trees/{}/set_{}/'.format(d, num)\n",
    "bce_filestr = filestr + 'bce/model_{}_{}.h5'\n",
    "bdt_filestr = filestr + 'bdt/model_{}.h5'\n",
    "gbc_filestr = filestr + 'gbc/model_{}_{}.h5'\n",
    "\n",
    "if not os.path.isdir(filestr):\n",
    "    os.mkdir(filestr)\n",
    "\n",
    "if not os.path.isdir(filestr + 'bce/'):\n",
    "    os.mkdir(filestr + 'bce/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'bdt/'):\n",
    "    os.mkdir(filestr + 'bdt/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'gbc/'):\n",
    "    os.mkdir(filestr + 'gbc/')\n",
    "\n",
    "# Data parameters\n",
    "X = np.load('data/trees/{}/X_trn.npy'.format(d)).reshape(-1, 1)\n",
    "y = np.load('data/trees/{}/y_trn.npy'.format(d)).astype('float32')\n",
    "\n",
    "bkgd = stats.norm(-0.1, 1)\n",
    "sgnl = stats.norm(+0.1, 1)\n",
    "\n",
    "lr = make_lr(bkgd, sgnl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c89e58-6e56-4b84-9385-049327bed049",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10**6\n",
    "data, m, s = split_data(X[:N], y[:N])\n",
    "\n",
    "bce_lrs = [None] * reps\n",
    "gbc_lrs = [None] * reps\n",
    "for i in range(reps):\n",
    "    print(i, end = ' ')\n",
    "    bce_model = create_model(**bce_params)\n",
    "    bce_model.load_weights(bce_filestr.format(N, i))\n",
    "    bce_lrs[i] = odds_lr(bce_model, m, s)\n",
    "\n",
    "    gbc_model = load(gbc_filestr.format(N, i))\n",
    "    gbc_lrs[i] = tree_lr(gbc_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5856c227-271c-440f-95d0-3e93becf92e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.linspace(-6, 6, 1201).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ca7c19-d2bc-4bc6-b872-8a44b142272f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bce_preds = get_preds(bce_lrs, xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19a6431-0b68-4d35-a8c3-8c37c3ea4db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc_preds = get_preds(gbc_lrs, xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b130a1cc-e58a-4743-9016-e06387cf46bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_bce = bce_preds.mean(axis = 0)\n",
    "avg_gbc = gbc_preds.mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09e0a31-4089-402d-ab48-f62fb0d35123",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_plot([bce_preds, gbc_preds], ['BCE', 'GBC'], lr, xs.reshape(-1), \n",
    "           figsize = (w, h), title = '\\it Likelihood Ratio Models', \n",
    "           filename = 'plots/lr_models.png') "
   ]
  },
  {
   "cell_type": "raw",
   "id": "c2945cb9-735d-4029-a433-32efe7aa5f8b",
   "metadata": {},
   "source": [
    "for N in Ns:\n",
    "    print('===================================================\\n{}'.format(N))\n",
    "    # Take the first N samples.\n",
    "    data, m, s = split_data(X[:N], y[:N])\n",
    "    \n",
    "    # Train BDT model (only need to train 1)\n",
    "    #bdt_model = XGBClassifier(early_stopping_rounds = 10)\n",
    "    #X_trn, X_vld, y_trn, y_vld = data\n",
    "    #bdt_model.fit(X_trn, y_trn, eval_set = [(X_vld, y_vld)], verbose = 0)\n",
    "    #trace = bdt_model.evals_result()['validation_0']\n",
    "    #print(trace['logloss'][-1], '\\t', len(trace['logloss']), end = '\\n')\n",
    "    #bdt_model.save_model(bdt_filestr.format(N))\n",
    "\n",
    "    for i in range(reps):\n",
    "        print(i, end = ' ')\n",
    "        # Train BCE model\n",
    "        #bce_model, trace = train(data, **bce_params)\n",
    "        #bce_model.save_weights(bce_filestr.format(N, i))\n",
    "        \n",
    "        # Train GBC model\n",
    "        gbc_model = GradientBoostingClassifier(validation_fraction = 0.25,\n",
    "                                               n_iter_no_change = 10)\n",
    "        gbc_model.fit(X[:N], y[:N])\n",
    "        dump(gbc_model, gbc_filestr.format(N, i))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78d6730-4b33-40bd-8abb-acf1be1d2f30",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## $d = 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12422843-2cf2-4280-b1b4-aae3127e5cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "num = 0\n",
    "reps = 100\n",
    "d = 2\n",
    "Ns = 10**np.arange(2, 8)\n",
    "\n",
    "# Model parameters\n",
    "bce_params = {'loss':bce, 'd': d}\n",
    "\n",
    "filestr = 'models/trees/{}/set_{}/'.format(d, num)\n",
    "bce_filestr = filestr + 'bce/model_{}_{}.h5'\n",
    "bdt_filestr = filestr + 'bdt/model_{}.h5'\n",
    "gbc_filestr = filestr + 'gbc/model_{}_{}.h5'\n",
    "\n",
    "if not os.path.isdir(filestr):\n",
    "    os.mkdir(filestr)\n",
    "\n",
    "if not os.path.isdir(filestr + 'bce/'):\n",
    "    os.mkdir(filestr + 'bce/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'bdt/'):\n",
    "    os.mkdir(filestr + 'bdt/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'gbc/'):\n",
    "    os.mkdir(filestr + 'gbc/')\n",
    "\n",
    "# Data parameters\n",
    "X = np.load('data/trees/{}/X_trn.npy'.format(d))\n",
    "y = np.load('data/trees/{}/y_trn.npy'.format(d)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1a1f4c-7d62-4617-9eaf-1c2ab3f56bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for N in Ns:\n",
    "    print('===================================================\\n{}'.format(N))\n",
    "    # Take the first N samples.\n",
    "    data, m, s = split_data(X[:N], y[:N])\n",
    "    \n",
    "    # Train BDT model (only need to train 1)\n",
    "    #bdt_model = XGBClassifier(early_stopping_rounds = 10)\n",
    "    #X_trn, X_vld, y_trn, y_vld = data\n",
    "    #bdt_model.fit(X_trn, y_trn, eval_set = [(X_vld, y_vld)], verbose = 0)\n",
    "    #trace = bdt_model.evals_result()['validation_0']\n",
    "    #print(trace['logloss'][-1], '\\t', len(trace['logloss']), end = '\\n')\n",
    "    #bdt_model.save_model(bdt_filestr.format(N))\n",
    "\n",
    "    for i in range(reps):\n",
    "        print(i, end = ' ')\n",
    "        # Train BCE model\n",
    "        #bce_model, trace = train(data, **bce_params)\n",
    "        #bce_model.save_weights(bce_filestr.format(N, i))\n",
    "        \n",
    "        # Train GBC model\n",
    "        gbc_model = GradientBoostingClassifier(validation_fraction = 0.25,\n",
    "                                               n_iter_no_change = 10)\n",
    "        gbc_model.fit(X[:N], y[:N])\n",
    "        dump(gbc_model, gbc_filestr.format(N, i))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d942469-7ef3-4907-87eb-c9296e3409bb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## $d=4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9f85bd-b60b-473c-bcc8-373a9d120f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "num = 0\n",
    "reps = 100\n",
    "d = 4\n",
    "Ns = 10**np.arange(2, 8)\n",
    "\n",
    "# Model parameters\n",
    "bce_params = {'loss':bce, 'd': d}\n",
    "\n",
    "filestr = 'models/trees/{}/set_{}/'.format(d, num)\n",
    "bce_filestr = filestr + 'bce/model_{}_{}.h5'\n",
    "bdt_filestr = filestr + 'bdt/model_{}.h5'\n",
    "gbc_filestr = filestr + 'gbc/model_{}_{}.h5'\n",
    "\n",
    "if not os.path.isdir(filestr):\n",
    "    os.mkdir(filestr)\n",
    "\n",
    "if not os.path.isdir(filestr + 'bce/'):\n",
    "    os.mkdir(filestr + 'bce/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'bdt/'):\n",
    "    os.mkdir(filestr + 'bdt/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'gbc/'):\n",
    "    os.mkdir(filestr + 'gbc/')\n",
    "\n",
    "# Data parameters\n",
    "X = np.load('data/trees/{}/X_trn.npy'.format(d))\n",
    "y = np.load('data/trees/{}/y_trn.npy'.format(d)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb3e63d-4966-43eb-af89-f6ae1d3370eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for N in Ns:\n",
    "    print('===================================================\\n{}'.format(N))\n",
    "    # Take the first N samples.\n",
    "    data, m, s = split_data(X[:N], y[:N])\n",
    "    \n",
    "    # Train BDT model (only need to train 1)\n",
    "    #bdt_model = XGBClassifier(early_stopping_rounds = 10)\n",
    "    #X_trn, X_vld, y_trn, y_vld = data\n",
    "    #bdt_model.fit(X_trn, y_trn, eval_set = [(X_vld, y_vld)], verbose = 0)\n",
    "    #trace = bdt_model.evals_result()['validation_0']\n",
    "    #print(trace['logloss'][-1], '\\t', len(trace['logloss']), end = '\\n')\n",
    "    #bdt_model.save_model(bdt_filestr.format(N))\n",
    "\n",
    "    for i in range(91, reps):\n",
    "        print(i, end = ' ')\n",
    "        # Train BCE model\n",
    "        #bce_model, trace = train(data, **bce_params)\n",
    "        #bce_model.save_weights(bce_filestr.format(N, i))\n",
    "        \n",
    "        # Train GBC model\n",
    "        gbc_model = GradientBoostingClassifier(validation_fraction = 0.25,\n",
    "                                               n_iter_no_change = 10)\n",
    "        gbc_model.fit(X[:N], y[:N])\n",
    "        dump(gbc_model, gbc_filestr.format(N, i))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1b039a-fa91-4a35-8233-c4a017babc44",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## $d=8$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe81baf6-0352-4b70-bcc8-ff37baeac13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "num = 0\n",
    "reps = 100\n",
    "d = 8\n",
    "Ns = 10**np.arange(2, 8)\n",
    "\n",
    "# Model parameters\n",
    "bce_params = {'loss':bce, 'd': d}\n",
    "\n",
    "filestr = 'models/trees/{}/set_{}/'.format(d, num)\n",
    "bce_filestr = filestr + 'bce/model_{}_{}.h5'\n",
    "bdt_filestr = filestr + 'bdt/model_{}.h5'\n",
    "gbc_filestr = filestr + 'gbc/model_{}_{}.h5'\n",
    "\n",
    "if not os.path.isdir(filestr):\n",
    "    os.mkdir(filestr)\n",
    "\n",
    "if not os.path.isdir(filestr + 'bce/'):\n",
    "    os.mkdir(filestr + 'bce/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'bdt/'):\n",
    "    os.mkdir(filestr + 'bdt/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'gbc/'):\n",
    "    os.mkdir(filestr + 'gbc/')\n",
    "\n",
    "# Data parameters\n",
    "X = np.load('data/trees/{}/X_trn.npy'.format(d))\n",
    "y = np.load('data/trees/{}/y_trn.npy'.format(d)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e72def4-b63c-46e2-92d5-1d724b5d079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for N in Ns:\n",
    "    print('===================================================\\n{}'.format(N))\n",
    "    # Take the first N samples.\n",
    "    data, m, s = split_data(X[:N], y[:N])\n",
    "    \n",
    "    # Train BDT model (only need to train 1)\n",
    "    #bdt_model = XGBClassifier(early_stopping_rounds = 10)\n",
    "    #X_trn, X_vld, y_trn, y_vld = data\n",
    "    #bdt_model.fit(X_trn, y_trn, eval_set = [(X_vld, y_vld)], verbose = 0)\n",
    "    #trace = bdt_model.evals_result()['validation_0']\n",
    "    #print(trace['logloss'][-1], '\\t', len(trace['logloss']), end = '\\n')\n",
    "    #bdt_model.save_model(bdt_filestr.format(N))\n",
    "\n",
    "    for i in range(reps):\n",
    "        print(i, end = ' ')\n",
    "        # Train BCE model\n",
    "        #bce_model, trace = train(data, **bce_params)\n",
    "        #bce_model.save_weights(bce_filestr.format(N, i))\n",
    "        \n",
    "        # Train GBC model\n",
    "        gbc_model = GradientBoostingClassifier(validation_fraction = 0.25,\n",
    "                                               n_iter_no_change = 10)\n",
    "        gbc_model.fit(X[:N], y[:N])\n",
    "        dump(gbc_model, gbc_filestr.format(N, i))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7b33f6-be09-404b-bbca-7e567a651b1b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## $d=16$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed78be7a-1c20-4cde-bd24-d5a46012377e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "num = 0\n",
    "reps = 100\n",
    "d = 16\n",
    "Ns = 10**np.arange(2, 8)\n",
    "\n",
    "# Model parameters\n",
    "bce_params = {'loss':bce, 'd': d}\n",
    "\n",
    "filestr = 'models/trees/{}/set_{}/'.format(d, num)\n",
    "bce_filestr = filestr + 'bce/model_{}_{}.h5'\n",
    "bdt_filestr = filestr + 'bdt/model_{}.h5'\n",
    "gbc_filestr = filestr + 'gbc/model_{}_{}.h5'\n",
    "\n",
    "if not os.path.isdir(filestr):\n",
    "    os.mkdir(filestr)\n",
    "\n",
    "if not os.path.isdir(filestr + 'bce/'):\n",
    "    os.mkdir(filestr + 'bce/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'bdt/'):\n",
    "    os.mkdir(filestr + 'bdt/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'gbc/'):\n",
    "    os.mkdir(filestr + 'gbc/')\n",
    "\n",
    "# Data parameters\n",
    "X = np.load('data/trees/{}/X_trn.npy'.format(d))\n",
    "y = np.load('data/trees/{}/y_trn.npy'.format(d)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5262859-deb7-413f-b674-9c84a157ad73",
   "metadata": {},
   "outputs": [],
   "source": [
    "for N in Ns:\n",
    "    print('===================================================\\n{}'.format(N))\n",
    "    # Take the first N samples.\n",
    "    data, m, s = split_data(X[:N], y[:N])\n",
    "    \n",
    "    # Train BDT model (only need to train 1)\n",
    "    #bdt_model = XGBClassifier(early_stopping_rounds = 10)\n",
    "    #X_trn, X_vld, y_trn, y_vld = data\n",
    "    #bdt_model.fit(X_trn, y_trn, eval_set = [(X_vld, y_vld)], verbose = 0)\n",
    "    #trace = bdt_model.evals_result()['validation_0']\n",
    "    #print(trace['logloss'][-1], '\\t', len(trace['logloss']), end = '\\n')\n",
    "    #bdt_model.save_model(bdt_filestr.format(N))\n",
    "\n",
    "    for i in range(reps):\n",
    "        print(i, end = ' ')\n",
    "        # Train BCE model\n",
    "        #bce_model, trace = train(data, **bce_params)\n",
    "        #bce_model.save_weights(bce_filestr.format(N, i))\n",
    "        \n",
    "        # Train GBC model\n",
    "        gbc_model = GradientBoostingClassifier(validation_fraction = 0.25,\n",
    "                                               n_iter_no_change = 10)\n",
    "        gbc_model.fit(X[:N], y[:N])\n",
    "        dump(gbc_model, gbc_filestr.format(N, i))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ca2309-9077-432f-933c-77be9ede35e1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## $d = 32$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f08136-c5dc-4649-989b-4f4cea166cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "num = 0\n",
    "reps = 100\n",
    "d = 32\n",
    "Ns = 10**np.arange(2, 8)\n",
    "\n",
    "# Model parameters\n",
    "bce_params = {'loss':bce, 'd': d}\n",
    "\n",
    "filestr = 'models/trees/{}/set_{}/'.format(d, num)\n",
    "bce_filestr = filestr + 'bce/model_{}_{}.h5'\n",
    "bdt_filestr = filestr + 'bdt/model_{}.h5'\n",
    "gbc_filestr = filestr + 'gbc/model_{}_{}.h5'\n",
    "\n",
    "if not os.path.isdir(filestr):\n",
    "    os.mkdir(filestr)\n",
    "\n",
    "if not os.path.isdir(filestr + 'bce/'):\n",
    "    os.mkdir(filestr + 'bce/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'bdt/'):\n",
    "    os.mkdir(filestr + 'bdt/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'gbc/'):\n",
    "    os.mkdir(filestr + 'gbc/')\n",
    "\n",
    "# Data parameters\n",
    "X = np.load('data/trees/{}/X_trn.npy'.format(d))\n",
    "y = np.load('data/trees/{}/y_trn.npy'.format(d)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f9e44b-a9fb-49c9-923d-9c414e2c6a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "for N in Ns:\n",
    "    print('===================================================\\n{}'.format(N))\n",
    "    # Take the first N samples.\n",
    "    data, m, s = split_data(X[:N], y[:N])\n",
    "    \n",
    "    # Train BDT model (only need to train 1)\n",
    "    #bdt_model = XGBClassifier(early_stopping_rounds = 10)\n",
    "    #X_trn, X_vld, y_trn, y_vld = data\n",
    "    #bdt_model.fit(X_trn, y_trn, eval_set = [(X_vld, y_vld)], verbose = 0)\n",
    "    #trace = bdt_model.evals_result()['validation_0']\n",
    "    #print(trace['logloss'][-1], '\\t', len(trace['logloss']), end = '\\n')\n",
    "    #bdt_model.save_model(bdt_filestr.format(N))\n",
    "\n",
    "    for i in range(reps):\n",
    "        print(i, end = ' ')\n",
    "        # Train BCE model\n",
    "        #bce_model, trace = train(data, **bce_params)\n",
    "        #bce_model.save_weights(bce_filestr.format(N, i))\n",
    "        \n",
    "        # Train GBC model\n",
    "        gbc_model = GradientBoostingClassifier(validation_fraction = 0.25,\n",
    "                                               n_iter_no_change = 10)\n",
    "        gbc_model.fit(X[:N], y[:N])\n",
    "        dump(gbc_model, gbc_filestr.format(N, i))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ed990a-9a86-46df-a2b5-45d88cdb0e88",
   "metadata": {},
   "source": [
    "# Zenodo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c7beb4-a137-4bfc-a7a5-5cbc00b6b5ee",
   "metadata": {},
   "source": [
    "## $d = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "982a7583-5b0d-48b8-b4f1-c37fda6ba098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "num = 0\n",
    "reps = 100\n",
    "d = 1\n",
    "Ns = 10**np.arange(2, 8)\n",
    "\n",
    "# Model parameters\n",
    "bce_params = {'loss':bce, 'd': d}\n",
    "\n",
    "filestr = 'models/zenodo/{}/set_{}/'.format(d, num)\n",
    "bce_filestr = filestr + 'bce/model_{}_{}.h5'\n",
    "bdt_filestr = filestr + 'bdt/model_{}.h5'\n",
    "gbc_filestr = filestr + 'gbc/model_{}_{}.h5'\n",
    "\n",
    "if not os.path.isdir(filestr):\n",
    "    os.mkdir(filestr)\n",
    "\n",
    "if not os.path.isdir(filestr + 'bce/'):\n",
    "    os.mkdir(filestr + 'bce/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'bdt/'):\n",
    "    os.mkdir(filestr + 'bdt/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'gbc/'):\n",
    "    os.mkdir(filestr + 'gbc/')\n",
    "\n",
    "# Data parameters\n",
    "X = np.load('data/zenodo/{}/X_trn.npy'.format(d)).reshape(-1, 1)\n",
    "y = np.load('data/zenodo/{}/y_trn.npy'.format(d)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "faa8fdd1-bf3c-49dd-9e78-a2051e91857e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================\n",
      "10000000\n",
      "0.6910528750873685 \t 45\n",
      "87 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-10 19:32:06.351600: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-10 19:32:07.004419: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22243 MB memory:  -> device: 0, name: Quadro RTX 6000, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6913653612136841 \t 100\t\n",
      "88 0.6912950277328491 \t 100\t\n",
      "89 0.6913024187088013 \t 100\t\n",
      "90 0.6912943720817566 \t 100\t\n",
      "91 0.6913145780563354 \t 100\t\n",
      "92 0.691287100315094 \t 100\t\n",
      "93 0.6912788152694702 \t 100\t\n",
      "94 0.6912984848022461 \t 100\t\n",
      "95 0.6912990808486938 \t 100\t\n",
      "96 0.6913532018661499 \t 100\t\n",
      "97 0.6912767887115479 \t 100\t\n",
      "98 0.6912504434585571 \t 100\t\n",
      "99 0.6913511753082275 \t 100\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Ns = [10**7]\n",
    "for N in Ns:\n",
    "    print('===================================================\\n{}'.format(N))\n",
    "    # Take the first N samples.\n",
    "    data, m, s = split_data(X[:N], y[:N])\n",
    "    \n",
    "    # Train BDT model (only need to train 1)\n",
    "    bdt_model = XGBClassifier(early_stopping_rounds = 10)\n",
    "    X_trn, X_vld, y_trn, y_vld = data\n",
    "    bdt_model.fit(X_trn, y_trn, eval_set = [(X_vld, y_vld)], verbose = 0)\n",
    "    trace = bdt_model.evals_result()['validation_0']\n",
    "    print(trace['logloss'][-1], '\\t', len(trace['logloss']), end = '\\n')\n",
    "    bdt_model.save_model(bdt_filestr.format(N))\n",
    "\n",
    "    for i in range(87, reps):\n",
    "        print(i, end = ' ')\n",
    "        # Train BCE model\n",
    "        bce_model, trace = train(data, **bce_params)\n",
    "        bce_model.save_weights(bce_filestr.format(N, i))\n",
    "        \n",
    "        # Train GBC model\n",
    "        gbc_model = GradientBoostingClassifier(validation_fraction = 0.25,\n",
    "                                               n_iter_no_change = 10)\n",
    "        gbc_model.fit(X[:N], y[:N])\n",
    "        dump(gbc_model, gbc_filestr.format(N, i))\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf622c3-cfce-4a54-b8d5-2dca4a76c167",
   "metadata": {
    "tags": []
   },
   "source": [
    "## $d = 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07e462f-21a5-4517-a390-57d7e4e498fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "num = 0\n",
    "reps = 100\n",
    "d = 2\n",
    "Ns = 10**np.arange(2, 8)\n",
    "\n",
    "# Model parameters\n",
    "bce_params = {'loss':bce, 'd': d}\n",
    "\n",
    "filestr = 'models/zenodo/{}/set_{}/'.format(d, num)\n",
    "bce_filestr = filestr + 'bce/model_{}_{}.h5'\n",
    "bdt_filestr = filestr + 'bdt/model_{}.h5'\n",
    "gbc_filestr = filestr + 'gbc/model_{}_{}.h5'\n",
    "\n",
    "if not os.path.isdir(filestr):\n",
    "    os.mkdir(filestr)\n",
    "\n",
    "if not os.path.isdir(filestr + 'bce/'):\n",
    "    os.mkdir(filestr + 'bce/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'bdt/'):\n",
    "    os.mkdir(filestr + 'bdt/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'gbc/'):\n",
    "    os.mkdir(filestr + 'gbc/')\n",
    "\n",
    "# Data parameters\n",
    "X = np.load('data/zenodo/{}/X_trn.npy'.format(d))\n",
    "y = np.load('data/zenodo/{}/y_trn.npy'.format(d)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201c1e2a-1e85-4736-b009-fcac789b5d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "for N in Ns:\n",
    "    print('===================================================\\n{}'.format(N))\n",
    "    # Take the first N samples.\n",
    "    data, m, s = split_data(X[:N], y[:N])\n",
    "    \n",
    "    # Train BDT model (only need to train 1)\n",
    "    bdt_model = XGBClassifier(early_stopping_rounds = 10)\n",
    "    X_trn, X_vld, y_trn, y_vld = data\n",
    "    bdt_model.fit(X_trn, y_trn, eval_set = [(X_vld, y_vld)], verbose = 0)\n",
    "    trace = bdt_model.evals_result()['validation_0']\n",
    "    print(trace['logloss'][-1], '\\t', len(trace['logloss']), end = '\\n')\n",
    "    bdt_model.save_model(bdt_filestr.format(N))\n",
    "\n",
    "    for i in range(reps):\n",
    "        print(i, end = ' ')\n",
    "        # Train BCE model\n",
    "        bce_model, trace = train(data, **bce_params)\n",
    "        bce_model.save_weights(bce_filestr.format(N, i))\n",
    "        \n",
    "        # Train GBC model\n",
    "        gbc_model = GradientBoostingClassifier(validation_fraction = 0.25,\n",
    "                                               n_iter_no_change = 10)\n",
    "        gbc_model.fit(X[:N], y[:N])\n",
    "        dump(gbc_model, gbc_filestr.format(N, i))\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c198e6-0986-41aa-a9a7-d28f318d178d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## $d = 4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c9764d4-4e06-4426-b662-c1551a4c8747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "num = 0\n",
    "reps = 100\n",
    "d = 4\n",
    "Ns = 10**np.arange(2, 8)\n",
    "\n",
    "# Model parameters\n",
    "bce_params = {'loss':bce, 'd': d}\n",
    "\n",
    "filestr = 'models/zenodo/{}/set_{}/'.format(d, num)\n",
    "bce_filestr = filestr + 'bce/model_{}_{}.h5'\n",
    "bdt_filestr = filestr + 'bdt/model_{}.h5'\n",
    "gbc_filestr = filestr + 'gbc/model_{}_{}.h5'\n",
    "\n",
    "if not os.path.isdir(filestr):\n",
    "    os.mkdir(filestr)\n",
    "\n",
    "if not os.path.isdir(filestr + 'bce/'):\n",
    "    os.mkdir(filestr + 'bce/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'bdt/'):\n",
    "    os.mkdir(filestr + 'bdt/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'gbc/'):\n",
    "    os.mkdir(filestr + 'gbc/')\n",
    "\n",
    "# Data parameters\n",
    "X = np.load('data/zenodo/{}/X_trn.npy'.format(d))\n",
    "y = np.load('data/zenodo/{}/y_trn.npy'.format(d)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15382cf7-0ebb-41ab-bc6f-037c5c058056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================\n",
      "100\n",
      "1.0303720071911813 \t 11\n",
      "0 0.7152805924415588 \t 15\t\n",
      "1 0.7930371165275574 \t 21\t\n",
      "2 0.7494280338287354 \t 20\t\n",
      "3 0.7697917222976685 \t 20\t\n",
      "4 0.688759982585907 \t 17\t\n",
      "5 0.7402358055114746 \t 19\t\n",
      "6 0.7901978492736816 \t 21\t\n",
      "7 0.7297656536102295 \t 14\t\n",
      "8 0.7083227634429932 \t 18\t\n",
      "9 0.7640546560287476 \t 22\t\n",
      "10 0.7565174102783203 \t 22\t\n",
      "11 0.7596052289009094 \t 20\t\n",
      "12 0.6614600419998169 \t 14\t\n",
      "13 0.7304542064666748 \t 17\t\n",
      "14 0.722554624080658 \t 19\t\n",
      "15 0.7085067629814148 \t 20\t\n",
      "16 0.7608336806297302 \t 17\t\n",
      "17 0.7507644891738892 \t 21\t\n",
      "18 0.7018964886665344 \t 17\t\n",
      "19 0.7381808757781982 \t 17\t\n",
      "20 0.7348161339759827 \t 21\t\n",
      "21 0.7106039524078369 \t 18\t\n",
      "22 0.7307910323143005 \t 16\t\n",
      "23 0.7920247912406921 \t 22\t\n",
      "24 0.7406968474388123 \t 18\t\n",
      "25 0.7073522210121155 \t 22\t\n",
      "26 0.7665988802909851 \t 21\t\n",
      "27 0.7552981376647949 \t 21\t\n",
      "28 0.7021545171737671 \t 18\t\n",
      "29 0.719815731048584 \t 20\t\n",
      "30 0.7171556949615479 \t 19\t\n",
      "31 0.7294284701347351 \t 22\t\n",
      "32 0.7039250135421753 \t 21\t\n",
      "33 0.6961967945098877 \t 17\t\n",
      "34 0.7264349460601807 \t 20\t\n",
      "35 0.7684410214424133 \t 20\t\n",
      "36 0.7745237946510315 \t 22\t\n",
      "37 0.6816476583480835 \t 15\t\n",
      "38 0.7009669542312622 \t 15\t\n",
      "39 0.7383064031600952 \t 20\t\n",
      "40 0.7510724067687988 \t 26\t\n",
      "41 0.7335491180419922 \t 15\t\n",
      "42 0.751288890838623 \t 21\t\n",
      "43 0.8065552711486816 \t 20\t\n",
      "44 0.7021626830101013 \t 18\t\n",
      "45 0.7011334300041199 \t 17\t\n",
      "46 0.7373791337013245 \t 15\t\n",
      "47 0.7328140139579773 \t 21\t\n",
      "48 0.692608654499054 \t 19\t\n",
      "49 0.7204650044441223 \t 21\t\n",
      "50 0.7117851376533508 \t 18\t\n",
      "51 0.7272735834121704 \t 20\t\n",
      "52 0.7077503800392151 \t 16\t\n",
      "53 0.7408445477485657 \t 19\t\n",
      "54 0.7820680141448975 \t 22\t\n",
      "55 0.6957134008407593 \t 16\t\n",
      "56 0.7309109568595886 \t 21\t\n",
      "57 0.7345155477523804 \t 17\t\n",
      "58 0.6674621105194092 \t 16\t\n",
      "59 0.7325109839439392 \t 17\t\n",
      "60 0.810096263885498 \t 21\t\n",
      "61 0.6969505548477173 \t 16\t\n",
      "62 0.7533986568450928 \t 20\t\n",
      "63 0.7379110455513 \t 24\t\n",
      "64 0.7107946276664734 \t 18\t\n",
      "65 0.701907753944397 \t 13\t\n",
      "66 0.7390509247779846 \t 17\t\n",
      "67 0.693515419960022 \t 17\t\n",
      "68 0.716773271560669 \t 15\t\n",
      "69 0.7366254329681396 \t 17\t\n",
      "70 0.7591906785964966 \t 20\t\n",
      "71 0.7036805152893066 \t 19\t\n",
      "72 0.7186036705970764 \t 18\t\n",
      "73 0.6820605993270874 \t 18\t\n",
      "74 0.7396340370178223 \t 18\t\n",
      "75 0.7170779705047607 \t 16\t\n",
      "76 0.7243867516517639 \t 18\t\n",
      "77 0.748750627040863 \t 20\t\n",
      "78 0.7890963554382324 \t 27\t\n",
      "79 0.7856244444847107 \t 22\t\n",
      "80 0.7391909956932068 \t 17\t\n",
      "81 0.6787291169166565 \t 16\t\n",
      "82 0.7074825763702393 \t 16\t\n",
      "83 0.680866003036499 \t 16\t\n",
      "84 0.7155570387840271 \t 16\t\n",
      "85 0.7213323712348938 \t 17\t\n",
      "86 0.7150792479515076 \t 23\t\n",
      "87 0.6799059510231018 \t 18\t\n",
      "88 0.703216016292572 \t 17\t\n",
      "89 0.7233222723007202 \t 20\t\n",
      "90 0.6683319807052612 \t 15\t\n",
      "91 0.680989146232605 \t 15\t\n",
      "92 0.7083696722984314 \t 19\t\n",
      "93 0.7060077786445618 \t 14\t\n",
      "94 0.7592912316322327 \t 24\t\n",
      "95 0.7356387376785278 \t 21\t\n",
      "96 0.7020652890205383 \t 17\t\n",
      "97 0.792898416519165 \t 21\t\n",
      "98 0.7524667382240295 \t 18\t\n",
      "99 0.7229195237159729 \t 17\t\n",
      "\n",
      "===================================================\n",
      "1000\n",
      "0.756363850235939 \t 11\n",
      "0 0.7166744470596313 \t 11\t\n",
      "1 0.7127248048782349 \t 11\t\n",
      "2 0.7199257612228394 \t 11\t\n",
      "3 0.714540958404541 \t 11\t\n",
      "4 0.7149016857147217 \t 11\t\n",
      "5 0.7096689939498901 \t 11\t\n",
      "6 0.7216070890426636 \t 11\t\n",
      "7 0.7181931734085083 \t 11\t\n",
      "8 0.7129502892494202 \t 11\t\n",
      "9 0.7155858874320984 \t 11\t\n",
      "10 0.7180542349815369 \t 11\t\n",
      "11 0.7198800444602966 \t 11\t\n",
      "12 0.719441831111908 \t 11\t\n",
      "13 0.7226971387863159 \t 11\t\n",
      "14 0.7205359935760498 \t 11\t\n",
      "15 0.7198776006698608 \t 11\t\n",
      "16 0.7192020416259766 \t 11\t\n",
      "17 0.7140291929244995 \t 11\t\n",
      "18 0.7199349999427795 \t 11\t\n",
      "19 0.7190295457839966 \t 11\t\n",
      "20 0.715236246585846 \t 11\t\n",
      "21 0.7138714790344238 \t 11\t\n",
      "22 0.7160800695419312 \t 11\t\n",
      "23 0.7208529114723206 \t 11\t\n",
      "24 0.7181870341300964 \t 11\t\n",
      "25 0.7209402918815613 \t 11\t\n",
      "26 0.7180371284484863 \t 11\t\n",
      "27 0.7177309989929199 \t 11\t\n",
      "28 0.7144150733947754 \t 12\t\n",
      "29 0.7136163711547852 \t 11\t\n",
      "30 0.7242040634155273 \t 11\t\n",
      "31 0.7132611870765686 \t 11\t\n",
      "32 0.7173117399215698 \t 11\t\n",
      "33 0.719989538192749 \t 11\t\n",
      "34 0.719260036945343 \t 11\t\n",
      "35 0.7210038900375366 \t 11\t\n",
      "36 0.7174279093742371 \t 11\t\n",
      "37 0.7224467992782593 \t 11\t\n",
      "38 0.7210299968719482 \t 11\t\n",
      "39 0.714985191822052 \t 11\t\n",
      "40 0.7219000458717346 \t 11\t\n",
      "41 0.7139680981636047 \t 11\t\n",
      "42 0.7206177115440369 \t 11\t\n",
      "43 0.7138333320617676 \t 11\t\n",
      "44 0.7215364575386047 \t 11\t\n",
      "45 0.7228273153305054 \t 11\t\n",
      "46 0.7211605906486511 \t 11\t\n",
      "47 0.7208733558654785 \t 11\t\n",
      "48 0.7123931050300598 \t 11\t\n",
      "49 0.7141233086585999 \t 11\t\n",
      "50 0.7182576656341553 \t 11\t\n",
      "51 0.719505786895752 \t 11\t\n",
      "52 0.7125775218009949 \t 11\t\n",
      "53 0.7235890626907349 \t 11\t\n",
      "54 0.7212292551994324 \t 11\t\n",
      "55 0.7207608222961426 \t 11\t\n",
      "56 0.7081417441368103 \t 11\t\n",
      "57 0.7192239165306091 \t 11\t\n",
      "58 0.7202861309051514 \t 11\t\n",
      "59 0.7220319509506226 \t 11\t\n",
      "60 0.717517077922821 \t 11\t\n",
      "61 0.7213149666786194 \t 11\t\n",
      "62 0.7159662246704102 \t 11\t\n",
      "63 0.7225733399391174 \t 11\t\n",
      "64 0.7201120853424072 \t 11\t\n",
      "65 0.7121908068656921 \t 11\t\n",
      "66 0.7172051668167114 \t 11\t\n",
      "67 0.712906002998352 \t 11\t\n",
      "68 0.716809093952179 \t 11\t\n",
      "69 0.7183510065078735 \t 11\t\n",
      "70 0.7168142199516296 \t 11\t\n",
      "71 0.7161697149276733 \t 11\t\n",
      "72 0.7147840857505798 \t 11\t\n",
      "73 0.7190850973129272 \t 12\t\n",
      "74 0.718190610408783 \t 11\t\n",
      "75 0.7203537821769714 \t 11\t\n",
      "76 0.7095344066619873 \t 11\t\n",
      "77 0.7185862064361572 \t 11\t\n",
      "78 0.7142004370689392 \t 11\t\n",
      "79 0.7199901342391968 \t 11\t\n",
      "80 0.7155150175094604 \t 11\t\n",
      "81 0.7192923426628113 \t 11\t\n",
      "82 0.7171226739883423 \t 11\t\n",
      "83 0.7234264016151428 \t 11\t\n",
      "84 0.7166131138801575 \t 11\t\n",
      "85 0.7213954329490662 \t 11\t\n",
      "86 0.7175154089927673 \t 11\t\n",
      "87 0.7189134359359741 \t 11\t\n",
      "88 0.7152373194694519 \t 11\t\n",
      "89 0.7162336707115173 \t 11\t\n",
      "90 0.7138844132423401 \t 11\t\n",
      "91 0.7151899337768555 \t 12\t\n",
      "92 0.7154902219772339 \t 12\t\n",
      "93 0.7182517051696777 \t 11\t\n",
      "94 0.7166585326194763 \t 12\t\n",
      "95 0.7128669619560242 \t 11\t\n",
      "96 0.7154632210731506 \t 11\t\n",
      "97 0.7183153033256531 \t 11\t\n",
      "98 0.7126626968383789 \t 11\t\n",
      "99 0.7167407274246216 \t 11\t\n",
      "\n",
      "===================================================\n",
      "10000\n",
      "0.691845316144824 \t 12\n",
      "0 0.6878968477249146 \t 26\t\n",
      "1 0.6886090636253357 \t 16\t\n",
      "2 0.6875255703926086 \t 19\t\n",
      "3 0.6880006790161133 \t 24\t\n",
      "4 0.6876842975616455 \t 15\t\n",
      "5 0.6881545782089233 \t 26\t\n",
      "6 0.6887527108192444 \t 19\t\n",
      "7 0.6886850595474243 \t 15\t\n",
      "8 0.6882455348968506 \t 19\t\n",
      "9 0.6880081295967102 \t 15\t\n",
      "10 0.6885698437690735 \t 18\t\n",
      "11 0.6888014674186707 \t 20\t\n",
      "12 0.6885200142860413 \t 17\t\n",
      "13 0.6886568069458008 \t 26\t\n",
      "14 0.6886451840400696 \t 14\t\n",
      "15 0.6881320476531982 \t 25\t\n",
      "16 0.6887345910072327 \t 21\t\n",
      "17 0.6884352564811707 \t 24\t\n",
      "18 0.6876924633979797 \t 23\t\n",
      "19 0.6880607604980469 \t 15\t\n",
      "20 0.6892517805099487 \t 21\t\n",
      "21 0.689801812171936 \t 39\t\n",
      "22 0.6878059506416321 \t 15\t\n",
      "23 0.6882495284080505 \t 18\t\n",
      "24 0.6891164779663086 \t 22\t\n",
      "25 0.688332736492157 \t 28\t\n",
      "26 0.6882922053337097 \t 37\t\n",
      "27 0.6889641880989075 \t 28\t\n",
      "28 0.6892110705375671 \t 35\t\n",
      "29 0.688639760017395 \t 20\t\n",
      "30 0.6879351139068604 \t 22\t\n",
      "31 0.6881197094917297 \t 26\t\n",
      "32 0.6877297163009644 \t 24\t\n",
      "33 0.6880195140838623 \t 17\t\n",
      "34 0.6882275342941284 \t 16\t\n",
      "35 0.6886119246482849 \t 15\t\n",
      "36 0.6883501410484314 \t 28\t\n",
      "37 0.6883700489997864 \t 17\t\n",
      "38 0.6879516243934631 \t 27\t\n",
      "39 0.6885191798210144 \t 19\t\n",
      "40 0.6881618499755859 \t 15\t\n",
      "41 0.688947856426239 \t 15\t\n",
      "42 0.6880530118942261 \t 26\t\n",
      "43 0.6894339323043823 \t 27\t\n",
      "44 0.688439130783081 \t 18\t\n",
      "45 0.688469409942627 \t 21\t\n",
      "46 0.6874358654022217 \t 15\t\n",
      "47 0.6885507106781006 \t 17\t\n",
      "48 0.6877237558364868 \t 16\t\n",
      "49 0.687950074672699 \t 22\t\n",
      "50 0.6881510019302368 \t 20\t\n",
      "51 0.6880414485931396 \t 21\t\n",
      "52 0.689819872379303 \t 28\t\n",
      "53 0.6881412267684937 \t 31\t\n",
      "54 0.6881850361824036 \t 27\t\n",
      "55 0.688300609588623 \t 23\t\n",
      "56 0.6875279545783997 \t 30\t\n",
      "57 0.6882448792457581 \t 19\t\n",
      "58 0.6886829137802124 \t 15\t\n",
      "59 0.6889060735702515 \t 24\t\n",
      "60 0.68852698802948 \t 30\t\n",
      "61 0.6889203190803528 \t 15\t\n",
      "62 0.6883864998817444 \t 21\t\n",
      "63 0.6884980201721191 \t 20\t\n",
      "64 0.6878315210342407 \t 18\t\n",
      "65 0.688197135925293 \t 37\t\n",
      "66 0.689950704574585 \t 28\t\n",
      "67 0.6883050203323364 \t 31\t\n",
      "68 0.6884880661964417 \t 16\t\n",
      "69 0.6875596642494202 \t 25\t\n",
      "70 0.6876122951507568 \t 16\t\n",
      "71 0.6886908411979675 \t 28\t\n",
      "72 0.688791036605835 \t 19\t\n",
      "73 0.6877252459526062 \t 17\t\n",
      "74 0.6884282827377319 \t 30\t\n",
      "75 0.6878074407577515 \t 15\t\n",
      "76 0.6879379153251648 \t 15\t\n",
      "77 0.687788188457489 \t 27\t\n",
      "78 0.687752902507782 \t 31\t\n",
      "79 0.6882764101028442 \t 26\t\n",
      "80 0.6882169842720032 \t 23\t\n",
      "81 0.6888828873634338 \t 18\t\n",
      "82 0.6881334185600281 \t 27\t\n",
      "83 0.6880431771278381 \t 15\t\n",
      "84 0.6884781122207642 \t 28\t\n",
      "85 0.6887097358703613 \t 22\t\n",
      "86 0.6877427697181702 \t 16\t\n",
      "87 0.6880693435668945 \t 26\t\n",
      "88 0.6879911422729492 \t 20\t\n",
      "89 0.6878249645233154 \t 24\t\n",
      "90 0.6889832615852356 \t 25\t\n",
      "91 0.6874247789382935 \t 17\t\n",
      "92 0.6882349848747253 \t 15\t\n",
      "93 0.6889501214027405 \t 24\t\n",
      "94 0.6883372068405151 \t 15\t\n",
      "95 0.6884599328041077 \t 25\t\n",
      "96 0.6880961656570435 \t 22\t\n",
      "97 0.6880884766578674 \t 19\t\n",
      "98 0.6876107454299927 \t 20\t\n",
      "99 0.6880044937133789 \t 15\t\n",
      "\n",
      "===================================================\n",
      "100000\n",
      "0.6812957424539328 \t 20\n",
      "0 0.6821429133415222 \t 56\t\n",
      "1 0.6820333003997803 \t 69\t\n",
      "2 0.6818349957466125 \t 63\t\n",
      "3 0.6821051836013794 \t 66\t\n",
      "4 0.682115375995636 \t 61\t\n",
      "5 0.6821930408477783 \t 54\t\n",
      "6 0.682066023349762 \t 81\t\n",
      "7 0.6819871664047241 \t 72\t\n",
      "8 0.6823326349258423 \t 66\t\n",
      "9 0.6820245981216431 \t 52\t\n",
      "10 0.6820383071899414 \t 51\t\n",
      "11 0.6822550296783447 \t 39\t\n",
      "12 0.6821836829185486 \t 57\t\n",
      "13 0.6819856762886047 \t 62\t\n",
      "14 0.6819664835929871 \t 58\t\n",
      "15 0.6819966435432434 \t 53\t\n",
      "16 0.6820155382156372 \t 55\t\n",
      "17 0.6822097897529602 \t 56\t\n",
      "18 0.6820162534713745 \t 47\t\n",
      "19 0.6819322109222412 \t 52\t\n",
      "20 0.6820418834686279 \t 68\t\n",
      "21 0.682033121585846 \t 48\t\n",
      "22 0.6817386150360107 \t 100\t\n",
      "23 0.6817985773086548 \t 65\t\n",
      "24 0.681878387928009 \t 76\t\n",
      "25 0.6819101572036743 \t 74\t\n",
      "26 0.6820811033248901 \t 70\t\n",
      "27 0.6820062398910522 \t 66\t\n",
      "28 0.6819852590560913 \t 54\t\n",
      "29 0.6819508075714111 \t 56\t\n",
      "30 0.681961715221405 \t 49\t\n",
      "31 0.6819787621498108 \t 48\t\n",
      "32 0.6820548176765442 \t 62\t\n",
      "33 0.6820107698440552 \t 66\t\n",
      "34 0.6819604635238647 \t 58\t\n",
      "35 0.68208909034729 \t 65\t\n",
      "36 0.6820393800735474 \t 68\t\n",
      "37 0.6821997761726379 \t 60\t\n",
      "38 0.682119607925415 \t 62\t\n",
      "39 0.6822013258934021 \t 34\t\n",
      "40 0.6820630431175232 \t 79\t\n",
      "41 0.6821699738502502 \t 76\t\n",
      "42 0.6818900108337402 \t 100\t\n",
      "43 0.6816328167915344 \t 100\t\n",
      "44 0.6819989681243896 \t 78\t\n",
      "45 0.6820361614227295 \t 59\t\n",
      "46 0.6821033358573914 \t 42\t\n",
      "47 0.6821302175521851 \t 44\t\n",
      "48 0.6819772720336914 \t 68\t\n",
      "49 0.6819278001785278 \t 56\t\n",
      "50 0.6821860074996948 \t 47\t\n",
      "51 0.682178258895874 \t 51\t\n",
      "52 0.6821451783180237 \t 65\t\n",
      "53 0.6822564601898193 \t 63\t\n",
      "54 0.6821545362472534 \t 49\t\n",
      "55 0.6822259426116943 \t 46\t\n",
      "56 0.6823378801345825 \t 41\t\n",
      "57 0.6819879412651062 \t 58\t\n",
      "58 0.68219393491745 \t 58\t\n",
      "59 0.6820723414421082 \t 79\t\n",
      "60 0.6822485327720642 \t 63\t\n",
      "61 0.6820321679115295 \t 73\t\n",
      "62 0.6816253066062927 \t 100\t\n",
      "63 0.6820845603942871 \t 50\t\n",
      "64 0.682201623916626 \t 61\t\n",
      "65 0.6820042729377747 \t 64\t\n",
      "66 0.6821167469024658 \t 78\t\n",
      "67 0.6815747022628784 \t 100\t\n",
      "68 0.6817527413368225 \t 61\t\n",
      "69 0.682197630405426 \t 58\t\n",
      "70 0.6822700500488281 \t 55\t\n",
      "71 0.6817107200622559 \t 100\t\n",
      "72 0.6821191310882568 \t 41\t\n",
      "73 0.6819555759429932 \t 57\t\n",
      "74 0.6821191310882568 \t 81\t\n",
      "75 0.6821953058242798 \t 43\t\n",
      "76 0.6818495392799377 \t 100\t\n",
      "77 0.6822765469551086 \t 41\t\n",
      "78 0.6819890141487122 \t 79\t\n",
      "79 0.6819144487380981 \t 53\t\n",
      "80 0.6821165084838867 \t 45\t\n",
      "81 0.681991457939148 \t 53\t\n",
      "82 0.6820200085639954 \t 84\t\n",
      "83 0.6820682287216187 \t 54\t\n",
      "84 0.6822212338447571 \t 63\t\n",
      "85 0.6820718050003052 \t 74\t\n",
      "86 0.682539165019989 \t 43\t\n",
      "87 0.6819038987159729 \t 48\t\n",
      "88 0.6821653842926025 \t 53\t\n",
      "89 0.6822657585144043 \t 67\t\n",
      "90 0.6821179389953613 \t 58\t\n",
      "91 0.6822747588157654 \t 76\t\n",
      "92 0.68229740858078 \t 50\t\n",
      "93 0.682022213935852 \t 69\t\n",
      "94 0.682335376739502 \t 42\t\n",
      "95 0.6821781396865845 \t 55\t\n",
      "96 0.681540846824646 \t 100\t\n",
      "97 0.6822745203971863 \t 54\t\n",
      "98 0.6820491552352905 \t 58\t\n",
      "99 0.6821134090423584 \t 53\t\n",
      "\n",
      "===================================================\n",
      "1000000\n",
      "0.6770499480839255 \t 71\n",
      "0 0.6787963509559631 \t 100\t\n",
      "1 0.6787541508674622 \t 100\t\n",
      "2 0.6788221001625061 \t 100\t\n",
      "3 0.6784877777099609 \t 100\t\n",
      "4 0.6785604953765869 \t 100\t\n",
      "5 0.6786550879478455 \t 100\t\n",
      "6 0.678648054599762 \t 100\t\n",
      "7 0.6787700057029724 \t 100\t\n",
      "8 0.6784223318099976 \t 100\t\n",
      "9 0.6786335110664368 \t 100\t\n",
      "10 0.6788621544837952 \t 100\t\n",
      "11 0.678730845451355 \t 100\t\n",
      "12 0.6788461208343506 \t 100\t\n",
      "13 0.678737998008728 \t 100\t\n",
      "14 0.6790534853935242 \t 100\t\n",
      "15 0.6789115071296692 \t 100\t\n",
      "16 0.6785424947738647 \t 100\t\n",
      "17 0.6786066889762878 \t 100\t\n",
      "18 0.6787896156311035 \t 100\t\n",
      "19 0.6787605285644531 \t 100\t\n",
      "20 0.6788294315338135 \t 100\t\n",
      "21 0.6785271167755127 \t 100\t\n",
      "22 0.6786279082298279 \t 100\t\n",
      "23 0.6788671016693115 \t 100\t\n",
      "24 0.6787713766098022 \t 100\t\n",
      "25 0.6787518858909607 \t 100\t\n",
      "26 0.678478479385376 \t 100\t\n",
      "27 0.6787803769111633 \t 100\t\n",
      "28 0.6786049604415894 \t 100\t\n",
      "29 0.6785382628440857 \t 100\t\n",
      "30 0.6788198947906494 \t 100\t\n",
      "31 0.6786969900131226 \t 100\t\n",
      "32 0.6787611246109009 \t 100\t\n",
      "33 0.6787558197975159 \t 100\t\n",
      "34 0.6787477731704712 \t 100\t\n",
      "35 0.6786507964134216 \t 100\t\n",
      "36 0.6788048148155212 \t 100\t\n",
      "37 0.6787508726119995 \t 100\t\n",
      "38 0.6787245869636536 \t 100\t\n",
      "39 0.6786854863166809 \t 100\t\n",
      "40 0.6785776615142822 \t 100\t\n",
      "41 0.6784966588020325 \t 100\t\n",
      "42 0.678674042224884 \t 100\t\n",
      "43 0.6787523627281189 \t 94\t\n",
      "44 0.6784734725952148 \t 100\t\n",
      "45 0.6786606311798096 \t 100\t\n",
      "46 0.6788427233695984 \t 100\t\n",
      "47 0.6789283752441406 \t 100\t\n",
      "48 0.6788767576217651 \t 100\t\n",
      "49 0.6787727475166321 \t 100\t\n",
      "51 0.6786976456642151 \t 100\t\n",
      "52 0.6786531805992126 \t 100\t\n",
      "53 0.6787914633750916 \t 100\t\n",
      "54 0.6787359714508057 \t 100\t\n",
      "55 0.6786573529243469 \t 100\t\n",
      "56 0.67853844165802 \t 100\t\n",
      "57 0.6787751317024231 \t 100\t\n",
      "58 0.6789833903312683 \t 100\t\n",
      "59 0.6786304116249084 \t 100\t\n",
      "60 0.6787526607513428 \t 100\t\n",
      "61 0.6787706017494202 \t 100\t\n",
      "62 0.6785318851470947 \t 100\t\n",
      "63 0.6785640120506287 \t 100\t\n",
      "64 0.6789607405662537 \t 100\t\n",
      "65 0.678598165512085 \t 100\t\n",
      "66 0.6787300705909729 \t 100\t\n",
      "67 0.6788697242736816 \t 100\t\n",
      "68 0.6786303520202637 \t 100\t\n",
      "69 0.6787620186805725 \t 100\t\n",
      "70 0.6786851286888123 \t 100\t\n",
      "71 0.6787732243537903 \t 100\t\n",
      "72 0.6787470579147339 \t 100\t\n",
      "73 0.6786297559738159 \t 100\t\n",
      "74 0.6786592602729797 \t 100\t\n",
      "75 0.6786634922027588 \t 100\t\n",
      "76 0.6787068843841553 \t 100\t\n",
      "77 0.6788414716720581 \t 100\t\n",
      "78 0.6786222457885742 \t 100\t\n",
      "79 0.678695797920227 \t 100\t\n",
      "80 0.6785991191864014 \t 100\t\n",
      "81 0.678560733795166 \t 100\t\n",
      "82 0.6787131428718567 \t 100\t\n",
      "83 0.6785870790481567 \t 100\t\n",
      "84 0.6785855293273926 \t 100\t\n",
      "85 0.6786553859710693 \t 100\t\n",
      "86 0.6786222457885742 \t 100\t\n",
      "87 0.6787727475166321 \t 100\t\n",
      "88 0.678824245929718 \t 100\t\n",
      "89 0.6786184906959534 \t 100\t\n",
      "90 0.67875736951828 \t 100\t\n",
      "91 0.6786792278289795 \t 100\t\n",
      "92 0.6789093613624573 \t 100\t\n",
      "93 0.6787317395210266 \t 100\t\n",
      "94 0.6788125038146973 \t 100\t\n",
      "95 0.678795337677002 \t 100\t\n",
      "96 0.67864990234375 \t 100\t\n",
      "97 0.6787506937980652 \t 100\t\n",
      "98 0.678737998008728 \t 100\t\n",
      "99 0.6785796284675598 \t 100\t\n",
      "\n",
      "===================================================\n",
      "10000000\n",
      "0.6757857166656779 \t 100\n",
      "0 0.6773370504379272 \t 100\t\n",
      "1 0.6773890256881714 \t 100\t\n",
      "2 0.6775267124176025 \t 100\t\n",
      "3 0.6775228977203369 \t 100\t\n",
      "4 0.6774517893791199 \t 100\t\n",
      "5 0.6776573657989502 \t 100\t\n",
      "6 0.6777623295783997 \t 100\t\n",
      "7 0.6775069236755371 \t 100\t\n",
      "8 0.6774346232414246 \t 100\t\n",
      "9 0.6777529120445251 \t 100\t\n",
      "10 0.6774334907531738 \t 100\t\n",
      "11 0.677515983581543 \t 100\t\n",
      "12 0.6776198744773865 \t 100\t\n",
      "13 0.6774360537528992 \t 100\t\n",
      "14 0.6774284839630127 \t 100\t\n",
      "15 0.6775581240653992 \t 100\t\n",
      "16 0.6777570843696594 \t 100\t\n",
      "17 0.6773850321769714 \t 100\t\n",
      "18 0.6774020791053772 \t 100\t\n",
      "19 0.6775873899459839 \t 100\t\n",
      "20 0.6775277853012085 \t 100\t\n",
      "21 0.6773185133934021 \t 100\t\n",
      "22 0.6773185729980469 \t 100\t\n",
      "23 0.6775904297828674 \t 100\t\n",
      "24 0.6774309277534485 \t 100\t\n",
      "25 0.6778125762939453 \t 100\t\n",
      "26 0.6774646043777466 \t 100\t\n",
      "27 0.6775654554367065 \t 100\t\n",
      "28 0.6775128245353699 \t 100\t\n",
      "29 0.6774923205375671 \t 100\t\n",
      "30 0.6776857376098633 \t 100\t\n",
      "31 0.6775512099266052 \t 100\t\n",
      "32 0.6775441765785217 \t 100\t\n",
      "33 0.6773367524147034 \t 100\t\n",
      "34 0.677562415599823 \t 100\t\n",
      "35 0.6773591041564941 \t 100\t\n",
      "36 0.6774616241455078 \t 100\t\n",
      "37 0.677363932132721 \t 100\t\n",
      "38 0.6772983074188232 \t 100\t\n",
      "39 0.6776465773582458 \t 100\t\n",
      "40 0.677497386932373 \t 100\t\n",
      "41 0.6773096919059753 \t 100\t\n",
      "42 0.6776283979415894 \t 100\t\n",
      "43 0.6779935956001282 \t 100\t\n",
      "44 0.6776748895645142 \t 100\t\n",
      "45 0.67763352394104 \t 100\t\n",
      "46 0.6773386001586914 \t 100\t\n",
      "47 0.6775578260421753 \t 100\t\n",
      "48 0.6774654984474182 \t 100\t\n",
      "49 0.677450954914093 \t 100\t\n",
      "50 0.6773912906646729 \t 100\t\n",
      "51 0.6775532960891724 \t 100\t\n",
      "52 0.6773650050163269 \t 100\t\n",
      "53 0.6775807738304138 \t 100\t\n",
      "54 0.6775400042533875 \t 100\t"
     ]
    }
   ],
   "source": [
    "for N in Ns:\n",
    "    print('===================================================\\n{}'.format(N))\n",
    "    # Take the first N samples.\n",
    "    data, m, s = split_data(X[:N], y[:N])\n",
    "    \n",
    "    # Train BDT model (only need to train 1)\n",
    "    bdt_model = XGBClassifier(early_stopping_rounds = 10)\n",
    "    X_trn, X_vld, y_trn, y_vld = data\n",
    "    bdt_model.fit(X_trn, y_trn, eval_set = [(X_vld, y_vld)], verbose = 0)\n",
    "    trace = bdt_model.evals_result()['validation_0']\n",
    "    print(trace['logloss'][-1], '\\t', len(trace['logloss']), end = '\\n')\n",
    "    bdt_model.save_model(bdt_filestr.format(N))\n",
    "\n",
    "    for i in range(reps):\n",
    "        print(i, end = ' ')\n",
    "        # Train BCE model\n",
    "        bce_model, trace = train(data, **bce_params)\n",
    "        bce_model.save_weights(bce_filestr.format(N, i))\n",
    "        \n",
    "        # Train GBC model\n",
    "        gbc_model = GradientBoostingClassifier(validation_fraction = 0.25,\n",
    "                                               n_iter_no_change = 10)\n",
    "        gbc_model.fit(X[:N], y[:N])\n",
    "        dump(gbc_model, gbc_filestr.format(N, i))\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd8d17d-d03a-4d22-9a8f-537bfb822352",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## $d = 8$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d23a752-df10-4f18-bc9f-222e20f10212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "num = 0\n",
    "reps = 100\n",
    "d = 8\n",
    "Ns = 10**np.arange(2, 8)\n",
    "\n",
    "# Model parameters\n",
    "bce_params = {'loss':bce, 'd': d}\n",
    "\n",
    "filestr = 'models/zenodo/{}/set_{}/'.format(d, num)\n",
    "bce_filestr = filestr + 'bce/model_{}_{}.h5'\n",
    "bdt_filestr = filestr + 'bdt/model_{}.h5'\n",
    "gbc_filestr = filestr + 'gbc/model_{}_{}.h5'\n",
    "\n",
    "if not os.path.isdir(filestr):\n",
    "    os.mkdir(filestr)\n",
    "\n",
    "if not os.path.isdir(filestr + 'bce/'):\n",
    "    os.mkdir(filestr + 'bce/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'bdt/'):\n",
    "    os.mkdir(filestr + 'bdt/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'gbc/'):\n",
    "    os.mkdir(filestr + 'gbc/')\n",
    "\n",
    "# Data parameters\n",
    "X = np.load('data/zenodo/{}/X_trn.npy'.format(d))\n",
    "y = np.load('data/zenodo/{}/y_trn.npy'.format(d)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a5ea4b-dc38-470e-a65d-9c3217dcc73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for N in Ns:\n",
    "    print('===================================================\\n{}'.format(N))\n",
    "    # Take the first N samples.\n",
    "    data, m, s = split_data(X[:N], y[:N])\n",
    "    \n",
    "    # Train BDT model (only need to train 1)\n",
    "    bdt_model = XGBClassifier(early_stopping_rounds = 10)\n",
    "    X_trn, X_vld, y_trn, y_vld = data\n",
    "    bdt_model.fit(X_trn, y_trn, eval_set = [(X_vld, y_vld)], verbose = 0)\n",
    "    trace = bdt_model.evals_result()['validation_0']\n",
    "    print(trace['logloss'][-1], '\\t', len(trace['logloss']), end = '\\n')\n",
    "    bdt_model.save_model(bdt_filestr.format(N))\n",
    "\n",
    "    for i in range(reps):\n",
    "        print(i, end = ' ')\n",
    "        # Train BCE model\n",
    "        bce_model, trace = train(data, **bce_params)\n",
    "        bce_model.save_weights(bce_filestr.format(N, i))\n",
    "        \n",
    "        # Train GBC model\n",
    "        gbc_model = GradientBoostingClassifier(validation_fraction = 0.25,\n",
    "                                               n_iter_no_change = 10)\n",
    "        gbc_model.fit(X[:N], y[:N])\n",
    "        dump(gbc_model, gbc_filestr.format(N, i))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef99106-1889-4016-afdc-36c47f2bb0a2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## $d = 11$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa77dcda-85fc-4e15-afc9-2081a23d9796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "num = 0\n",
    "reps = 100\n",
    "d = 11\n",
    "Ns = 10**np.arange(2, 8)\n",
    "\n",
    "# Model parameters\n",
    "bce_params = {'loss':bce, 'd': d}\n",
    "\n",
    "filestr = 'models/zenodo/{}/set_{}/'.format(d, num)\n",
    "bce_filestr = filestr + 'bce/model_{}_{}.h5'\n",
    "bdt_filestr = filestr + 'bdt/model_{}.h5'\n",
    "gbc_filestr = filestr + 'gbc/model_{}_{}.h5'\n",
    "\n",
    "if not os.path.isdir(filestr):\n",
    "    os.mkdir(filestr)\n",
    "\n",
    "if not os.path.isdir(filestr + 'bce/'):\n",
    "    os.mkdir(filestr + 'bce/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'bdt/'):\n",
    "    os.mkdir(filestr + 'bdt/')\n",
    "    \n",
    "if not os.path.isdir(filestr + 'gbc/'):\n",
    "    os.mkdir(filestr + 'gbc/')\n",
    "\n",
    "# Data parameters\n",
    "X = np.load('data/zenodo/{}/X_trn.npy'.format(d))\n",
    "y = np.load('data/zenodo/{}/y_trn.npy'.format(d)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918ba1cb-8b9c-4545-bb0d-9ddbaf89bfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for N in Ns:\n",
    "    print('===================================================\\n{}'.format(N))\n",
    "    # Take the first N samples.\n",
    "    data, m, s = split_data(X[:N], y[:N])\n",
    "    \n",
    "    # Train BDT model (only need to train 1)\n",
    "    bdt_model = XGBClassifier(early_stopping_rounds = 10)\n",
    "    X_trn, X_vld, y_trn, y_vld = data\n",
    "    bdt_model.fit(X_trn, y_trn, eval_set = [(X_vld, y_vld)], verbose = 0)\n",
    "    trace = bdt_model.evals_result()['validation_0']\n",
    "    print(trace['logloss'][-1], '\\t', len(trace['logloss']), end = '\\n')\n",
    "    bdt_model.save_model(bdt_filestr.format(N))\n",
    "\n",
    "    for i in range(reps):\n",
    "        print(i, end = ' ')\n",
    "        # Train BCE model\n",
    "        bce_model, trace = train(data, **bce_params)\n",
    "        bce_model.save_weights(bce_filestr.format(N, i))\n",
    "        \n",
    "        # Train GBC model\n",
    "        gbc_model = GradientBoostingClassifier(validation_fraction = 0.25,\n",
    "                                               n_iter_no_change = 10)\n",
    "        gbc_model.fit(X[:N], y[:N])\n",
    "        dump(gbc_model, gbc_filestr.format(N, i))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multifold",
   "language": "python",
   "name": "multifold"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
